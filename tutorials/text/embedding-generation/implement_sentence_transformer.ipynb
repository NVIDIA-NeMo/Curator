{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745f8008",
   "metadata": {},
   "source": [
    "## Re implement ModelStage to work with Sentence Transformers\n",
    "\n",
    "1. We create a new stage that extends `EmbeddingModelStage` and overwrites setup() to use SentenceTransformers\n",
    "2. We create a new composte stage which replaces `EmbeddingCreatorStage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nemo_curator.backends.base import WorkerMetadata\n",
    "from nemo_curator.stages.base import CompositeStage, ProcessingStage\n",
    "from nemo_curator.stages.text.embedders.base import EmbeddingModelStage\n",
    "from nemo_curator.stages.text.models.tokenizer import TokenizerStage\n",
    "from nemo_curator.tasks import DocumentBatch\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbeddingModelStage(EmbeddingModelStage):\n",
    "    def __init__(  # noqa: PLR0913\n",
    "        self,\n",
    "        model_identifier: str,\n",
    "        embedding_field: str = \"embeddings\",\n",
    "        hf_token: str | None = None,\n",
    "        model_inference_batch_size: int = 1024,\n",
    "        has_seq_order: bool = True,\n",
    "        padding_side: Literal[\"left\", \"right\"] = \"right\",\n",
    "        autocast: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_identifier=model_identifier,\n",
    "            hf_token=hf_token,\n",
    "            model_inference_batch_size=model_inference_batch_size,\n",
    "            has_seq_order=has_seq_order,\n",
    "            padding_side=padding_side,\n",
    "            autocast=autocast,\n",
    "        )\n",
    "        # Override unpack_inference_batch to False (EmbeddingModelStage sets it to True)\n",
    "        self.unpack_inference_batch = False\n",
    "        self.embedding_field = embedding_field\n",
    "\n",
    "    def outputs(self) -> tuple[list[str], list[str]]:\n",
    "        return [\"data\"], [self.embedding_field]\n",
    "\n",
    "    def setup(self, _: WorkerMetadata | None = None) -> None:\n",
    "        \"\"\"Load the model for inference.\"\"\"\n",
    "        self.model = SentenceTransformer(self.model_identifier, local_files_only=True)\n",
    "        self.model.eval().to(\"cuda\")\n",
    "\n",
    "    def process_model_output(\n",
    "        self,\n",
    "        outputs: torch.Tensor,\n",
    "        model_input_batch: dict[str, torch.Tensor] | None = None,  # noqa: ARG002\n",
    "    ) -> torch.Tensor:\n",
    "        return outputs[\"sentence_embedding\"].cpu()\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class SentenceTransformerEmbeddingCreatorStage(CompositeStage[DocumentBatch, DocumentBatch]):\n",
    "    model_identifier: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    text_field: str = \"text\"\n",
    "    embedding_field: str = \"embeddings\"\n",
    "    max_chars: int | None = None\n",
    "    max_seq_length: int | None = None\n",
    "    padding_side: Literal[\"left\", \"right\"] = \"right\"\n",
    "    model_inference_batch_size: int = 1024\n",
    "\n",
    "    autocast: bool = True\n",
    "    sort_by_length: bool = True\n",
    "    hf_token: str | None = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.stages = [\n",
    "            TokenizerStage(\n",
    "                model_identifier=self.model_identifier,\n",
    "                hf_token=self.hf_token,\n",
    "                text_field=self.text_field,\n",
    "                max_chars=self.max_chars,\n",
    "                max_seq_length=self.max_seq_length,\n",
    "                padding_side=self.padding_side,\n",
    "                sort_by_length=self.sort_by_length,\n",
    "            ),\n",
    "            SentenceTransformerEmbeddingModelStage(\n",
    "                model_identifier=self.model_identifier,\n",
    "                embedding_field=self.embedding_field,\n",
    "                hf_token=self.hf_token,\n",
    "                model_inference_batch_size=self.model_inference_batch_size,\n",
    "                has_seq_order=self.sort_by_length,\n",
    "                padding_side=self.padding_side,\n",
    "                autocast=self.autocast,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def decompose(self) -> list[ProcessingStage]:\n",
    "        return self.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911270e4",
   "metadata": {},
   "source": [
    "## Setup the stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc4bceabcf6495a9aa07cb015314fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b78549fbec4876bd3eb73796b5ed5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/embeddinggemma-300m\"\n",
    "st_composite_stage = SentenceTransformerEmbeddingCreatorStage(model_identifier=model_name)\n",
    "\n",
    "st_tokenizer_stage = st_composite_stage.decompose()[0]\n",
    "st_model_stage = st_composite_stage.decompose()[1]\n",
    "\n",
    "st_tokenizer_stage.setup_on_node(None)\n",
    "st_tokenizer_stage.setup(None)\n",
    "\n",
    "st_model_stage.setup_on_node(None)\n",
    "st_model_stage.setup(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa0c75",
   "metadata": {},
   "source": [
    "### Run using the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f9d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>绝不能放弃，世界上没有失败，只有放弃。</td>\n",
       "      <td>[2, 239306, 17055, 91435, 236900, 82255, 8939,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[-0.15909849107265472, 0.0327397957444191, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is there any doubt about it \"None whatsoever\"</td>\n",
       "      <td>[2, 511, 993, 1027, 9370, 1003, 625, 623, 9336...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[-0.17032906413078308, 0.03656821325421333, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>세상 어떤 짐승이 이를 드러내고 사냥을 해? 약한 짐승이나 몸을 부풀리지, 진짜 짐...</td>\n",
       "      <td>[2, 238040, 237774, 51955, 236743, 242596, 239...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-0.07146891951560974, 0.012988940812647343, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>そのように二番目に死を偽装して生き残るようになったイタドリがどうして初めて見る自分をこんなに...</td>\n",
       "      <td>[2, 9266, 19164, 237725, 238508, 143926, 23854...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-0.08917465806007385, 0.03781035169959068, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                绝不能放弃，世界上没有失败，只有放弃。   \n",
       "1      is there any doubt about it \"None whatsoever\"   \n",
       "2  세상 어떤 짐승이 이를 드러내고 사냥을 해? 약한 짐승이나 몸을 부풀리지, 진짜 짐...   \n",
       "3  そのように二番目に死を偽装して生き残るようになったイタドリがどうして初めて見る自分をこんなに...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [2, 239306, 17055, 91435, 236900, 82255, 8939,...   \n",
       "1  [2, 511, 993, 1027, 9370, 1003, 625, 623, 9336...   \n",
       "2  [2, 238040, 237774, 51955, 236743, 242596, 239...   \n",
       "3  [2, 9266, 19164, 237725, 238508, 143926, 23854...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.15909849107265472, 0.0327397957444191, 0.0...  \n",
       "1  [-0.17032906413078308, 0.03656821325421333, 0....  \n",
       "2  [-0.07146891951560974, 0.012988940812647343, 0...  \n",
       "3  [-0.08917465806007385, 0.03781035169959068, 0....  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo_curator.tasks import DocumentBatch\n",
    "\n",
    "input_text = [\n",
    "    \"绝不能放弃，世界上没有失败，只有放弃。\",  # noqa: RUF001\n",
    "    'is there any doubt about it \"None whatsoever\"',\n",
    "    \"세상 어떤 짐승이 이를 드러내고 사냥을 해? 약한 짐승이나 몸을 부풀리지, 진짜 짐승은 누구보다 침착하지.\",\n",
    "    \"そのように二番目に死を偽装して生き残るようになったイタドリがどうして初めて見る自分をこんなに気遣ってくれるのかと尋ねると「私が大切にする人たちがあなたを大切にするから」と答えては\",\n",
    "]\n",
    "\n",
    "\n",
    "dummy_batch = DocumentBatch(\n",
    "    task_id=\"dummy_task\",\n",
    "    dataset_name=\"dummy_dataset\",\n",
    "    data=pd.DataFrame({\"text\": input_text}),\n",
    ")\n",
    "\n",
    "tokenized_output_task = st_tokenizer_stage.process(dummy_batch)\n",
    "model_output_task = st_model_stage.process(tokenized_output_task)\n",
    "\n",
    "model_output_task.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65843c62",
   "metadata": {},
   "source": [
    "### Run using raw SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "095abca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_raw_model = SentenceTransformer(model_name)\n",
    "st_raw_model.eval().to(\"cuda\")\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    st_raw_output = st_raw_model.encode(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e66198",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "230f0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.testing.assert_allclose(np.asarray(model_output_task.data[\"embeddings\"].tolist()), st_raw_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
