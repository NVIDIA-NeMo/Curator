# Download and Extract Common Crawl, Wikipedia, and ArXiv Data

This Jupyter notebook tutorial demonstrates how to use NeMo Curator to download text data from [Common Crawl](https://commoncrawl.org/), [Wikipedia](https://dumps.wikimedia.org/backup-index.html), and [ArXiv](https://info.arxiv.org/help/bulk_data_s3.html), respectively.

Please note that the ArXiv section of the tutorial requires the [s5cmd](https://github.com/peak/s5cmd) tool to be installed and configured with proper AWS credentials.
