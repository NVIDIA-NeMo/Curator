{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1a4119",
   "metadata": {},
   "source": [
    "# Nemo Curator Pipeline Example\n",
    "\n",
    "## NeMo Curator Introduction\n",
    "The NeMo Curator is a Python library that consists of a collection of scalable data-mining modules for curating natural language processing (NLP) data for training large language models (LLMs). The modules within the NeMo Data Curator enable NLP researchers to mine high-quality text at scale from massive uncurated web corpora. \n",
    "\n",
    "NeMo Curator includes the following modules to perform data curation:\n",
    "- Data download and Extraction\n",
    "- Language identification and separation\n",
    "- Text reformatting and cleaning\n",
    "- Quality filtering\n",
    "- Document-level deduplication\n",
    "- Multilingual downstream-task decontamination\n",
    "- Distributed Data Classification\n",
    "\n",
    "NeMo Curator team has perform ablation experiments using Common Crawl dataset to train a 357M GPT-style model to assess the effect of different curation stage on model performance. \n",
    "\n",
    "![alt text](./image/zeroshot_ablations.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41377f",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "\n",
    "This notebook will use **Thai Wikipedia dataset** as example to demonstrate a typical data curation pipeline using NeMo Curator. After running through this script, user will be able to know how to use NDC to download wikipedia data, perform language separation using fasttext, perform GPU based exact deduplication and fuzzy deduplication and use CPU based heuristic filtering. \n",
    "\n",
    "Step description:\n",
    "1. Download and extract data\n",
    "2. Language detection and separation\n",
    "3. GPU based deduplication\n",
    "    1. Exact deduplication\n",
    "    2. Fuzzy deduplication\n",
    "4. Heuristic filtering\n",
    "\n",
    "What is not included:\n",
    "1. Customized downloading\n",
    "2. Classifier filtering\n",
    "3. Downstream-task deduplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860c239",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### System Requirements\n",
    "Here is the hardware setting for this notebook\n",
    "\n",
    "**GPU**: NVIDIA A10 24G. \n",
    "\n",
    "**CUDA & Nvidia Drivers**: CUDA 12.2 with Driver 535.154.05\n",
    "\n",
    "**OS**: ubuntu 22.04\n",
    "\n",
    "### Getting NeMo Framework Training Container\n",
    "- Get access to the container via https://developer.nvidia.com/nemo-framework\n",
    "- Set your docker credentials \n",
    "    ```bash\n",
    "    docker login nvcr.io\n",
    "\n",
    "    Username: $oauthtoken\n",
    "    Password: <Your NGC Key>\n",
    "- Get NeMo NeMo FrameWork Training Container\n",
    "    ```bash\n",
    "    docker pull nvcr.io/ea-bignlp/ga-participants/nemofw-training:24.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bff1b",
   "metadata": {},
   "source": [
    "## 0. Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24dce020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-4.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6831f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from nemo_curator.utils.distributed_utils import get_client,get_num_workers\n",
    "from nemo_curator.utils.script_utils import add_distributed_args\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under, separate_by_metadata\n",
    "from nemo_curator.utils.distributed_utils import read_data, write_to_disk\n",
    "from nemo_curator.gpu_deduplication.utils import (create_logger, parse_nc_args, performance_report_if, enable_spilling)\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28739b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imports():\n",
    "    import cudf \n",
    "\n",
    "def load_dataset(input_data_dir, file_type='jsonl'):\n",
    "    files = list(get_all_files_paths_under(input_data_dir))\n",
    "    raw_data = read_data(files, file_type=file_type, backend=\"pandas\", add_filename=True)\n",
    "    dataset = DocumentDataset(raw_data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def attach_args(parser=argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)):\n",
    "    return add_distributed_args(parser)\n",
    "\n",
    "def check_jsonl_file(file_dir):\n",
    "    for file in os.listdir(file_dir):\n",
    "        if 'jsonl' not in file:\n",
    "            continue\n",
    "        with open(os.path.join(file_dir,file), 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            print(first_line)\n",
    "        break\n",
    "\n",
    "def extract_lines_with_id(file_path,target_list):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for obj in reader:\n",
    "            if obj.get('id') in target_list:\n",
    "                yield obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d279329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nluo_data/NeMo-Curator/tutorials/single_node_tutorial\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "print(cur_dir)\n",
    "data_dir = f\"{cur_dir}/workspace/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f452a3",
   "metadata": {},
   "source": [
    "## 1. Download\n",
    "In this example, Thai wikipedia data will be downloaded.\n",
    "\n",
    "Here is what happens when function `download_wikipedia()` is called:\n",
    "1. Run `get_wikipedia_urls()` to obtain a list of urls to download .bz2 files for Thai wikipedia data. In this module, we use the base link and the language from user input to formulate a repo links for downloadable wikipedia .bz2 dump files. The formulated link will be `https://dumps.wikimedia.org/<language>wiki`. All the links will be stored in a .txt file. Argument for this function includes:\n",
    "    - `dump_dates`: A date in the string format of 'YYYYMMDD'. It determines which wikipedia snapshot will be downloaded. If not specified, the `latest` snapshot will be downloaded\n",
    "    - `language`: language code of the desired language in lower case. Default value is `en`\n",
    "\n",
    "2. \n",
    "    Run `download_and_extract()` to download and extract contents based on the url list obtained from `get_wikipedia_urls`. User will need to define `downloader`, `extractor` and `iterator` for the dataset. \n",
    "    In this case, `WikipediaDownloader`,`WikipediaIterator` and `WikipediaExtractor` are used.\n",
    "    - `WikipediaDownloader`: Downloads wikipedia dumps file to local folder.\n",
    "    - `WikipediaIterator`: Extracts the .bz2 files and useful content from the base html content.\n",
    "    -  `WikipediaExtractor`: Performs further task specific html content cleaning such as removing media files, removing references/tables etc. and finally yield pure text data which will be store in .jsonl format. \n",
    "    Please refer to `./NeMo-Curator/nemo_curator/download/wikipedia.py` for  detail implementation.\n",
    "    \n",
    "    Argument for this function includes:\n",
    "    - `output_path`: Output path for downloaded and extracted dataset\n",
    "    - `output_type`: Type of output file. Default is .jsonl. User might choose other types such as parquet. In this example, .jsonl will be used\n",
    "    - `language`: See above\n",
    "    - `dump_date`: See above\n",
    "    - `raw_download_dir`: Output path for intermediate downloaded .bz2 file. If not specified, will be downloaded to `output_path`\n",
    "    - `keep_raw_download`: Whether to keep downloaded .bz2 files after extraction. Default is not to keep.\n",
    "    - `force_download`: Whether to restart downloading process if the target .bz2 files are detected under the `raw_download_dir` \n",
    "    - `url_limit`: Number of .bz2 files to be downloaded.\n",
    "\n",
    "The resultant .jsonl for Thai wikipedia will contain the following keys:\n",
    "1. text\n",
    "2. title\n",
    "3. id\n",
    "4. url\n",
    "5. language\n",
    "6. source_id\n",
    "7. file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1773cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.download import download_wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711a8f8",
   "metadata": {},
   "source": [
    " Start a CPU based Dask cluster. Please modify `n_workers` and `memory_limit` according to your hardware specification. To process TH wikipedia data, it's advised to have `memory_limit` greater than 12GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ec66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=10, processes=True, memory_limit='16GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794b51c",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90f3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output\n",
    "download_base_directory= os.path.join(data_dir,\"wiki_downloads\")\n",
    "download_output_directory = os.path.join(download_base_directory,\"data\")\n",
    "\n",
    "#Relevant parameter\n",
    "dump_date = \"20240201\"\n",
    "language = 'th'\n",
    "url_limit = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628356b",
   "metadata": {},
   "source": [
    "Download TH wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = download_wikipedia(download_output_directory,\n",
    "                   language=language, \n",
    "                   dump_date=dump_date,\n",
    "                   url_limit=url_limit).df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae29dd",
   "metadata": {},
   "source": [
    "Verify result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "169fadb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloads  thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl\n",
      "162164 /nluo_data/NeMo-Curator/tutorials/single_node_tutorial/workspace/wiki_downloads/data/thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls {download_output_directory}\n",
    "! wc -l  {download_output_directory}/thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2bcb168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"–\\n\\nป้ายบอกทาง \\n ศาลาประชาคม – กระดานข่าว โครงการ ทรัพยากรและกิจกรรมซึ่งครอบคลุมวิกิพีเดียอย่างกว้างขวาง\\n แผนกช่วยเหลือ – ถามข้อสงสัยเกี่ยวกับการใช้งานวิกิพีเดีย\\n ปุจฉา-วิสัชนา – ถามข้อสงสัยทั่วไปที่คุณอยากรู้\\n ข่าวไซต์ – ประกาศ อัพเดต บทความและข้อมูลข่าวเกี่ยวกับวิกิพีเดียและมูลนิธิวิกิมีเดีย\\n สภากาแฟ – สำหรับอภิปรายเกี่ยวกับวิกิพีเดีย รวมถึงรายงานปัญหาเทคนิคและเสนอนโยบาย\\n Local Embassy – For Wikipedia-related discussion in languages other than Thai.\\n สร้างบทความใหม่ – บทช่วยสอนสำหรับเตรียมพร้อมสร้างบทความแรกของคุณ\\n\\nภาษาอื่น \\n\\n \",\"title\":\"หน้าหลัก\",\"id\":\"1\",\"url\":\"https:\\/\\/th.wikipedia.org\\/wiki\\/%E0%B8%AB%E0%B8%99%E0%B9%89%E0%B8%B2%E0%B8%AB%E0%B8%A5%E0%B8%B1%E0%B8%81\",\"language\":\"th\",\"source_id\":\"thwiki-20240201-thwiki-20240201-pages-articles-multistream.xml.bz2\",\"filename\":\"thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_jsonl_file(download_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa2d13",
   "metadata": {},
   "source": [
    "**[Optional]**Close the Dask cluster.You might encounter error such as `Caught signal 11`.It's OK, just rerun the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "590c489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.cluster.close()\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba566fc",
   "metadata": {},
   "source": [
    "## 2.Language separation and unicode fixing\n",
    "\n",
    "**Note**: In order to be run on interactive python. Please comment `from.code import *` and the related imports in `./nemo_curator/filters/__init__.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742b881",
   "metadata": {},
   "source": [
    "In this section, we will be using a language classification model by fasttext to separate the TH wikipedia dataset based on the document major languages, and we will also fix the unicode in the documents. Detailed steps are:\n",
    "\n",
    "1. Download fasttext model for text language detection\n",
    "2. Construct a filter which uses the downloaded fasttext model to produce a language label to each document. \n",
    "3. Separate each document by the language label. This will create sub-folders for each languages under the output path and the documents under the same language will be output to a .jsonl file in the corresponding sub-folder.\n",
    "4. Load .jsonl file in the folder of desirable language. In this example, `TH` folder will be loaded.\n",
    "5. Apply `UnicodeReformatter` to the data and output the result in .jsonl format. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a6e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import ScoreFilter,Modify\n",
    "from nemo_curator.filters import FastTextLangId\n",
    "from nemo_curator.modifiers import UnicodeReformatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916079c",
   "metadata": {},
   "source": [
    "**[Optional]**8Start a cpu based Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a63375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(n_workers=10, processes=True, memory_limit='16GB')\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d7357",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6270de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path\n",
    "multilingual_data_path = download_output_directory\n",
    "\n",
    "# Output path\n",
    "language_base_output_path = os.path.join(data_dir,\"language_sep\")\n",
    "language_data_output_path = os.path.join(language_base_output_path,\"data\")\n",
    "language_separated_output_path = os.path.join(language_data_output_path,\"language\")\n",
    "lang_sep_cleaned_data_output_path = os.path.join(language_data_output_path,\"cleaned\")\n",
    "\n",
    "# Fasttext model path\n",
    "model_path = language_base_output_path\n",
    "\n",
    "# Define desired language\n",
    "target_language = \"TH\"\n",
    "\n",
    "# Define key in output .jsonl files to store the language information\n",
    "language_field = \"language\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cff2d",
   "metadata": {},
   "source": [
    "Download fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7cc007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-22 08:40:55--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.74.12, 13.227.74.118, 13.227.74.9, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.74.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 131266198 (125M) [application/octet-stream]\n",
      "Saving to: ‘/nluo_data/NeMo-Curator/tutorials/single_node_tutorial/workspace/language_sep/lid.176.bin’\n",
      "\n",
      "lid.176.bin         100%[===================>] 125.18M   220MB/s    in 0.6s    \n",
      "\n",
      "2024-03-22 08:40:56 (220 MB/s) - ‘/nluo_data/NeMo-Curator/tutorials/single_node_tutorial/workspace/language_sep/lid.176.bin’ saved [131266198/131266198]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -P {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875771b",
   "metadata": {},
   "source": [
    "Apply fasttext model to separate documents by their languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c959800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for splitting language:147.80864667892456\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Load dataset \n",
    "multilingual_dataset = load_dataset(multilingual_data_path)\n",
    "\n",
    "#Define Language separation pipeline\n",
    "lang_filter = FastTextLangId(os.path.join(model_path,'lid.176.bin'))\n",
    "language_id_pipeline = ScoreFilter(lang_filter, score_field=language_field, score_type='object')\n",
    "filtered_dataset = language_id_pipeline(multilingual_dataset)\n",
    "\n",
    "# The language separation pipeline will produce a result looks like ['EN',0.96873], we only want to keep the 'EN' label and drop the detailed classifier score\n",
    "filtered_dataset.df[language_field] = filtered_dataset.df[language_field].apply(lambda score: score[1],meta = (language_field, 'object'))\n",
    "\n",
    "# Split the dataset to corresponding language sub-folders\n",
    "language_stats = separate_by_metadata(filtered_dataset.df, language_separated_output_path, metadata_field=language_field).compute()\n",
    "\n",
    "print(f\"Time taken for splitting language:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54a24a",
   "metadata": {},
   "source": [
    "Load `UnicodeReformatter` to reformat any unicode appeared in the desired language dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c09bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "Writing to disk complete for 1 partitions\n",
      "Time taken for fixing unicode:444.5816135406494\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Read the language specific data and fix the unicode in it\n",
    "lang_data_path = os.path.join(language_separated_output_path, target_language)\n",
    "lang_data = load_dataset(lang_data_path)\n",
    "\n",
    "cleaner = Modify(UnicodeReformatter())\n",
    "cleaned_data = cleaner(lang_data)\n",
    "\n",
    "# Write the cleaned_data\n",
    "write_to_disk(cleaned_data.df, lang_sep_cleaned_data_output_path, write_to_filename=True, output_type='jsonl')\n",
    "\n",
    "print(f\"Time taken for fixing unicode:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6e5a1",
   "metadata": {},
   "source": [
    "Verify the result. We can see that some documents has been removed from TH wikipedia dataset since the number of lines in this output file is less than the original file (no. of lines = 162164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2b34d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl\n",
      "161748 /nluo_data/NeMo-Curator/tutorials/single_node_tutorial/workspace/language_sep/data/cleaned/thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls {lang_sep_cleaned_data_output_path}\n",
    "! wc -l  {lang_sep_cleaned_data_output_path}/thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d539a2",
   "metadata": {},
   "source": [
    "Furthur verify by loading documents that has been identified as other language, such as 'EN'. We can see from output that the removed document is indeed in English and contains very little or even no Thai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_jsonl_file(os.path.join(language_separated_output_path,'EN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b817bf7",
   "metadata": {},
   "source": [
    "**[Optional]**Close the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bf05b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.cluster.close()\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b6aef",
   "metadata": {},
   "source": [
    "## 3.Add ID\n",
    "TH wikipedia data do have `id` field, but the `id` field contains number only. It will be better if we unified the `id` field and transform it to the format of `<prefix>_<id>`. In this way, when handling multiple dataset, we will able to know which document from which dataset has been removed. This `id` will be useful when we are running deduplication and heuristic filtering. The function we will be using is `AddID()`. Arguments for this function include:\n",
    "- `id_field`: fields will be added to input .json file. If the key already exists in the .jsonl, it's value will be replaced.\n",
    "- `id_prefix`: prefix used in ID. Default is 'doc-id'\n",
    "- `start_index`: starting index in ID. Default is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe9e6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import AddId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c01a5",
   "metadata": {},
   "source": [
    "**[Optional]**If there is no running Dask cluster, start CPU based Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f3f483eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(n_workers=10, processes=True, memory_limit='16GB')\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be65a51",
   "metadata": {},
   "source": [
    "Define relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "054019a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "add_id_input_data_dir = lang_sep_cleaned_data_output_path\n",
    "\n",
    "#Output\n",
    "added_id_output_path = os.path.join(data_dir,\"add_id/cleaned\")\n",
    "\n",
    "#Format of output ID will be <prefix>_<id>, Define prefix here\n",
    "add_ID_id_prefix=\"TH_wiki\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9591c",
   "metadata": {},
   "source": [
    "Adding ID to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8fd7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "Writing to disk complete for 1 partitions\n",
      "Time taken for add ID:56.01176333427429\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# Read input files\n",
    "dataset = load_dataset(add_id_input_data_dir)\n",
    "\n",
    "# Run AddID() on the input dataset\n",
    "add_id = AddId(id_field='id',id_prefix=add_ID_id_prefix,start_index=0)\n",
    "id_dataset = add_id(dataset)\n",
    "\n",
    "#Output files\n",
    "write_to_disk(id_dataset.df, output_file_dir=added_id_output_path, write_to_filename=True, output_type='jsonl')\n",
    "\n",
    "print(f\"Time taken for add ID:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50016a50",
   "metadata": {},
   "source": [
    "Verify the result. From the output, we can see that the `id` value has been changed to `TH_wiki-0000000000` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27a634e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"filename\":\"thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl\",\"id\":\"TH_wiki-0000000000\",\"language\":\"TH\",\"source_id\":\"thwiki-20240201-thwiki-20240201-pages-articles-multistream.xml.bz2\",\"text\":\"–\\n\\nป้ายบอกทาง \\n ศาลาประชาคม – กระดานข่าว โครงการ ทรัพยากรและกิจกรรมซึ่งครอบคลุมวิกิพีเดียอย่างกว้างขวาง\\n แผนกช่วยเหลือ – ถามข้อสงสัยเกี่ยวกับการใช้งานวิกิพีเดีย\\n ปุจฉา-วิสัชนา – ถามข้อสงสัยทั่วไปที่คุณอยากรู้\\n ข่าวไซต์ – ประกาศ อัพเดต บทความและข้อมูลข่าวเกี่ยวกับวิกิพีเดียและมูลนิธิวิกิมีเดีย\\n สภากาแฟ – สำหรับอภิปรายเกี่ยวกับวิกิพีเดีย รวมถึงรายงานปัญหาเทคนิคและเสนอนโยบาย\\n Local Embassy – For Wikipedia-related discussion in languages other than Thai.\\n สร้างบทความใหม่ – บทช่วยสอนสำหรับเตรียมพร้อมสร้างบทความแรกของคุณ\\n\\nภาษาอื่น \\n\\n \",\"title\":\"หน้าหลัก\",\"url\":\"https:\\/\\/th.wikipedia.org\\/wiki\\/%E0%B8%AB%E0%B8%99%E0%B9%89%E0%B8%B2%E0%B8%AB%E0%B8%A5%E0%B8%B1%E0%B8%81\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_jsonl_file(added_id_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7084fed",
   "metadata": {},
   "source": [
    "Close Dask cluster. This cell needs to be run as we are starting a new GPU Dask cluster in the following task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16399469",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb227709",
   "metadata": {},
   "source": [
    "## 4.Exact Dedplication\n",
    "\n",
    "In exact deduplication, the document text is hashed into unique string using certain hashing algorithm, such as 'md5'. The documents with exact hashed values are having identical text. We will output the `ID` of duplicated documents for removal later. The function used is `ExactDuplicates()`. Arguments for this function include:\n",
    "- `id_field`: Key in input file for identifying document ID\n",
    "- `text_field`: Key in input file which contains document text.\n",
    "- `hash_method`: Hashing algorithm used. Default is `md5`\n",
    "- `cache_dir`: If specified, the duplicated document IDs will be output to the `cache_dir`. Otherwise, the IDs will not be saved\n",
    "\n",
    "Also, we are going to use GPU dask cluster to accelerate computation for deduplication (both exact and fuzzy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fa6c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modules import ExactDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70fd06",
   "metadata": {},
   "source": [
    "Start a GPU based Dask cluster. Since GPU based Dask cluster involves setting several arguments, we will use the `get_client()` wrapper function to quickly set up. Please make sure the `device` in `args` is `gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e9530f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(scheduler_address=None, scheduler_file=None, n_workers=20, threads_per_worker=1, rmm_pool_size=None, protocol='tcp', nvlink_only=False, files_per_partition=2, num_files=-1, device='gpu', set_torch_to_use_rmm=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv=['','--device','gpu']\n",
    "parser = argparse.ArgumentParser()\n",
    "args = attach_args(parser).parse_args()\n",
    "args.set_torch_to_use_rmm = False\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f71ab145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dask worker:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:37795': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = get_client(args, args.device)\n",
    "print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef57149",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26e6927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "exact_dedup_input_dataset_dir = added_id_output_path\n",
    "\n",
    "#Output\n",
    "exact_dedup_base_output_path = os.path.join(data_dir,\"exact_dedup\")\n",
    "exact_dedup_log_dir = os.path.join(exact_dedup_base_output_path,'log')\n",
    "exact_dedup_output_dir = os.path.join(exact_dedup_base_output_path,'data')\n",
    "\n",
    "#Parameters for ExactDuplicates()\n",
    "exact_dedup_dataset_id_field = \"id\"\n",
    "exact_dedup_dataset_text_field = \"text\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9a75a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {exact_dedup_log_dir}\n",
    "!mkdir -p {exact_dedup_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc0bd2",
   "metadata": {},
   "source": [
    "Apply exact deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daf8f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "Number of exact duplicated file:53\n",
      "Time taken for exact duplicate:3.0404415130615234\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# Read input dataset\n",
    "input_dataset = DocumentDataset.read_json(exact_dedup_input_dataset_dir, backend='cudf')\n",
    "\n",
    "#Run exact deduplication to the input\n",
    "exact_dup = ExactDuplicates(\n",
    "    logger=exact_dedup_log_dir,\n",
    "    id_field=exact_dedup_dataset_id_field,\n",
    "    text_field=exact_dedup_dataset_text_field,\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=exact_dedup_output_dir #Duplicated document ID list is output to the cache_dir\n",
    ")\n",
    "duplicates = exact_dup(dataset=input_dataset)\n",
    "\n",
    "print(f\"Number of exact duplicated file:{len(duplicates)}\")\n",
    "\n",
    "print(f\"Time taken for exact duplicate:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c60e4",
   "metadata": {},
   "source": [
    "Verify the output duplicated ID. We can group by the `_hashes` to get the list of duplicated documents having the same _hashes and use `extract_lines_with_id()` to verify that those documents are indeed exact duplicates. Please note that the `id` might changes, therefore, please replace the `target_list` when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f3c67f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exact duplicated document:53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>_hashes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TH_wiki-0000021211</td>\n",
       "      <td>1708cb56ec582f78716f0864dca9382d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TH_wiki-0000021213</td>\n",
       "      <td>1708cb56ec582f78716f0864dca9382d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TH_wiki-0000105191</td>\n",
       "      <td>e77a248506ef16737288fae5759db33a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TH_wiki-0000105192</td>\n",
       "      <td>2e386f5c3af70f43874618988d4842b2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TH_wiki-0000105193</td>\n",
       "      <td>2e386f5c3af70f43874618988d4842b2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                           _hashes\n",
       "0  TH_wiki-0000021211  1708cb56ec582f78716f0864dca9382d\n",
       "1  TH_wiki-0000021213  1708cb56ec582f78716f0864dca9382d\n",
       "2  TH_wiki-0000105191  e77a248506ef16737288fae5759db33a\n",
       "3  TH_wiki-0000105192  2e386f5c3af70f43874618988d4842b2\n",
       "4  TH_wiki-0000105193  2e386f5c3af70f43874618988d4842b2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_dedup_res = pd.read_parquet(os.path.join(exact_dedup_output_dir,\"_exact_duplicates.parquet\"))\n",
    "print(f\"Number of exact duplicated document:{len(exact_dedup_res)}\")\n",
    "exact_dedup_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ed7d4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_hashes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b908a91cdf0544c1ef3015cff4ee07e</td>\n",
       "      <td>TH_wiki-0000157216 TH_wiki-0000066307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15f35c239b6579b4642f7656e64576ac</td>\n",
       "      <td>TH_wiki-0000098621 TH_wiki-0000074714 TH_wiki-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1708cb56ec582f78716f0864dca9382d</td>\n",
       "      <td>TH_wiki-0000021211 TH_wiki-0000021213 TH_wiki-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2e386f5c3af70f43874618988d4842b2</td>\n",
       "      <td>TH_wiki-0000105192 TH_wiki-0000105193 TH_wiki-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3e6e96a80410d5a191d098f464e66f86</td>\n",
       "      <td>TH_wiki-0000122055 TH_wiki-0000116550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _hashes  \\\n",
       "0  0b908a91cdf0544c1ef3015cff4ee07e   \n",
       "1  15f35c239b6579b4642f7656e64576ac   \n",
       "2  1708cb56ec582f78716f0864dca9382d   \n",
       "3  2e386f5c3af70f43874618988d4842b2   \n",
       "4  3e6e96a80410d5a191d098f464e66f86   \n",
       "\n",
       "                                                  id  \n",
       "0              TH_wiki-0000157216 TH_wiki-0000066307  \n",
       "1  TH_wiki-0000098621 TH_wiki-0000074714 TH_wiki-...  \n",
       "2  TH_wiki-0000021211 TH_wiki-0000021213 TH_wiki-...  \n",
       "3  TH_wiki-0000105192 TH_wiki-0000105193 TH_wiki-...  \n",
       "4              TH_wiki-0000122055 TH_wiki-0000116550  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_dedup_res.groupby('_hashes')['id'].agg(lambda x: ' '.join(x)).reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3051ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl', 'id': 'TH_wiki-0000066307', 'language': 'TH', 'source_id': 'thwiki-20240201-thwiki-20240201-pages-articles-multistream.xml.bz2', 'text': '\\n\\nแหล่งข้อมูลอื่น \\n\\nสงขลา\\n \\nรายชื่อเกี่ยวกับจังหวัดสงขลา', 'title': 'รายชื่อโบราณสถานในจังหวัดสงขลา', 'url': 'https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B9%82%E0%B8%9A%E0%B8%A3%E0%B8%B2%E0%B8%93%E0%B8%AA%E0%B8%96%E0%B8%B2%E0%B8%99%E0%B9%83%E0%B8%99%E0%B8%88%E0%B8%B1%E0%B8%87%E0%B8%AB%E0%B8%A7%E0%B8%B1%E0%B8%94%E0%B8%AA%E0%B8%87%E0%B8%82%E0%B8%A5%E0%B8%B2'}\n",
      "{'filename': 'thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl', 'id': 'TH_wiki-0000157216', 'language': 'TH', 'source_id': 'thwiki-20240201-thwiki-20240201-pages-articles-multistream.xml.bz2', 'text': '\\n\\nแหล่งข้อมูลอื่น \\n\\nสงขลา\\n \\nรายชื่อเกี่ยวกับจังหวัดสงขลา', 'title': 'รายชื่อโบราณสถานในจังหวัดสงขลา (อำเภอเมืองสงขลาและสิงหนคร)', 'url': 'https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B9%82%E0%B8%9A%E0%B8%A3%E0%B8%B2%E0%B8%93%E0%B8%AA%E0%B8%96%E0%B8%B2%E0%B8%99%E0%B9%83%E0%B8%99%E0%B8%88%E0%B8%B1%E0%B8%87%E0%B8%AB%E0%B8%A7%E0%B8%B1%E0%B8%94%E0%B8%AA%E0%B8%87%E0%B8%82%E0%B8%A5%E0%B8%B2%20%28%E0%B8%AD%E0%B8%B3%E0%B9%80%E0%B8%A0%E0%B8%AD%E0%B9%80%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%87%E0%B8%AA%E0%B8%87%E0%B8%82%E0%B8%A5%E0%B8%B2%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B8%AA%E0%B8%B4%E0%B8%87%E0%B8%AB%E0%B8%99%E0%B8%84%E0%B8%A3%29'}\n"
     ]
    }
   ],
   "source": [
    "target_list = ['TH_wiki-0000157216', 'TH_wiki-0000066307']\n",
    "for line in extract_lines_with_id(os.path.join(exact_dedup_input_dataset_dir,'thwiki-20240201-pages-articles-multistream.xml.bz2.jsonl'),target_list):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31440b",
   "metadata": {},
   "source": [
    "**[Optional]** You might choose to close Dask cluster here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ee05303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.cluster.close()\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e8540",
   "metadata": {},
   "source": [
    "## 5. Fuzzy Deduplication\n",
    "Fuzzy deduplication involves 5 intermediate steps to generate duplicates. Refer to https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html for details\n",
    "\n",
    "Fuzzy deduplication in this example is a GPU implementation of MinhashLSH algorithm. This algorithm measures similarity based on statistics but not semantic meanings of text. There are a few concepts to be introduced before heading into fuzzy deduplication.\n",
    "1. Jaccard similarity: Jaccard similarity is often used as a metric to calculate the similarity between two sets. It's calculated by dividing the number of common elements in the two sets (Intersection) by the number of total unique elements in the two sets (Union). In the case of text documents, we transform a document into a set of n-grams. If two documents share a large amount of n-grams, most likely the documents are similar. \n",
    "\n",
    "    ![alt text](./image/jaccard.png )\n",
    "\n",
    "2. Complexity of the problem: To find all the similar document pairs in a dataset, we need to compute pair-wise Jaccard similarity across the dataset. Hence, making the complexity $O(N^2)$\n",
    "\n",
    "The MinhashLSH algorithm is a technique for quickly estimating the similarity between sets, such as the similarity between documents represented as sets of shingles (n-grams). It's able to find out Jaccard similar pair in the corpus but in a much computational efficient way. This algorithm has following steps in a high-level:\n",
    "1. Compute minhash for each document\n",
    "2. Run Locality Sensitive Hashing (LSH) based on the minhash which further assign buckets to each document. Each documents will be assigned to multiple buckets. Documents within the same bucket are deemed to be similar.\n",
    "3. Run pair-wise Jaccard similarity within each buckets to remove false positive cases within the buckets\n",
    "4. Based on the Jaccard similarity, transform the similarity matrix to a graph ans run connected component algorithm. For a group of connected components in the graph, they are the final similar document groups and the IDs within each groups will be output for duplicate removal.\n",
    "More detailed explanation please refer to https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/cpudeduplication.html.\n",
    "\n",
    "For implementation of MinhahsLSH on GPU, there are 5 steps:\n",
    "1. Minhash computation\n",
    "2. Bucket computation\n",
    "3. Jaccard shuffle for load balancing in a distributed system\n",
    "4. Jaccard similarity computation\n",
    "5. Connected component "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b99c5e",
   "metadata": {},
   "source": [
    "**If there is not running Dask cluster, start a GPU Dask cluster here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "115ff2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33223': None}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sys.argv=['','--device','gpu']\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = attach_args(parser).parse_args()\n",
    "# args.set_torch_to_use_rmm = False\n",
    "\n",
    "# client = get_client(args, args.device)\n",
    "# get_num_workers(client)\n",
    "# client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979977d",
   "metadata": {},
   "source": [
    "### 5.1 Minhash\n",
    "\n",
    "Run `MinHash()` for this section. The output of a minhash is a parquet file which contains document ID and hashed value which is an array contains 260 32-bit integer data. To obtain such hashed values we need to go through the following steps:\n",
    "1. Generate a set of n-gram components of a document. For example, doc = `Nemo Curator is a data curation tool`, a 3-gram set of this document will be `['Nemo Curator is','Curator is a','is a data','a data curation','data curation tool']`\n",
    "2. Hashed each n-gram into numerical values\n",
    "3. Generate a random hash function $H_1()$ which will hash each numeric n-gram into a 32-bit integer and take the minimum integer to use as minhash value for $H_1()$\n",
    "4. Repeat step 2 and 3 with hash function $H_x()$ until desired minhash length is reached. Minhash value of each iteration will be append together to form the final minhash array. \n",
    "\n",
    "Arguments include:\n",
    "- `seed`:Random seed used for initializing the hash functions used to compute the MinHashes. It's advised to keep this value the same for different experiment for reproducibility\n",
    "- `num_hashes`:Length of each minhash array. Default is 260. Longer minhash length will have better estimate of actual Jaccard similarity, but require more computational power\n",
    "- `char_ngrams`:n-gram length\n",
    "- `use_64bit_hash`:Whether to use 64bit or 32bit hash function\n",
    "- `id_field`: Key in input file for identifying document ID\n",
    "- `text_field`: Key in input file which contains document text.\n",
    "- `cache_dir`: If specified, the intermediate result will be output to the `cache_dir`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9b2a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import MinHash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c152974",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "117a569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "minhash_data_path = added_id_output_path\n",
    "#Output\n",
    "minshah_base_output_path = os.path.join(data_dir,\"fuzzy/minhash\")\n",
    "minshah_log_dir = os.path.join(minshah_base_output_path,'log')\n",
    "minshah_output_dir = os.path.join(minshah_base_output_path,'data')\n",
    "#Specify dataset name\n",
    "dataset_name = 'TH_wikipedia'\n",
    "\n",
    "#Relevant parameters\n",
    "minhash_id_field = 'id'\n",
    "minhash_text_field = 'text'\n",
    "seed = 10\n",
    "minhash_length = 260\n",
    "char_ngram = 5\n",
    "use_64bit_hash = False\n",
    "files_per_partition = 2\n",
    "\n",
    "!mkdir -p {minshah_log_dir}\n",
    "!mkdir -p {minshah_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1ad41",
   "metadata": {},
   "source": [
    "Run MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a17954eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing minhashes for /nluo_data/NeMo-Curator/tutorials/single_node_tutorial/workspace/add_id/cleaned\n",
      "Reading 1 files\n",
      "Time taken for MinHash:7.543871879577637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print(f\"Computing minhashes for {minhash_data_path}\")\n",
    "\n",
    "# Load data. Only the [minhash_id_field, text_field] columns are needed\n",
    "files = get_all_files_paths_under(root=minhash_data_path, recurse_subdirectories=False)\n",
    "files = [f for f in files if f.endswith(\".jsonl\")]\n",
    "df = read_data(\n",
    "    files,\n",
    "    file_type=\"jsonl\",\n",
    "    backend=\"cudf\",\n",
    "    files_per_partition=files_per_partition,\n",
    "    add_filename=False,\n",
    ")[[minhash_id_field, minhash_text_field]]\n",
    "\n",
    "# Run MinHash() on input data\n",
    "minhasher = MinHash(\n",
    "    seed=seed,\n",
    "    num_hashes=minhash_length,\n",
    "    char_ngrams=char_ngram,\n",
    "    use_64bit_hash=use_64bit_hash,\n",
    "    logger=minshah_log_dir,\n",
    "    id_field=minhash_id_field,\n",
    "    text_field=minhash_text_field,\n",
    "    cache_dir=minshah_output_dir\n",
    ")\n",
    "res = minhasher(DocumentDataset(df)).df\n",
    "\n",
    "print(f\"Time taken for MinHash:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cddba5",
   "metadata": {},
   "source": [
    "Verify result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df83eec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>_minhash_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TH_wiki-0000000000</td>\n",
       "      <td>[11565725, 19782487, 9831980, 5480992, 2306475...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TH_wiki-0000000001</td>\n",
       "      <td>[407876, 107572, 824528, 346831, 216554, 10963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TH_wiki-0000000002</td>\n",
       "      <td>[727721, 694551, 233868, 346831, 216554, 77001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TH_wiki-0000000003</td>\n",
       "      <td>[1149282, 931656, 2515604, 1428622, 4964646, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TH_wiki-0000000004</td>\n",
       "      <td>[1559901, 11771639, 487706, 826569, 1203860, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                 _minhash_signature\n",
       "0  TH_wiki-0000000000  [11565725, 19782487, 9831980, 5480992, 2306475...\n",
       "1  TH_wiki-0000000001  [407876, 107572, 824528, 346831, 216554, 10963...\n",
       "2  TH_wiki-0000000002  [727721, 694551, 233868, 346831, 216554, 77001...\n",
       "3  TH_wiki-0000000003  [1149282, 931656, 2515604, 1428622, 4964646, 4...\n",
       "4  TH_wiki-0000000004  [1559901, 11771639, 487706, 826569, 1203860, 5..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minhash_res = pd.read_parquet(os.path.join(minshah_output_dir, \"_minhashes.parquet\"))\n",
    "minhash_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ab08a",
   "metadata": {},
   "source": [
    "### 5.2 LSH\n",
    "`LSH()` implements LSH algorithm which includes the following steps:\n",
    "1. Divide the minhash array into `X` different portions. \n",
    "2. For each portions, hash the minhash values into buckets. One document will be assigned to `X` buckets.\n",
    "3. Documents within the same bucket will be deemed similar. Since every document will be assigned `X` buckets and as long as two documents share 1 or more buckets they are deemed similar, the result of LSH will have more false positive as compared to false negative. The false positive cases will be filtered in following modules, namely jaccard compute.\n",
    "\n",
    "Arguments include:\n",
    "- `minhash_length`:Length of minhash signature. Must bu consistent with `MinHash()`\n",
    "- `num_buckets`: Number of buckets\n",
    "- `buckets_per_shuffle`: Number of buckets to shuffle concurrently\n",
    "- `id_field`: Key in input file for identifying document ID\n",
    "- `minhash_field`: Key in input file for identifying document MinHash signature \n",
    "- `cache_dir`:If specified, the intermediate result will be output to the `cache_dir`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "138544a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import LSH\n",
    "from nemo_curator.gpu_deduplication.jaccard_utils.doc_id_mapping import \\\n",
    "    convert_str_id_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178fd0e4",
   "metadata": {},
   "source": [
    "Define parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21d2a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "lsh_input_data_path = minshah_output_dir\n",
    "\n",
    "#Output\n",
    "lsh_base_output_path = os.path.join(data_dir,\"fuzzy/lsh\")\n",
    "lsh_log_dir = os.path.join(lsh_base_output_path,'log')\n",
    "lsh_output_dir = os.path.join(lsh_base_output_path,'data')\n",
    "\n",
    "#Relevant parameters\n",
    "lsh_id_field = 'id'\n",
    "minhash_field = '_minhash_signature'\n",
    "minhash_length=260\n",
    "num_bands=20\n",
    "buckets_per_shuffle=1\n",
    "\n",
    "!mkdir -p {lsh_log_dir}\n",
    "!mkdir -p {lsh_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18708d2",
   "metadata": {},
   "source": [
    "Run LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9eebeb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LSH:20.533941984176636\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#Load MinHash output\n",
    "df = dask_cudf.read_parquet(lsh_input_data_path, blocksize=\"2GB\", aggregate_files=True, backend = \"cudf\")\n",
    "df = df.map_partitions(\n",
    "    convert_str_id_to_int,\n",
    "    id_column=lsh_id_field,\n",
    "    meta=cudf.DataFrame(\n",
    "        {minhash_field: [[1, 2, 3]], \"doc_id\": [1], \"dataset_id\": np.uint32(1)}\n",
    "    ),\n",
    ")\n",
    "\n",
    "#Run LSH()\n",
    "lsh = LSH(\n",
    "    cache_dir=lsh_output_dir,\n",
    "    minhash_length=minhash_length,\n",
    "    num_buckets=num_bands,\n",
    "    buckets_per_shuffle=buckets_per_shuffle,\n",
    "    id_fields=[\"dataset_id\", \"doc_id\"],\n",
    "    minhash_field=minhash_field,\n",
    "    logger=lsh_log_dir,\n",
    ")\n",
    "res = lsh(DocumentDataset(df))\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Time taken for LSH:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813603e2",
   "metadata": {},
   "source": [
    "Verify result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c47da6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>_bucket_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>124692</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>85282</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>156638</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>160566</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>160567</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id  doc_id  _bucket_id\n",
       "0  1692361878  124692          96\n",
       "1  1692361878   85282         385\n",
       "2  1692361878  156638         529\n",
       "3  1692361878  160566         540\n",
       "4  1692361878  160567         540"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_res = pd.read_parquet(os.path.join(lsh_output_dir, \"_buckets.parquet\"))\n",
    "lsh_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bade4a",
   "metadata": {},
   "source": [
    "### 5.3 Jaccard Shuffle\n",
    "In this section, we will be using `_MapBucket()` and `_Shuffle()`.\n",
    "\n",
    "For `_MapBucket()`, it is designed to take input text data in .jsonl format and bucket information which is output of LSH, map the documents to their respective buckets, and write the resulting DataFrame containing the anchor documents and their associated bucket information to a Parquet file.Arguments include:\n",
    "- `id_field`: Key in input .jsonl file for identifying document ID\n",
    "- `text_field`: Key in input .jsonl file which contains document text.\n",
    "- `bucket_field`: Key in input _buckets.parquet which contains `bucket_id`.\n",
    "- `num_anchors`: Number of anchors (document in the same buckets) to be output\n",
    "\n",
    "\n",
    "For `_Shuffle()`, it perform a shuffling operation on the documents based on their bucket assignments, output in .parquet format. This shuffling operation is a crucial step in the deduplication process, as it helps distribute similar documents across different partitions or workers, enabling efficient parallel processing and deduplication in subsequent steps. Arguments include:\n",
    "- `id_fields`: Columns in `_buckets.parquet` that maps to original `id` in .jsonl data file. In this example, it is `[\"dataset_id\", \"doc_id\"]`\n",
    "- `text_field`: Key in input .jsonl file which contains document text.\n",
    "- `int_to_str_id`:  Key in input .jsonl file for identifying document ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "565253ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.fuzzy_dedup_utils.io_utils import (\n",
    "    get_bucket_ddf_from_parquet_path,\n",
    "    get_text_ddf_from_json_path_with_blocksize,\n",
    ")\n",
    "from nemo_curator.modules.fuzzy_dedup import _MapBuckets,_Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70387977",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cff7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "input_data_paths = [minhash_data_path]\n",
    "input_bucket_path = lsh_output_dir\n",
    "\n",
    "#Output\n",
    "jaccard_shuffle_base_output_path = os.path.join(data_dir,\"fuzzy/jaccard_shuffle\")\n",
    "output_anchor_docs_with_bk_path = os.path.join(jaccard_shuffle_base_output_path, \"anchor_docs_with_bk.parquet\")\n",
    "input_anchor_docs_with_bk_dir = output_anchor_docs_with_bk_path\n",
    "output_shuffled_docs_path = os.path.join(jaccard_shuffle_base_output_path, \"shuffled_docs.parquet\")\n",
    "\n",
    "#Relevant parameter for _MapBucket()\n",
    "text_ddf_blocksize = 256\n",
    "bucket_mapping_ddf_blocksize = 256\n",
    "num_files = None\n",
    "shuffle_type ='tasks'\n",
    "input_bucket_field = '_bucket_id'\n",
    "input_id_field = 'id'\n",
    "input_text_field = 'text'\n",
    "\n",
    "#Relevant parameter for _Shuffle()\n",
    "shuffle_id_fields=[\"dataset_id\", \"doc_id\"]\n",
    "int_to_str_id='id'\n",
    "\n",
    "!mkdir -p {jaccard_shuffle_base_output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a53f1",
   "metadata": {},
   "source": [
    "Run Jaccard map bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a6e5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files being read for jaccard calculation = 1\n",
      "Number of ddf_bk partitions = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Bucket Mapping:2.1162023544311523 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "num_workers = get_num_workers(client)\n",
    "\n",
    "# Read .jsonl input data\n",
    "ddf_text = get_text_ddf_from_json_path_with_blocksize(\n",
    "    input_data_paths=input_data_paths,\n",
    "    num_files=num_files,\n",
    "    blocksize=text_ddf_blocksize,\n",
    "    id_column=input_id_field,\n",
    "    text_column=input_text_field,\n",
    ")\n",
    "# Read \"_buckets.parquet\"\n",
    "ddf_bk = get_bucket_ddf_from_parquet_path(input_bucket_path=input_bucket_path, num_workers=num_workers)\n",
    "\n",
    "#Run _MapBuckets()\n",
    "map_buckets = _MapBuckets(id_fields=shuffle_id_fields, bucket_field=input_bucket_field)\n",
    "ddf_anchor_docs_with_bk = map_buckets.map_buckets_with_anchors(documents_df=ddf_text, buckets_df=ddf_bk, shuffle_type=shuffle_type)\n",
    "\n",
    "#Write to disk\n",
    "ddf_anchor_docs_with_bk.to_parquet(output_anchor_docs_with_bk_path, write_index=False)\n",
    "\n",
    "print(f\"Time taken for Bucket Mapping:{time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96246266",
   "metadata": {},
   "source": [
    "Verify results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09e65f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>anchor_1_dataset_id</th>\n",
       "      <th>anchor_1_doc_id</th>\n",
       "      <th>anchor_0_dataset_id</th>\n",
       "      <th>anchor_0_doc_id</th>\n",
       "      <th>_output_partition_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>138220</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>145256</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>143672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>50509</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>50509</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>50457</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>93989</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>93846</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>93807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>20448</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>20090</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>20444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>93991</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>93927</td>\n",
       "      <td>1692361878</td>\n",
       "      <td>93697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id  doc_id  anchor_1_dataset_id  anchor_1_doc_id  \\\n",
       "0  1692361878  138220           1692361878           145256   \n",
       "1  1692361878   50509           1692361878            50509   \n",
       "2  1692361878   93989           1692361878            93846   \n",
       "3  1692361878   20448           1692361878            20090   \n",
       "4  1692361878   93991           1692361878            93927   \n",
       "\n",
       "   anchor_0_dataset_id  anchor_0_doc_id  _output_partition_id  \n",
       "0           1692361878           143672                     0  \n",
       "1           1692361878            50457                     0  \n",
       "2           1692361878            93807                     0  \n",
       "3           1692361878            20444                     0  \n",
       "4           1692361878            93697                     0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_bucket_res = pd.read_parquet(output_anchor_docs_with_bk_path)\n",
    "map_bucket_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb1e86",
   "metadata": {},
   "source": [
    "**[Optional]**Remove previous Jaccard Shuffle results. Run only when there are files under the Jaccard Shuffle output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da7dcc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r {output_shuffled_docs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c2b39d",
   "metadata": {},
   "source": [
    "Run Jaccard Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9dcf646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started processing bucket-map partitions 0 through 1 of 1\n",
      "Using 1 text partitions.\n",
      "Starting text bytes aware shuffle\n",
      "Will write 30596 rows to disk\n",
      "Text-df partition  1/1 completed in 3.394432544708252\n",
      "Bucket partition  1/1 completed in 3.4057791233062744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Jaccard Shuffle = 3.4692487716674805 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#Run _Shuffle() on results of _MapBucket()\n",
    "shuffle = _Shuffle(\n",
    "    id_fields=shuffle_id_fields,\n",
    "    text_field=input_text_field,\n",
    "    int_to_str_id=int_to_str_id\n",
    ")\n",
    "shuffle.shuffle_docs_on_buckets(\n",
    "    documents_df=ddf_text,\n",
    "    bucket_w_anchors_path=input_anchor_docs_with_bk_dir,\n",
    "    output_shuffled_docs_path=output_shuffled_docs_path,\n",
    "    bucket_mapping_df_blocksize=bucket_mapping_ddf_blocksize,\n",
    "#     parts_per_worker=1,\n",
    "#     bucket_parts_per_worker=8,\n",
    "    partition_on=\"_output_partition_id\",\n",
    ")\n",
    "\n",
    "print(f\"Time taken for Jaccard Shuffle = {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f2177",
   "metadata": {},
   "source": [
    "Verify results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf44fb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>_text_bytes</th>\n",
       "      <th>id</th>\n",
       "      <th>anchor_0_id</th>\n",
       "      <th>anchor_1_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>พุทธศักราช 676 ใกล้เคียงกับ\\n เมษายน ค.ศ. 133 ...</td>\n",
       "      <td>263</td>\n",
       "      <td>1692361878-7032</td>\n",
       "      <td>1692361878-7032</td>\n",
       "      <td>1692361878-7052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>พุทธศักราช 41 ใกล้เคียงกับ ก่อน คริสต์ศักราช 5...</td>\n",
       "      <td>217</td>\n",
       "      <td>1692361878-9082</td>\n",
       "      <td>1692361878-8805</td>\n",
       "      <td>1692361878-9071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>พุทธศักราช 41 ใกล้เคียงกับ ก่อน คริสต์ศักราช 5...</td>\n",
       "      <td>217</td>\n",
       "      <td>1692361878-9082</td>\n",
       "      <td>1692361878-9028</td>\n",
       "      <td>1692361878-9045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>พุทธศักราช 41 ใกล้เคียงกับ ก่อน คริสต์ศักราช 5...</td>\n",
       "      <td>217</td>\n",
       "      <td>1692361878-9082</td>\n",
       "      <td>1692361878-9072</td>\n",
       "      <td>1692361878-9082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ประเทศฮังการี เข้าร่วมแข่งขันกีฬาโอลิมปิกฤดูร้...</td>\n",
       "      <td>2039</td>\n",
       "      <td>1692361878-49091</td>\n",
       "      <td>1692361878-49093</td>\n",
       "      <td>1692361878-49087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  _text_bytes  \\\n",
       "0  พุทธศักราช 676 ใกล้เคียงกับ\\n เมษายน ค.ศ. 133 ...          263   \n",
       "1  พุทธศักราช 41 ใกล้เคียงกับ ก่อน คริสต์ศักราช 5...          217   \n",
       "2  พุทธศักราช 41 ใกล้เคียงกับ ก่อน คริสต์ศักราช 5...          217   \n",
       "3  พุทธศักราช 41 ใกล้เคียงกับ ก่อน คริสต์ศักราช 5...          217   \n",
       "4  ประเทศฮังการี เข้าร่วมแข่งขันกีฬาโอลิมปิกฤดูร้...         2039   \n",
       "\n",
       "                 id       anchor_0_id       anchor_1_id  \n",
       "0   1692361878-7032   1692361878-7032   1692361878-7052  \n",
       "1   1692361878-9082   1692361878-8805   1692361878-9071  \n",
       "2   1692361878-9082   1692361878-9028   1692361878-9045  \n",
       "3   1692361878-9082   1692361878-9072   1692361878-9082  \n",
       "4  1692361878-49091  1692361878-49093  1692361878-49087  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_shuffle_res = pd.read_parquet(os.path.join(output_shuffled_docs_path,\"_output_partition_id=0/batch_1_1.parquet\"))\n",
    "jaccard_shuffle_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb70238",
   "metadata": {},
   "source": [
    "### 5.4 Jaccard Compute\n",
    "We will be using `JaccardSimilarity()`.This is to computes the Jaccard similarity between document pairs. Result is a parquet dataset consisting of document id pair along with their Jaccard similarity score. To compute Jaccard similarity between two documents, we first convert the document into sets of n-grams and then compute the Jaccard similarity of the two sets.\n",
    "\n",
    "Arguments include:\n",
    "- `id_field`: Column in input .parquet file identifying document ID\n",
    "- `text_field`: Column in input .parquet file identifying document text\n",
    "- `anchor_id_fields`: Column in input .parquet file identifying anchors. This can be generated by specifying number of anchor used in `_MapBucket` whose default value is 2\n",
    "- `ngram_width`: n-gram used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06346b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modules.fuzzy_dedup import JaccardSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f440f",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "457ae138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "shuffled_docs_path = output_shuffled_docs_path\n",
    "\n",
    "#Output\n",
    "jaccard_compute_base_output_path = os.path.join(data_dir,\"fuzzy/jaccard_compute\")\n",
    "jaccard_compute_output_results_path = os.path.join(jaccard_compute_base_output_path, \"jaccard_similarity_results.parquet\")\n",
    "\n",
    "#Relevant parameters\n",
    "input_id_field = 'id'\n",
    "input_text_field = 'text'\n",
    "ngram_size = 5\n",
    "num_anchors = 2\n",
    "\n",
    "!mkdir -p {jaccard_compute_base_output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619bf820",
   "metadata": {},
   "source": [
    "Run Jaccard Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f094db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running jaccard compute script\n",
      "Time taken for Jaccard Computing: 0.8689384460449219\n"
     ]
    }
   ],
   "source": [
    "enable_spilling()\n",
    "client.run(enable_spilling)\n",
    "\n",
    "print(\"Running jaccard compute script\", flush=True)\n",
    "t0 = time.time()\n",
    "\n",
    "jaccard = JaccardSimilarity(\n",
    "    id_field=input_id_field,\n",
    "    text_field=input_text_field,\n",
    "    anchor_id_fields=[f\"anchor_{i}_{input_id_field}\" for i in range(num_anchors)],\n",
    "    ngram_width=ngram_size,\n",
    ")\n",
    "\n",
    "#Load and run Jaccard compute\n",
    "result_df = jaccard.jaccard_compute(shuffled_docs_path)\n",
    "\n",
    "result_df.to_parquet(jaccard_compute_output_results_path, write_index=False, write_metadata_file=False)\n",
    "\n",
    "print(f\"Time taken for Jaccard Computing: {time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e619c",
   "metadata": {},
   "source": [
    "Verify output. You might see that there are repeated `id_x` and `id_y` pairs. This is expected as a pair of similar documents is likely to share numerous same buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae2efe3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_x</th>\n",
       "      <th>id_y</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692361878-127521</td>\n",
       "      <td>1692361878-127517</td>\n",
       "      <td>0.755481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1692361878-127521</td>\n",
       "      <td>1692361878-127517</td>\n",
       "      <td>0.755481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1692361878-45934</td>\n",
       "      <td>1692361878-45940</td>\n",
       "      <td>0.922061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1692361878-45934</td>\n",
       "      <td>1692361878-45940</td>\n",
       "      <td>0.922061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1692361878-45934</td>\n",
       "      <td>1692361878-45940</td>\n",
       "      <td>0.922061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_x               id_y   jaccard\n",
       "0  1692361878-127521  1692361878-127517  0.755481\n",
       "1  1692361878-127521  1692361878-127517  0.755481\n",
       "2   1692361878-45934   1692361878-45940  0.922061\n",
       "3   1692361878-45934   1692361878-45940  0.922061\n",
       "4   1692361878-45934   1692361878-45940  0.922061"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_compute_res = pd.read_parquet(jaccard_compute_output_results_path)\n",
    "jaccard_compute_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f1831",
   "metadata": {},
   "source": [
    "### 5.5 Connected Components\n",
    "This section uses `ConnectedComponents()`.This section takes a dataset consisting of document pairs and their corresponding jaccard similarity to construct a non-directed graph. A edge will be form between documents whose Jaccard similarity is higher than the threshold (0.8 in this example). It will then identify the connected components in this graph. Documents within the same connected components are deemed duplicated\n",
    "\n",
    "Arguments include:\n",
    "- `cache_dir`:Output path for intermediate results\n",
    "- `jaccard_pairs_path`:Input path for `jaccard_similarity_results.parquet`\n",
    "- `id_column`:prefix of ID column in `jaccard_similarity_results.parquet`\n",
    "- `jaccard_threshold`:Threshold to determine if an edge exists between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5756fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modules.fuzzy_dedup import ConnectedComponents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217957d6",
   "metadata": {},
   "source": [
    "Define parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72a1952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "jaccard_pairs_path = jaccard_compute_output_results_path\n",
    "\n",
    "#Output\n",
    "connected_component_base_output_path = os.path.join(data_dir,\"fuzzy/cc\")\n",
    "connected_component_output_path = os.path.join(connected_component_base_output_path, \"connected_components.parquet\")\n",
    "connected_component_cache_dir = os.path.join(connected_component_base_output_path, \"cache\")\n",
    "\n",
    "#Relevant parameter\n",
    "input_id_field = 'id'\n",
    "jaccard_threshold = 0.8\n",
    "\n",
    "!mkdir -p {connected_component_base_output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b3a8c",
   "metadata": {},
   "source": [
    "Run Connected Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46578e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = 0/1, time = 0.3100006580352783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/io/parquet/core.py:421: FutureWarning: The `aggregate_files` argument will be deprecated in the future. Please consider using `from_map` to create a DataFrame collection with a custom file-to-partition mapping.\n",
      "\n",
      "If you strongly oppose the deprecation of `aggregate_files`, please comment at https://github.com/dask/dask/issues/9051\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of groups 5465\n",
      "# of docs removed 3079\n",
      "assert num_nodes:8544==labels_df:8544 passed\n",
      "Time taken for Connected Component: 11.238884925842285 s\n"
     ]
    }
   ],
   "source": [
    "client.run(enable_spilling)\n",
    "\n",
    "t0 = time.time()\n",
    "    \n",
    "components_stage = ConnectedComponents(\n",
    "    cache_dir=connected_component_cache_dir,\n",
    "    jaccard_pairs_path=jaccard_pairs_path,\n",
    "    id_column=input_id_field,\n",
    "    convert_str_ids=True,\n",
    "    jaccard_threshold=jaccard_threshold,\n",
    ")\n",
    "\n",
    "#Load and run connected component\n",
    "components_stage.cc_workflow(output_path=connected_component_output_path)\n",
    "print(f\"Time taken for Connected Component: {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827158e",
   "metadata": {},
   "source": [
    "Verify the result of `Connected Components`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bcfc470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>136999</td>\n",
       "      <td>3837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>85318</td>\n",
       "      <td>3838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>70670</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>134587</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>136125</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id  doc_id  group\n",
       "0  1692361878  136999   3837\n",
       "1  1692361878   85318   3838\n",
       "2  1692361878   70670   1196\n",
       "3  1692361878  134587    138\n",
       "4  1692361878  136125   1320"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_compute_res = pd.read_parquet(connected_component_output_path)\n",
    "cc_compute_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ee07d",
   "metadata": {},
   "source": [
    "Let's check if the output fuzzy duplicated documents within the same group are similar. Please note that the `group` id in your output might be different from the notebook output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1f10a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121</td>\n",
       "      <td>134756, 134762, 134748, 134742, 134740, 134750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "      <td>134587, 134908, 135024, 135029, 135019, 134566...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>323</td>\n",
       "      <td>134794, 134780, 134793, 134785, 134798, 134781...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>344</td>\n",
       "      <td>136092, 136103, 136090, 136093, 136100, 136089...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>428</td>\n",
       "      <td>94120, 94084, 94059, 94128, 94130, 94056, 9413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>8539</td>\n",
       "      <td>125651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>8540</td>\n",
       "      <td>125971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>8541</td>\n",
       "      <td>84926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>8542</td>\n",
       "      <td>40115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>8543</td>\n",
       "      <td>50282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5465 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      group                                             doc_id\n",
       "0       121  134756, 134762, 134748, 134742, 134740, 134750...\n",
       "1       138  134587, 134908, 135024, 135029, 135019, 134566...\n",
       "2       323  134794, 134780, 134793, 134785, 134798, 134781...\n",
       "3       344  136092, 136103, 136090, 136093, 136100, 136089...\n",
       "4       428  94120, 94084, 94059, 94128, 94130, 94056, 9413...\n",
       "...     ...                                                ...\n",
       "5460   8539                                             125651\n",
       "5461   8540                                             125971\n",
       "5462   8541                                              84926\n",
       "5463   8542                                              40115\n",
       "5464   8543                                              50282\n",
       "\n",
       "[5465 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_compute_res['doc_id'] = cc_compute_res['doc_id'].astype(str)\n",
    "cc_compute_res.groupby('group')['doc_id'].agg(lambda x: ', '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621c2cb",
   "metadata": {},
   "source": [
    "Change the `group` number if necessary. By running the code below, we can obtain a list of near duplicated documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd79a7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>121545</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>121487</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>121541</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>121539</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>1692361878</td>\n",
       "      <td>121524</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset_id  doc_id  group\n",
       "14   1692361878  121545    735\n",
       "66   1692361878  121487    735\n",
       "213  1692361878  121541    735\n",
       "291  1692361878  121539    735\n",
       "422  1692361878  121524    735"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_compute_res[cc_compute_res['group']==735].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c02f4b",
   "metadata": {},
   "source": [
    "Print the text of near duplicated document. Please replace the `id` if necessary, `id` should be in the format of `<dataset_id>_<doc_id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd0b2e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ประเทศสวิตเซอร์แลนด์ ได้เข้าร่วมแข่งขันกีฬาโอลิมปิกเยาวชนฤดูหนาว ครั้งที่ 3 ค.ศ. 2020 (พ.ศ. 2563) ณ เมืองโลซาน ประเทศสวิตเซอร์แลนด์ ระหว่างวันที่ 9 - 22 มกราคม พ.ศ. 2563 คณะกรรมการโอลิมปิกแห่งชาติสวิตเซอร์แลนด์ได้ส่งทีมนักกีฬาเข้าแข่งขันทั้งหมด 56 คน แบ่งเป็นเป็นชาย 32 คนและหญิง 56 คน เข้าร่วมการแข่งขันใน 15 ชนิดกีฬา\\n\\nจำนวนผู้เข้าแข่งขัน\\n\\nผลการแข่งขัน\\n\\nสเกตลีลา\\n\\nสเกตความเร็ว\\n\\nสเกตความเร็วระยะสั้น\\n\\nฮอกกี้น้ำแข็ง\\n\\nเคอร์ลิง\\n\\nสกีลงเขา\\n\\nสกีข้ามทุ่ง\\n\\nสกีกระโดดไกล\\n\\nสกีนอร์ดิกผสม\\n\\nสกีลีลา\\n\\nสกีปีนเขา\\n\\nสโนว์บอร์ด\\n\\nทวิกีฬาฤดูหนาว\\n\\nบอบสเล\\n\\nสเกเลตัน\\n\\nอ้างอิง\\n\\nแหล่งข้อมูลอื่น \\n เว็บไซต์อย่างเป็นทางการ \\n\\nประเทศสวิตเซอร์แลนด์ในโอลิมปิกเยาวชน\\nประเทศที่เข้าร่วมแข่งขันโอลิมปิกเยาวชนฤดูหนาว 2020',\n",
       "       'ประเทศบัลแกเรีย ได้เข้าร่วมแข่งขันกีฬาโอลิมปิกเยาวชนฤดูหนาว ครั้งที่ 3 ค.ศ. 2020 (พ.ศ. 2563) ณ เมืองโลซาน ประเทศสวิตเซอร์แลนด์ ระหว่างวันที่ 9 - 22 มกราคม พ.ศ. 2563 คณะกรรมการโอลิมปิกแห่งชาติบัลแกเรียได้ส่งทีมนักกีฬาเข้าแข่งขันทั้งหมด 18 คน แบ่งเป็นเป็นชาย 11 คนและหญิง 7 คน เข้าร่วมการแข่งขันใน 8 ชนิดกีฬา\\n\\nจำนวนผู้เข้าแข่งขัน\\n\\nผลการแข่งขัน\\n\\nสเกตลีลา\\n\\nสเกตความเร็ว\\n\\nสเกตความเร็วระยะสั้น\\n\\nฮอกกี้น้ำแข็ง\\n\\nเคอร์ลิง\\n\\nสกีลงเขา\\n\\nสกีข้ามทุ่ง\\n\\nสกีกระโดดไกล\\n\\nสกีนอร์ดิกผสม\\n\\nสกีลีลา\\n\\nสกีปีนเขา\\n\\nสโนว์บอร์ด\\n\\nทวิกีฬาฤดูหนาว\\n\\nลูช\\n\\nบอบสเล\\n\\nสเกเลตัน\\n\\nอ้างอิง\\n\\nแหล่งข้อมูลอื่น \\n เว็บไซต์อย่างเป็นทางการ \\n\\nประเทศบัลแกเรียในโอลิมปิกเยาวชน\\nประเทศที่เข้าร่วมแข่งขันโอลิมปิกเยาวชนฤดูหนาว 2020'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_shuffle_res[jaccard_shuffle_res['id'].isin(['1692361878-121545','1692361878-121487'])]['text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8d12f",
   "metadata": {},
   "source": [
    "Below is the English translation of the output above. We can see that the two documents are indeed very similar to each other.\n",
    "- `Text 1`:\n",
    "```\n",
    "Switzerland participated in the 3rd Youth Olympic Winter Games in 2020 (B.E. 2563) in Lausanne, Switzerland from January 9 - 22, 2563. The Swiss Olympic Committee sent a total of 56 athletes, consisting of 32 men and 56 women, to compete in 15 sports.\n",
    "Number of Competitors:\n",
    "Competition Results:\n",
    "Figure Skating\n",
    "Speed Skating\n",
    "Short Track Speed Skating\n",
    "Ice Hockey\n",
    "Curling\n",
    "Alpine Skiing\n",
    "Cross-Country Skiing\n",
    "Ski Jumping\n",
    "Nordic Combined\n",
    "Freestyle Skiing\n",
    "Ski Mountaineering\n",
    "Snowboard\n",
    "Biathlon\n",
    "Bobsleigh\n",
    "Skeleton\n",
    "References:\n",
    "Other Resources:\n",
    "Official Website\n",
    "Switzerland at the Youth Olympics\n",
    "Countries at the 2020 Youth Winter Olympics\n",
    "```\n",
    "- `Text 2`:\n",
    "```\n",
    "Bulgaria participated in the 3rd Youth Olympic Winter Games in 2020 (B.E. 2563) in Lausanne, Switzerland from January 9 - 22, 2563. The Bulgarian Olympic Committee sent a total of 18 athletes, consisting of 11 men and 7 women, to compete in 8 sports.\n",
    "Number of Competitors:\n",
    "Competition Results:\n",
    "Figure Skating\n",
    "Speed Skating\n",
    "Short Track Speed Skating\n",
    "Ice Hockey\n",
    "Curling\n",
    "Alpine Skiing\n",
    "Cross-Country Skiing\n",
    "Ski Jumping\n",
    "Nordic Combined\n",
    "Freestyle Skiing\n",
    "Ski Mountaineering\n",
    "Snowboard\n",
    "Biathlon\n",
    "Luge\n",
    "Bobsleigh\n",
    "Skeleton\n",
    "References:\n",
    "Other Resources:\n",
    "Official Website\n",
    "Bulgaria at the Youth Olympics\n",
    "Countries at the 2020 Youth Winter Olympics\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca66df",
   "metadata": {},
   "source": [
    "## 6. Remove duplicates\n",
    "\n",
    "Now we have duplicated document IDs output by both exact deduplication and fuzzy deduplication. We will run this section to remove those documents. This is done be loading the output .parquet files and the unicode fixed input dataset in .jsonl as DataFrame. Then use DataFrame operation to remove the duplicated documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d031ec",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "911be9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "dataset_dir = added_id_output_path\n",
    "\n",
    "#Output\n",
    "dudped_output_dir = os.path.join(data_dir,\"remove_duplicate/result.parquet\")\n",
    "\n",
    "#Relevant parameter\n",
    "input_id_field = 'id'\n",
    "id_prefix = add_ID_id_prefix\n",
    "\n",
    "!mkdir -p {dudped_output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969f6543",
   "metadata": {},
   "source": [
    "We will first process the result of exact deduplication. Since result of exact deduplication contains original ID used in input dataset, it is more straightforward to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bbbfdbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "Reading 1 files\n"
     ]
    }
   ],
   "source": [
    "#Load .jsonl dataset\n",
    "input_dataset = DocumentDataset.read_json(dataset_dir, backend='cudf')\n",
    "\n",
    "#Load exact deduplicate result and extract list of duplicated document ID\n",
    "exact_duplicates = DocumentDataset.read_parquet(os.path.join(exact_dedup_output_dir,\"_exact_duplicates.parquet\"), backend='cudf')\n",
    "exact_docs_to_remove = exact_duplicates.df.map_partitions(\n",
    "    lambda x: x[x._hashes.duplicated(keep=\"first\")]\n",
    ")\n",
    "\n",
    "#Remove the duplicated document from input dataset\n",
    "result = input_dataset.df[\n",
    "    ~input_dataset.df[input_id_field].isin(exact_docs_to_remove[input_id_field].compute())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97567d",
   "metadata": {},
   "source": [
    "For result of fuzzy deduplication, we need to first reconstructed document ID by combining `dataset_id` and `doc_id`, then use the reconstructed `ID` for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "513cf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of id_prefix used in Add ID\n",
    "base_ids = [id_prefix]\n",
    "\n",
    "#Obtain a mapping between `dataset_id` and `id_prefix`\n",
    "df = cudf.DataFrame()\n",
    "df['base_id'] = [base_id for base_id in base_ids]\n",
    "df['dataset_id'] = df['base_id'].hash_values()\n",
    "df_pd = df.to_pandas()\n",
    "mapping = {\n",
    "      hashed_id: base_id\n",
    "      for base_id, hashed_id in zip(df_pd['base_id'], df_pd['dataset_id'])\n",
    "}\n",
    "\n",
    "#Load result of fuzzy deduplication\n",
    "fuzzy_duplicates = pd.read_parquet(connected_component_output_path)\n",
    "#Reconstruct the original document ID\n",
    "fuzzy_duplicates['id']=fuzzy_duplicates.apply(lambda x: f\"{mapping[x['dataset_id']]}-{x['doc_id']:010d}\", axis=1)\n",
    "#Generate list of near duplicate document ID\n",
    "fuzzy_docs_to_remove = fuzzy_duplicates.drop_duplicates(subset=['group'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc7d647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "#Remove near duplicates\n",
    "result = result[~result[input_id_field].isin(fuzzy_docs_to_remove[input_id_field])]\n",
    "\n",
    "#Save final result to local\n",
    "write_to_disk(result, dudped_output_dir, output_type=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a967f",
   "metadata": {},
   "source": [
    "Verify the result of duplicate removal. We can see that the number of document in resultant document is less than the original dataset (length = 161748)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e8097b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of duplicate removed dataset:156257\n"
     ]
    }
   ],
   "source": [
    "res = pd.read_parquet(dudped_output_dir)\n",
    "print(f\"Length of duplicate removed dataset:{len(res)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caf66f",
   "metadata": {},
   "source": [
    "Close the GPU Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd91f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6cee97",
   "metadata": {},
   "source": [
    "## 7. Heuristic Fitlering\n",
    "\n",
    "In this section, we will apply multiple heuristic filters to the dataset, record the heuristic score for documents and documents removed for each filter. For each heuristic filter, the filter calculates a quality scores based on user defined heuristics/algorithms and classifies documents into high quality documents or low quality documents if the quality score is above the user defined threshold.\n",
    "\n",
    "Sample lists of heuristic filters can be found in `./config/`\n",
    "- `heuristic_filter_en.yaml`: Sample heuristic filter list for English dataset\n",
    "- `heuristic_filter_non-en.yaml`:Sample heuristic filter list for Non-English dataset\n",
    "- `heuristic_filter_code.yaml`:Sample heuristic filter list for Code language dataset\n",
    "Please adjust the sample list e.g. remove/add filters or change filter threshold based on your own use case. In this example, `heuristic_filter_non-en.yaml` will be used.\n",
    "\n",
    "For detailed implementation and description of each heuristic filter, please refer to `./NeMo-Curator/nemo-curator/filters/heuristics_filter.py`. For customized heuristic filter implementation, user shall follow the sample implementations, write customized filters and update the .yaml files accordingly.\n",
    "\n",
    "For analysis of impact of each filters on the dataset, user should set `log-score` to true for the filters in the corresponding config .yaml file. This will output quality score for all filters in separate .txt files for each individual filter. With the quality score and filter threshold, use can calculate quality score distribution and other analysis to assess the effectiveness of each filter.\n",
    "\n",
    "In this example, in order to get a comprehensive output of each filter, we are iterating through ever filter using a for loop and saving the intermediate result. This process will involve extensive I/O operations and is less effective. Alternatively, after loading input dataset and filter pipeline, user can simply call `filter_pipeline(dataset)` to obtain the final filtered result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ddff58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "from nemo_curator import Score, Filter, ScoreFilter\n",
    "from nemo_curator.utils.file_utils import get_batched_files,expand_outdir_and_mkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728a161",
   "metadata": {},
   "source": [
    "**[Optional]**The following cell is to remove warning from dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5114945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable the metadata warning\n",
    "warnings.filterwarnings(\"ignore\",module=\"dask.dataframe.core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243a7cb",
   "metadata": {},
   "source": [
    "Create a CPU Dask Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa752ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=10, processes=True, memory_limit='16GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dda877",
   "metadata": {},
   "source": [
    "Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a8abf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_complement(original_df, filtered_df):\n",
    "    def partition_complement(part_original_df, partition_info=None):\n",
    "        if not partition_info:\n",
    "            return part_original_df\n",
    "        part_filtered_df = filtered_df.get_partition(partition_info[\"number\"])\n",
    "        complement_mask = ~part_original_df.index.isin(part_filtered_df.index.persist())\n",
    "        complement_df = part_original_df[complement_mask]\n",
    "        return complement_df\n",
    "\n",
    "    return original_df.map_partitions(partition_complement)\n",
    "\n",
    "def write_scores(df, output_dir):\n",
    "    for column in df.columns:\n",
    "        output_path = os.path.join(output_dir, f\"{column}.txt\")\n",
    "        df[column].to_csv(output_path, single_file=True, encoding=\"utf-8\", header=False, index=False, mode=\"a\")\n",
    "\n",
    "def get_score_fields(pipeline):\n",
    "    score_fields = []\n",
    "    for nc_module in pipeline.modules:\n",
    "        if isinstance(nc_module, Score) or isinstance(nc_module, ScoreFilter):\n",
    "            if nc_module.score_field:\n",
    "                score_fields.append(nc_module.score_field)\n",
    "    return score_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6b0f8",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55e43a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "HF_input_data_dir = dudped_output_dir\n",
    "input_file_type = 'parquet'\n",
    "batch_size = 1\n",
    "\n",
    "#Output\n",
    "HF_base_output_path = os.path.join(data_dir,'heuristic_filtering')\n",
    "kept_document_dir =  os.path.join(HF_base_output_path,'data','hq.parquet')\n",
    "removed_document_dir =  os.path.join(HF_base_output_path,'data','lq.parquet')\n",
    "output_document_score_dir =  os.path.join(HF_base_output_path,'data','score')\n",
    "output_file_type = 'parquet'\n",
    "\n",
    "#Relevant parameters\n",
    "filter_config_file = './config/heuristic_filter_non-en.yaml'\n",
    "input_id_field = 'id'\n",
    "\n",
    "#Set to False if do not want to save intermediate results\n",
    "is_cache = True\n",
    "\n",
    "!mkdir -p {kept_document_dir}\n",
    "!mkdir -p {removed_document_dir}\n",
    "!mkdir -p {output_document_score_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f6c8e",
   "metadata": {},
   "source": [
    "Run heuristic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6f50332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "Saving data for symbol_to_word\n",
      "Writing to disk complete for 1 partitions\n",
      "Saving data for numbers_ratio\n",
      "Writing to disk complete for 1 partitions\n",
      "Saving data for urls_ratio\n",
      "Writing to disk complete for 1 partitions\n",
      "Saving data for word_count\n",
      "Writing to disk complete for 1 partitions\n",
      "Saving data for repeating_top_2grams\n",
      "Writing to disk complete for 1 partitions\n",
      "Saving data for repeating_top_3grams\n",
      "Writing to disk complete for 1 partitions\n",
      "Saving data for repeating_top_4grams\n",
      "Writing to disk complete for 1 partitions\n",
      "Writing to disk complete for 1 partitions\n",
      "Time taken for Heuristic filtering: 729.7436628341675 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "#Load filters from config\n",
    "filter_pipeline = build_filter_pipeline(filter_config_file)\n",
    "score_fields = get_score_fields(filter_pipeline)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(HF_input_data_dir,file_type='parquet')\n",
    "\n",
    "\n",
    "# Iterate through filters. For each filter, the low quality document will be removed from the dataset and output to corresponding folder for analysis\n",
    "# Output of previous filter will be input of the next filter\n",
    "if is_cache:\n",
    "    curr_dataset = prev_dataset = dataset\n",
    "    for filter_module in filter_pipeline.modules:\n",
    "        #Apply filter\n",
    "        curr_dataset = filter_module(curr_dataset).persist()\n",
    "\n",
    "        #Output filtered document\n",
    "        print(f\"Saving data for {filter_module.filter_obj._name}\")\n",
    "        removed_df = get_dataframe_complement(prev_dataset.df, curr_dataset.df)\n",
    "        removed_filter_dir = os.path.join(removed_document_dir, filter_module.filter_obj._name)\n",
    "        expand_outdir_and_mkdir(removed_filter_dir)\n",
    "        write_to_disk(removed_df, removed_filter_dir, write_to_filename=True, output_type=output_file_type)\n",
    "        prev_dataset = curr_dataset\n",
    "    filtered_dataset = curr_dataset\n",
    "else:\n",
    "    filtered_dataset = filter_pipeline(dataset)\n",
    "\n",
    "# Write scores of retained doucment to separate directory\n",
    "output_df = filtered_dataset.df[[input_id_field, *score_fields]]\n",
    "write_scores(output_df, output_document_score_dir)\n",
    "\n",
    "# Remove scores from dataset df\n",
    "filtered_dataset = DocumentDataset(filtered_dataset.df.drop(columns=score_fields))\n",
    "\n",
    "# Output filtered dataset\n",
    "write_to_disk(filtered_dataset.df, kept_document_dir, write_to_filename=True, output_type=output_file_type)\n",
    "\n",
    "print(f\"Time taken for Heuristic filtering: {time.time()-t0} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19731f5",
   "metadata": {},
   "source": [
    "Verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f945362",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_parquet(kept_document_dir)\n",
    "print(f\"Dataset size after heuristic filtering:{len(res)}\")\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52fe04",
   "metadata": {},
   "source": [
    "Close the CPU Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aaa9823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f6e74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
