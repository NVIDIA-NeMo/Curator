{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Curation in NeMo Curator\n",
    "\n",
    "In the following notebook, we'll be exploring all of the functionality that NeMo Curator has for image dataset curation.\n",
    "NeMo Curator has a few built-in modules for \n",
    "\n",
    "First, we'll need to install NeMo Curator!\n",
    "\n",
    "NOTE: Please ensure you meet the [requirements](https://github.com/NVIDIA/NeMo-Curator/tree/main?tab=readme-ov-file#install-nemo-curator) before proceeding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Sample Dataset\n",
    "If you already have a dataset in webdataset format, great! You can skip to the [next section](#create-clip-embeddings).\n",
    "In order to have a sample dataset to play with, we are going to download a subset of the [Microsoft Common Objects in Context (mscoco)](https://cocodataset.org/#home) dataset.\n",
    "MSCOCO is a dataset of 600,000 image-text pairs (around 76GB) that takes around 20 minutes to download.\n",
    "For the sake of this tutorial, we are only going to download a subset of the dataset.\n",
    "We will download 20,000 image-text pairs (around 3GB).\n",
    "\n",
    "To download the dataset, we are going to use a tool called img2dataset. Let's install it and download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install img2dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to get a list of URLs that identify where all the images are hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT/resolve/main/mscoco.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We truncate this list of URLs so we don't download the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_URLS = 20_000\n",
    "urls = pd.read_parquet(\"mscoco.parquet\")\n",
    "truncated_urls = urls[:NUM_URLS]\n",
    "truncated_urls.to_parquet(\"truncated_mscoco.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!img2dataset \\\n",
    "    --url_list truncated_mscoco.parquet \\\n",
    "    --input_format \"parquet\" \\\n",
    "    --output_folder mscoco \\\n",
    "    --output_format webdataset \\\n",
    "    --url_col \"URL\" \\\n",
    "    --caption_col \"TEXT\" \\\n",
    "    --processes_count 16 \\\n",
    "    --thread_count 64 \\\n",
    "    --resize_mode no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os._exit(0) # Shut down all kernels to prevent dependencies from conflicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NeMo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install cython ipywidgets\n",
    "# Install from source by default\n",
    "!pip install --extra-index-url https://pypi.nvidia.com ../../[image]\n",
    "# !pip install --extra-index-url https://pypi.nvidia.com nemo-curator[image]\n",
    "%env DASK_DATAFRAME__QUERY_PLANNING False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CLIP Embeddings\n",
    "\n",
    "### Load the Dataset\n",
    "Instead of operating on the images directly, most features in NeMo Curator take embeddings as inputs. So, as the first stage in the pipeline, we are going to generate embeddings for all the images in the dataset. To begin, let's load the dataset using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import get_client\n",
    "\n",
    "client = get_client(cluster_type=\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset path if you have your own dataset\n",
    "dataset_path = \"./mscoco\"\n",
    "# Change the unique identifier depending on your dataset\n",
    "id_col = \"key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.datasets import ImageTextPairDataset\n",
    "\n",
    "dataset = ImageTextPairDataset.from_webdataset(dataset_path, id_col)\n",
    "# Filter out any entries that failed to download\n",
    "dataset.metadata = dataset.metadata[dataset.metadata[\"error_message\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Embedder\n",
    "We can now define the embedding creation step in our pipeline. NeMo Curator has support for all [timm](https://pypi.org/project/timm/) models. NeMo Curator's aesthetic classifier is trained on embeddings from `vit_large_patch14_clip_quickgelu_224.openai`, so we will use that.\n",
    "\n",
    "The cell below will do the following:\n",
    "1. Download the model `vit_large_patch14_clip_quickgelu_224.openai`.\n",
    "1. Automatically convert the image preprocessing transformations of `vit_large_patch14_clip_quickgelu_224.openai` from their PyTorch form to DALI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.image.embedders import TimmImageEmbedder\n",
    "\n",
    "embedding_model = TimmImageEmbedder(\n",
    "                    \"vit_large_patch14_clip_quickgelu_224.openai\",\n",
    "                    pretrained=True,\n",
    "                    batch_size=1024,\n",
    "                    num_threads_per_worker=16,\n",
    "                    normalize_embeddings=True,\n",
    "                    autocast=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create embeddings for the whole dataset. It's important to understand what is going on internally in NeMo Curator so you can modify parameters appropriately.\n",
    "\n",
    "Once the computation is triggered, the cell below will\n",
    "1. Load a shard of metadata (a `.parquet` file) onto each GPU you have available using Dask-cuDF.\n",
    "1. Load a copy of `vit_large_patch14_clip_quickgelu_224.openai` onto each GPU.\n",
    "1. Repeatedly load images into batches of size `batch_size` onto each GPU with a given threads per worker (`num_threads_per_worker`) using DALI.\n",
    "1. The model is run on the batch (without `torch.autocast()` since `autocast=False`).\n",
    "1. The output embeddings of the model are normalized since `normalize_embeddings=True`.\n",
    "\n",
    "\n",
    "Since NeMo Curator uses Dask, the cell below will not cause the embeddings to be created. The computation will only begin once we inspect the output in the `dataset.metadata.head()` call or when we write to disk using `dataset.save_metadata()` or `dataset.to_webdataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask is lazy, so this will not compute embeddings\n",
    "dataset = embedding_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since embedding creation can take a long time, we save the embeddings right after they are generated. `dataset.save_metadata()` will add a new column for the image embeddings in the existing `.parquet` files in the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This triggers the computation\n",
    "# You don't need to save and load normally, but we do it here so\n",
    "# the embeddings aren't recomputed every time they are needed.\n",
    "dataset.save_metadata()\n",
    "dataset = ImageTextPairDataset.from_webdataset(dataset_path, id_col)\n",
    "dataset.metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aesthetic Classifier\n",
    "With the embeddings now created, we can use the aesthetic classifier. This classifier assigns a score from 0 to 10 that corresponds to how aesthetically pleasing the image is. A score of 0 means that the image is not pleasant to look at, while a score of 10 is pleasant to look at. The exact classifier used is the LAION-Aesthetics_Predictor V2. More information on the model can be found here: https://laion.ai/blog/laion-aesthetics/.\n",
    "\n",
    "The following cell will download the model to your local storage at `NEMO_CURATOR_HOME` (`/home/user/.nemo_curator`). The model is only 3.6MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.image.classifiers import AestheticClassifier\n",
    "\n",
    "aesthetic_dataset_path = \"./aesthetic_dataset\"\n",
    "aesthetic_classifier = AestheticClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aesthetic_classifier(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After annotating with the aesthetic score, we can filter by the aesthetic score. Finally, we save the resulting filter dataset to a new webdataset.\n",
    "\n",
    "Unlike `dataset.save_metadata()`, `dataset.to_webdataset()` will modify the tar files as well as the `.parquet` files. It will cause the tar files to be resharded and any entries that have an aesthetic score less than or equal to 6 will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.metadata[\"passes_aesthetic_check\"] = dataset.metadata[\"aesthetic_score\"] > 6\n",
    "dataset.to_webdataset(aesthetic_dataset_path, filter_column=\"passes_aesthetic_check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "Now that we have filtered our dataset based on aesthetic score, we can see what kinds of images are remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def display_image_from_tar(tar_file_path, image_file_name):\n",
    "    # Open the tar file\n",
    "    with tarfile.open(tar_file_path, 'r') as tar:\n",
    "        # Extract the specified image file\n",
    "        image_file = tar.extractfile(image_file_name)\n",
    "        \n",
    "        if image_file is not None:\n",
    "            # Read the image data\n",
    "            image_data = image_file.read()\n",
    "            \n",
    "            # Create a PIL Image object from the image data\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            \n",
    "            # Display the image using matplotlib\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')  # Hide axes\n",
    "            plt.title(f\"Image: {image_file_name}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Image file '{image_file_name}' not found in the tar archive.\")\n",
    "    \n",
    "\n",
    "output_shard = os.path.join(aesthetic_dataset_path, \"00000.tar\")\n",
    "image_file_name = '000000003.jpg'\n",
    "display_image_from_tar(output_shard, image_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Deduplication\n",
    "\n",
    "NeMo Curator provides an easy module for semantically deduplicating images. Semantic duplicates are images that contain almost the same information content, but are perceptually different. Two imges of the same dog taken from slightly different angles would be considered semantic duplicates. NeMo Curator' semantic deduplication approach is based on the paper [SemDeDup: Data-efficient learning at web-scale through semantic deduplication](https://arxiv.org/pdf/2303.09540) by Abbas et al which has demonstrated that deduplicating your data can lead to the same downstream performance in *half* the number of training iterations. For more information on the algorithm, you can check out the [documentation page](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/semdedup.html#data-curator-semdedup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator import ClusteringModel, SemanticClusterLevelDedup\n",
    "\n",
    "# Convert the dataset\n",
    "embeddings_dataset = DocumentDataset(dataset.metadata)\n",
    "\n",
    "semantic_dedup_outputs = \"./semantic_deduplication\"\n",
    "os.makedirs(semantic_dedup_outputs, exist_ok=True)\n",
    "\n",
    "# Run clustering\n",
    "clustering_output = os.path.join(semantic_dedup_outputs, \"cluster_output\")\n",
    "clustering_model = ClusteringModel(\n",
    "    id_col=id_col,\n",
    "    embedding_col=\"image_embedding\",\n",
    "    max_iter=10,\n",
    "    n_clusters=1,\n",
    "    clustering_output_dir=clustering_output,\n",
    ")\n",
    "clustered_dataset = clustering_model(embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cluster-level dedup\n",
    "emb_by_cluster_output = os.path.join(clustering_output, \"embs_by_nearest_center\")\n",
    "sorted_cluster_output = os.path.join(clustering_output, \"sorted\")\n",
    "duplicate_output = os.path.join(semantic_dedup_outputs, \"duplicates\")\n",
    "\n",
    "semantic_dedup = SemanticClusterLevelDedup(\n",
    "    n_clusters=1,\n",
    "    emb_by_clust_dir=emb_by_cluster_output,\n",
    "    sorted_clusters_dir=sorted_cluster_output,\n",
    "    id_col=id_col,\n",
    "    id_col_type=\"str\",\n",
    "    embedding_col=\"image_embedding\",\n",
    "    which_to_keep=\"hard\",\n",
    "    output_dir=duplicate_output,\n",
    ")\n",
    "semantic_dedup.compute_semantic_match_dfs([0.01, 0.001])\n",
    "deduplicated_dataset_ids = semantic_dedup.extract_dedup_data(eps_to_extract=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "\n",
    "We got a list of deduplicated image ids. Now we can remove entries with those ids from our dataset, and reshard the dataset to remove them from the tar files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "deduplicated_dataset_path = \"./deduplicated_dataset\"\n",
    "dataset.metadata[\"is_unique\"] = dataset.metadata[\"key\"].isin(deduplicated_dataset_ids.df[\"key\"].compute())\n",
    "dataset.to_webdataset(deduplicated_dataset_path, \"is_unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Duplicates\n",
    "\n",
    "To better understand what we did in semantic deduplication, let's examine the output of an intermediate step and visualize some of the duplicates NeMo Curator removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cluster_path = os.path.join(duplicate_output, \"semdedup_pruning_tables\", \"cluster_0.parquet\")\n",
    "df = pd.read_parquet(cluster_path)\n",
    "df = df[~df[\"eps=0.001\"]]\n",
    "df = df.sort_values(\"cosine_sim_score\", ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what the columns mean.\n",
    "1. The `id` column represents the id of the datapoint.\n",
    "1. `max_id` represents the id of the image that `id` is the most similar to.\n",
    "1. `cosine_sim_score` is the cosine similarity, where `1` indicates the two images are exactly the same and `0` means the images are completely different.\n",
    "1. `eps=0.01` is `True` if `cosine_sim_score` is `>= 0.99`\n",
    "1. `eps=0.001` is `True` if `cosine_sim_score` is `>= 0.999`\n",
    "\n",
    "This dataset has a lot of exact duplicates, so those are caught by `eps=0.001`, and we have filtered out the ids so we don't see them here. There are still a few documents that are similar, but not exact duplicates. We can examine `000008687.jpg` and `000008672.jpg` using our helper function from earlier to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shard = os.path.join(dataset_path, \"00000.tar\")\n",
    "\n",
    "display_image_from_tar(input_shard, \"000008687.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_tar(input_shard, \"000008672.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the images are different. Notably, the baby's head is looking in a different direction. Despite the small differences, these images would still be considered semantic duplicates. Feel free to adjust the epsilon threshold and see what kinds of "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
