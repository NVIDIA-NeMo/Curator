{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Curation in NeMo Curator\n",
    "\n",
    "In the following notebook, we'll be exploring all of the functionality that NeMo Curator has for image dataset curation.\n",
    "NeMo Curator has a few built-in modules for \n",
    "\n",
    "First, we'll need to install NeMo Curator!\n",
    "\n",
    "NOTE: Please ensure you meet the [requirements](https://github.com/NVIDIA/NeMo-Curator/tree/main?tab=readme-ov-file#install-nemo-curator) before proceeding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Sample Dataset\n",
    "If you already have a dataset in webdataset format, great! You can skip to the [next section](#create-clip-embeddings).\n",
    "In order to have a sample dataset to play with, we are going to download a subset of the [Microsoft Common Objects in Context (mscoco)](https://cocodataset.org/#home) dataset.\n",
    "MSCOCO is a dataset of 600,000 image-text pairs (around 76GB) that takes around 20 minutes to download.\n",
    "For the sake of this tutorial, we are only going to download a subset of the dataset.\n",
    "We will download 20,000 image-text pairs (around 3GB).\n",
    "\n",
    "To download the dataset, we are going to use a tool called img2dataset. Let's install it and download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install img2dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to get a list of URLs that identify where all the images are hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT/resolve/main/mscoco.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We truncate this list of URLs so we don't download the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_URLS = 20_000\n",
    "urls = pd.read_parquet(\"mscoco.parquet\")\n",
    "truncated_urls = urls[:NUM_URLS]\n",
    "truncated_urls.to_parquet(\"truncated_mscoco.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!img2dataset \\\n",
    "    --url_list truncated_mscoco.parquet \\\n",
    "    --input_format \"parquet\" \\\n",
    "    --output_folder mscoco \\\n",
    "    --output_format webdataset \\\n",
    "    --url_col \"URL\" \\\n",
    "    --caption_col \"TEXT\" \\\n",
    "    --processes_count 16 \\\n",
    "    --thread_count 64 \\\n",
    "    --resize_mode no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NeMo Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install cython\n",
    "!pip install --extra-index-url https://pypi.nvidia.com nemo-curator[image]\n",
    "%env DASK_DATAFRAME__QUERY_PLANNING False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CLIP Embeddings\n",
    "\n",
    "### Load the Dataset\n",
    "Instead of operating on the images directly, most features in NeMo Curator take embeddings as inputs. So, as the first stage in the pipeline, we are going to generate embeddings for all the images in the dataset. To begin, let's load the dataset using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import get_client\n",
    "\n",
    "client = get_client(cluster_type=\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset path if you have your own dataset\n",
    "dataset_path = \"./mscoco\"\n",
    "# Change the unique identifier depending on your dataset\n",
    "id_col = \"key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.datasets import ImageTextPairDataset\n",
    "\n",
    "dataset = ImageTextPairDataset.from_webdataset(dataset_path, id_col)\n",
    "# Filter out any entries that failed to download\n",
    "dataset.metadata = dataset.metadata[dataset.metadata[\"error_message\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Embedder\n",
    "We can now define the embedding creation step in our pipeline. NeMo Curator has support for all [timm](https://pypi.org/project/timm/) models. NeMo Curator's aesthetic classifier is trained on embeddings from `vit_large_patch14_clip_quickgelu_224.openai`, so we will use that.\n",
    "\n",
    "The cell below will do the following:\n",
    "1. Download the model `vit_large_patch14_clip_quickgelu_224.openai`.\n",
    "1. Automatically convert the image preprocessing transformations of `vit_large_patch14_clip_quickgelu_224.openai` from their PyTorch form to DALI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.image.embedders import TimmImageEmbedder\n",
    "\n",
    "embedding_model = TimmImageEmbedder(\n",
    "                    \"vit_large_patch14_clip_quickgelu_224.openai\",\n",
    "                    pretrained=True,\n",
    "                    batch_size=1024,\n",
    "                    num_threads_per_worker=16,\n",
    "                    normalize_embeddings=True,\n",
    "                    autocast=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create embeddings for the whole dataset. It's important to understand what is going on internally in NeMo Curator so you can modify parameters appropriately.\n",
    "\n",
    "Once the computation is triggered, the cell below will\n",
    "1. Load a shard of metadata (a `.parquet` file) onto each GPU you have available using Dask-cuDF.\n",
    "1. Load a copy of `vit_large_patch14_clip_quickgelu_224.openai` onto each GPU.\n",
    "1. Repeatedly load images into batches of size `batch_size` onto each GPU with a given threads per worker (`num_threads_per_worker`) using DALI.\n",
    "1. The model is run on the batch (without `torch.autocast()` since `autocast=False`).\n",
    "1. The output embeddings of the model are normalized since `normalize_embeddings=True`.\n",
    "\n",
    "\n",
    "Since NeMo Curator uses Dask, the cell below will not cause the embeddings to be created. The computation will only begin once we inspect the output in the `dataset.metadata.head()` call or when we write to disk using `dataset.save_metadata()` or `dataset.to_webdataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask is lazy, so this will not compute embeddings\n",
    "dataset = embedding_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This triggers the computation for the first shard in the dataset\n",
    "dataset.metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aesthetic Classifier\n",
    "With the embeddings now created, we can use the aesthetic classifier. This classifier assigns a score from 0 to 10 that corresponds to how aesthetically pleasing the image is. A score of 0 means that the image is not pleasant to look at, while a score of 10 is pleasant to look at. The exact classifier used is the LAION-Aesthetics_Predictor V2. More information on the model can be found here: https://laion.ai/blog/laion-aesthetics/.\n",
    "\n",
    "The following cell will download the model to your local storage at `NEMO_CURATOR_HOME` (`/home/user/.nemo_curator`). The model is only 3.6MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.image.classifiers import AestheticClassifier\n",
    "\n",
    "aesthetic_classifier = AestheticClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aesthetic_classifier(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset_path = \"./output_dataset\"\n",
    "dataset.metadata[\"passes_aesthetic_check\"] = dataset.metadata[\"aesthetic_score\"] > 6\n",
    "dataset.to_webdataset(output_dataset_path, \"passes_aesthetic_check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "Now that we have filtered our dataset based on aesthetic score, we can see what kinds of images are remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def display_image_from_tar(tar_file_path, image_file_name):\n",
    "    # Open the tar file\n",
    "    with tarfile.open(tar_file_path, 'r') as tar:\n",
    "        # Extract the specified image file\n",
    "        image_file = tar.extractfile(image_file_name)\n",
    "        \n",
    "        if image_file is not None:\n",
    "            # Read the image data\n",
    "            image_data = image_file.read()\n",
    "            \n",
    "            # Create a PIL Image object from the image data\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            \n",
    "            # Display the image using matplotlib\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')  # Hide axes\n",
    "            plt.title(f\"Image: {image_file_name}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Image file '{image_file_name}' not found in the tar archive.\")\n",
    "    \n",
    "\n",
    "output_shard = os.path.join(output_dataset_path, \"00000.tar\")\n",
    "image_file_name = '000000003.jpg'\n",
    "display_image_from_tar(output_shard, image_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Deduplication\n",
    "\n",
    "NeMo Curator provides an easy module for semantically deduplicating images. Semantic duplicates are images that contain almost the same information content, but are perceptually different. Two imges of the same dog taken from slightly different angles would be considered semantic duplicates. NeMo Curator' semantic deduplication approach is based on the paper [SemDeDup: Data-efficient learning at web-scale through semantic deduplication](https://arxiv.org/pdf/2303.09540) by Abbas et al which has demonstrated that deduplicating your data can lead to the same downstream performance in *half* the number of training iterations. For more information on the algorithm, you can check out the [documentation page](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/semdedup.html#data-curator-semdedup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator import ClusteringModel, SemanticClusterLevelDedup\n",
    "\n",
    "# Convert the dataset\n",
    "embeddings_dataset = DocumentDataset(dataset.metadata)\n",
    "embeddings_dataset.df = embeddings_dataset.df.rename(columns={\"image_embeddings\": \"embeddings\"})\n",
    "\n",
    "semantic_dedup_outputs = \"./semantic_deduplication\"\n",
    "os.makedirs(semantic_dedup_outputs, exist_ok=True)\n",
    "\n",
    "# Run clustering\n",
    "clustering_output = os.path.join(semantic_dedup_outputs, \"cluster_output\")\n",
    "clustering_model = ClusteringModel(\n",
    "    id_col=id_col,\n",
    "    max_iter=10,\n",
    "    n_clusters=1,\n",
    "    clustering_output_dir=clustering_output,\n",
    ")\n",
    "clustered_dataset = clustering_model(embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cluster-level dedup\n",
    "emb_by_cluster_output = os.path.join(semantic_dedup_outputs, \"emb_by_cluster\")\n",
    "sorted_cluster_output = os.path.join(semantic_dedup_outputs, \"sorted_cluster\")\n",
    "duplicate_output = os.path.join(semantic_dedup_outputs, \"duplicates\")\n",
    "\n",
    "semantic_dedup = SemanticClusterLevelDedup(\n",
    "    n_clusters=50000,\n",
    "    emb_by_clust_dir=emb_by_cluster_output,\n",
    "    sorted_clusters_dir=sorted_cluster_output,\n",
    "    id_col=id_col,\n",
    "    id_col_type=\"str\",\n",
    "    which_to_keep=\"hard\",\n",
    "    output_dir=duplicate_output,\n",
    ")\n",
    "semantic_dedup.compute_semantic_match_dfs()\n",
    "deduplicated_dataset_ids = semantic_dedup.extract_dedup_data(eps_to_extract=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
