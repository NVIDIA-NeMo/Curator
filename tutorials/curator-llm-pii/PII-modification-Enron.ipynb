{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1bfd87d-271b-4bc9-b824-57887bb1a510",
   "metadata": {},
   "source": [
    "## LLM-based PII Modification on Enron Email Dataset with NeMo Curator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2906c-3616-4932-825c-0d0e1a461044",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to use NeMo Curator's PII (Personally Identifiable Information) modification capabilities on a real-world dataset. We'll use a subset of the Enron email dataset to showcase both asynchronous and synchronous LLM-based PII modification approaches.\n",
    "\n",
    "## Why PII Modification?\n",
    "\n",
    "PII modification is crucial for:\n",
    "1. **Data Privacy with Utility**: Transforming sensitive information while maintaining data usefulness\n",
    "2. **Training Data Quality**: Creating realistic but privacy-safe training data\n",
    "3. **Regulatory Compliance**: Meeting privacy requirements while preserving data characteristics\n",
    "4. **Research Value**: Enabling research on sensitive datasets without compromising privacy\n",
    "5. **Safe ML Training**: Ensuring ML models don't learn or expose private information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344483f-306e-4fc1-b609-1d1feee6bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.modifiers.async_llm_pii_modifier import AsyncLLMPiiModifier\n",
    "from nemo_curator.modifiers.llm_pii_modifier import LLMPiiModifier\n",
    "from nemo_curator.modules.modify import Modify\n",
    "from nemo_curator.utils.distributed_utils import get_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d654a-79aa-4d96-bb8f-716f390a0f83",
   "metadata": {},
   "source": [
    "# Download Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b626b42-4718-4579-a6d8-e50b7a0dbcf4",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "The [Enron Email Dataset](https://www.cs.cmu.edu/~enron/) is a large public dataset containing real-world business emails from Enron Corporation employees. It was made public during the legal investigation of the Enron corporation and has become a valuable resource for research in natural language processing and email analysis.\n",
    "\n",
    "## Structure\n",
    "The dataset is organized as follows:\n",
    "- maildir/\n",
    "  - user1/\n",
    "    - inbox/\n",
    "    - sent/\n",
    "    - deleted_items/\n",
    "    ...\n",
    "  - user2/\n",
    "    ...\n",
    "\n",
    "In this tutorial, we will focus on extracting and processing emails from Philip Allen's mailbox (`allen-p`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4112fc8-21c0-4b98-b19d-906641a2849e",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e475aee-bf56-4857-bbaa-465579ff5593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Enron dataset from https://www.cs.cmu.edu/~enron/enron_mail_20150507.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513ac1a6092c410fa7a3ce73c3c64752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_enron_dataset(target_dir: str = \"enron_data\") -> None:\n",
    "    \"\"\"Download the full Enron email dataset\"\"\"\n",
    "    url = \"https://www.cs.cmu.edu/~enron/enron_mail_20150507.tar.gz\"\n",
    "    tar_file = os.path.join(target_dir, \"enron_mail_20150507.tar.gz\")\n",
    "\n",
    "    # Create target directory\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Download if not already downloaded\n",
    "    if not os.path.exists(tar_file):\n",
    "        print(f\"Downloading Enron dataset from {url}\")\n",
    "        response = requests.get(url, stream=True, timeout=10)\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "        with open(tar_file, \"wb\") as f, tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "            for data in response.iter_content(chunk_size=1024 * 1024):\n",
    "                size = f.write(data)\n",
    "                pbar.update(size)\n",
    "    else:\n",
    "        print(f\"Found existing download at {tar_file}\")\n",
    "\n",
    "    return tar_file\n",
    "\n",
    "\n",
    "# Download the dataset\n",
    "tar_file = download_enron_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf94d6b-b3b2-4dbf-918a-8f53ce6c5ef0",
   "metadata": {},
   "source": [
    "## Step 2: Extract Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13c5b6-516f-4a22-9681-30081e31b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting allen-p's mailbox...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cb4f8bac6949229d67af1b201b3e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting allen-p's emails:   0%|          | 0/3044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted to: enron_data/maildir/allen-p\n",
      "\n",
      "Mailbox structure:\n",
      "allen-p/\n",
      "    sent/\n",
      "        474.\n",
      "        338.\n",
      "        206.\n",
      "        38.\n",
      "        542.\n",
      "        ... (557 more files)\n",
      "    deleted_items/\n",
      "        206.\n",
      "        38.\n",
      "        140.\n",
      "        143.\n",
      "        382.\n",
      "        ... (356 more files)\n",
      "    all_documents/\n",
      "        474.\n",
      "        338.\n",
      "        206.\n",
      "        565.\n",
      "        38.\n",
      "        ... (623 more files)\n",
      "    notes_inbox/\n",
      "        38.\n",
      "        19.\n",
      "        4.\n",
      "        50.\n",
      "        42.\n",
      "        ... (43 more files)\n",
      "    contacts/\n",
      "        2.\n",
      "        1.\n",
      "    discussion_threads/\n",
      "        474.\n",
      "        206.\n",
      "        38.\n",
      "        542.\n",
      "        140.\n",
      "        ... (407 more files)\n",
      "    sent_items/\n",
      "        206.\n",
      "        38.\n",
      "        140.\n",
      "        143.\n",
      "        382.\n",
      "        ... (340 more files)\n",
      "    _sent_mail/\n",
      "        474.\n",
      "        338.\n",
      "        206.\n",
      "        565.\n",
      "        38.\n",
      "        ... (597 more files)\n",
      "    straw/\n",
      "        4.\n",
      "        2.\n",
      "        8.\n",
      "        1.\n",
      "        6.\n",
      "        ... (3 more files)\n",
      "    inbox/\n",
      "        38.\n",
      "        65.\n",
      "        19.\n",
      "        67.\n",
      "        4.\n",
      "        ... (61 more files)\n"
     ]
    }
   ],
   "source": [
    "def extract_user_mailbox(tar_file: str, username: str = \"allen-p\", target_dir: str = \"enron_data\") -> None:\n",
    "    \"\"\"Extract specific user's mailbox from the dataset\"\"\"\n",
    "    maildir_path = os.path.join(target_dir, \"maildir\")\n",
    "    user_path = os.path.join(maildir_path, username)\n",
    "\n",
    "    if not os.path.exists(user_path):\n",
    "        print(f\"Extracting {username}'s mailbox...\")\n",
    "        with tarfile.open(tar_file, \"r:gz\") as tar:\n",
    "            # Get all members that belong to the specified user\n",
    "            members = [m for m in tar.getmembers() if m.name.startswith(f\"maildir/{username}/\")]\n",
    "\n",
    "            # Extract user's mailbox\n",
    "            for member in tqdm(members, desc=f\"Extracting {username}'s emails\"):\n",
    "                tar.extract(member, target_dir)\n",
    "    else:\n",
    "        print(f\"Found existing extraction at {user_path}\")\n",
    "\n",
    "    return user_path\n",
    "\n",
    "\n",
    "# Extract Allen's mailbox\n",
    "allen_path = extract_user_mailbox(tar_file, username=\"allen-p\")\n",
    "print(f\"\\nExtracted to: {allen_path}\")\n",
    "\n",
    "# List contents of Allen's mailbox\n",
    "print(\"\\nMailbox structure:\")\n",
    "MAX_FILES_TO_SHOW = 5\n",
    "for root, _dirs, files in os.walk(allen_path):\n",
    "    level = root.replace(allen_path, \"\").count(os.sep)\n",
    "    indent = \" \" * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    if files:\n",
    "        subindent = \" \" * 4 * (level + 1)\n",
    "        for f in files[:MAX_FILES_TO_SHOW]:  # Show first 5 files in each directory\n",
    "            print(f\"{subindent}{f}\")\n",
    "        if len(files) > MAX_FILES_TO_SHOW:\n",
    "            print(f\"{subindent}... ({len(files) - 5} more files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c3033-36fe-434e-b149-18c1ef187fcd",
   "metadata": {},
   "source": [
    "## Step 3: Load Sample Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945539bd-1735-4965-8d24-54bab42a2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emails from enron_data/maildir/allen-p/inbox\n",
      "\n",
      "Loaded 10 emails\n",
      "\n",
      "Sample DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         10 non-null     object\n",
      " 1   text       10 non-null     object\n",
      " 2   file_path  10 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 372.0+ bytes\n",
      "None\n",
      "\n",
      "First email preview (first 300 characters):\n",
      "Message-ID: <26086640.1075858645326.JavaMail.evans@thyme>\n",
      "Date: Mon, 29 Oct 2001 16:22:13 -0800 (PST)\n",
      "From: arsystem@mailman.enron.com\n",
      "To: k..allen@enron.com\n",
      "Subject: Your Approval is Overdue: Access Request for matt.smith@enron.com\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Conten\n"
     ]
    }
   ],
   "source": [
    "def load_emails_from_folder(folder_path: str, max_emails: int = 10) -> list[dict]:\n",
    "    \"\"\"Load emails from a specific folder\"\"\"\n",
    "    emails = []\n",
    "    email_count = 0\n",
    "\n",
    "    print(f\"Loading emails from {folder_path}\")\n",
    "    for file in os.listdir(folder_path):\n",
    "        if email_count >= max_emails:\n",
    "            break\n",
    "\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            with open(file_path, encoding=\"latin-1\") as f:\n",
    "                content = f.read()\n",
    "                # Basic validation that it's an email\n",
    "                if \"Message-ID:\" in content or \"Date:\" in content:\n",
    "                    emails.append({\"id\": f\"allen-p_{email_count}\", \"text\": content, \"file_path\": file_path})\n",
    "                    email_count += 1\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(emails)\n",
    "\n",
    "\n",
    "# Load sample emails from Allen's inbox\n",
    "inbox_path = os.path.join(allen_path, \"inbox\")\n",
    "sample_df = load_emails_from_folder(inbox_path, max_emails=10)\n",
    "\n",
    "print(f\"\\nLoaded {len(sample_df)} emails\")\n",
    "print(\"\\nSample DataFrame info:\")\n",
    "print(sample_df.info())\n",
    "\n",
    "# Show preview of first email\n",
    "print(\"\\nFirst email preview (first 300 characters):\")\n",
    "print(sample_df.iloc[0][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875d29-836c-415b-a7a1-bdce35e33f6e",
   "metadata": {},
   "source": [
    "## Step 4: Convert to `DocumentDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00490d1-b6c2-4ccf-b81d-dd8608d9750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created DocumentDataset with shape: 10\n"
     ]
    }
   ],
   "source": [
    "dataset = DocumentDataset(dd.from_pandas(sample_df, npartitions=2))\n",
    "print(\"\\nCreated DocumentDataset with shape:\", dataset.df.shape[0].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba35aa-331e-4d76-b118-53af3f61c6f6",
   "metadata": {},
   "source": [
    "## Step 5: Configure and Apply LLM-based PII Modifiers\n",
    "Below, we use an NVIDIA-hosted NIM with an API key generated from [here](https://build.nvidia.com/meta/llama-3_1-70b-instruct). To set up a self-hosted NIM, please refer to the [NIM documentation]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b42fb5-23ed-4d79-a16d-c2971fe86bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35949 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Spilling is enabled\n",
      "Processing with Async LLM modifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.19it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.08it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:05<00:08,  2.87s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:06<00:04,  2.48s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:07<00:05,  2.74s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:08<00:01,  1.82s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:08<00:02,  2.27s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:10<00:00,  2.14s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.69s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.24it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00,  6.44it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.52s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.62s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.03it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.89it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 2 partition(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5/5 [00:19<00:00,  3.81s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:19<00:00, 19.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Sync LLM modifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.49it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:04<00:16,  4.02s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:09<00:14,  4.86s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:10<00:18,  6.13s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:10<00:06,  3.16s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:12<00:02,  2.72s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:12<00:02,  2.77s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.16s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.80s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.20it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  5.18it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:20<00:00,  4.13s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.67s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.04it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  5.49it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.63s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n",
      "\n",
      " 80%|████████  | 4/5 [00:05<00:01,  1.80s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.18s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 2 partition(s)\n"
     ]
    }
   ],
   "source": [
    "# Configure async PII modifier\n",
    "client = get_client(cluster_type=\"gpu\")\n",
    "async_modifier = AsyncLLMPiiModifier(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",  # Replace with your endpoint\n",
    "    api_key=\"\",  # Replace with your API key\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    "    max_concurrent_requests=10,\n",
    ")\n",
    "\n",
    "# Configure sync PII modifier\n",
    "sync_modifier = LLMPiiModifier(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",  # Replace with your endpoint\n",
    "    api_key=\"\",  # Replace with your API key\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    ")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"output/async_llm_enron\", exist_ok=True)\n",
    "os.makedirs(\"output/sync_llm_enron\", exist_ok=True)\n",
    "\n",
    "# Process with both modifiers\n",
    "print(\"Processing with Async LLM modifier...\")\n",
    "async_modified = Modify(async_modifier)(dataset)\n",
    "async_modified.to_json(\"output/async_llm_enron\", write_to_filename=False)\n",
    "\n",
    "print(\"\\nProcessing with Sync LLM modifier...\")\n",
    "sync_modified = Modify(sync_modifier)(dataset)\n",
    "sync_modified.to_json(\"output/sync_llm_enron\", write_to_filename=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e7e80-3e43-4c01-a4ae-8af881a34a44",
   "metadata": {},
   "source": [
    "This step above succesfully identified and modified various types of PII such as email addresses, phone numbers etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e599c81-08c3-4180-ac6f-41962a422040",
   "metadata": {},
   "source": [
    "## Step 6: Compare Results\n",
    "This showcases how certain PII like emails changed after the modification, and statistics about the modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711a1b7-4982-4dbd-be17-a78f50a7cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparison analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.56it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.21it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:07<00:06,  3.18s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:09<00:02,  2.70s/it]\u001b[A\n",
      " 20%|██        | 1/5 [00:10<00:40, 10.12s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:11<00:14,  4.98s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:11<00:05,  2.84s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:17<00:03,  3.99s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.30s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.51s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:31<00:00,  6.33s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.63s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.42it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.72it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  4.47it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.21it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  4.49it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.17s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.83s/it]\n",
      "\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.76s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.80s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.58it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.16it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:07<00:12,  4.10s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:07<00:05,  2.67s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:08<00:05,  2.93s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:09<00:02,  2.35s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:09<00:02,  2.01s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:23<00:00,  4.70s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.50s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.18it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.68it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.08s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:35<00:00, 35.39s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.07it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:08<00:02,  2.24s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.15s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:15<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Let's Compare How the Email Changed ===\n",
      "\n",
      "1. Original Email (first 200 characters):\n",
      "--------------------------------------------------\n",
      "Message-ID: <26086640.1075858645326.JavaMail.evans@thyme>\n",
      "Date: {{date_time}}\n",
      "From: {{email}}\n",
      "To: {{email}}\n",
      "Subject: Your Approval is Overdue: Access Request for {{email}}\n",
      "Mime-Version: 1.0\n",
      "Content-Ty\n",
      "\n",
      "2. After Async LLM Modified It (first 200 characters):\n",
      "--------------------------------------------------\n",
      "Message-ID: <26086640.1075858645326.JavaMail.evans@thyme>\n",
      "Date: {{date_time}}\n",
      "From: {{email}}\n",
      "To: {{email}}\n",
      "Subject: Your Approval is Overdue: Access Request for {{email}}\n",
      "Mime-Version: 1.0\n",
      "Content-Ty\n",
      "\n",
      "3. After Sync LLM Modified It (first 200 characters):\n",
      "--------------------------------------------------\n",
      "Message-ID: <26086640.1075858645326.JavaMail.evans@thyme>\n",
      "Date: {{date_time}}\n",
      "From: {{email}}\n",
      "To: {{email}}\n",
      "Subject: Your Approval is Overdue: Access Request for {{email}}\n",
      "Mime-Version: 1.0\n",
      "Content-Ty\n",
      "\n",
      "=== How Much Did the Emails Change? ===\n",
      "Original email was 914 characters long\n",
      "Async modified version is 821 characters (changed by -93 characters)\n",
      "Sync modified version is 821 characters (changed by -93 characters)\n"
     ]
    }
   ],
   "source": [
    "def compare_pii_modifications() -> None:\n",
    "    \"\"\"\n",
    "    Let's look at how our emails changed after PII modification\n",
    "    Think of this like a \"before and after\" comparison\n",
    "    \"\"\"\n",
    "\n",
    "    # First, let's get our emails ready to look at\n",
    "    # .compute() tells Dask \"I want to actually see this data now\"\n",
    "    original_df = dataset.df.compute()\n",
    "    async_df = async_modified.df.compute()\n",
    "    sync_df = sync_modified.df.compute()\n",
    "\n",
    "    # Let's look at the first email (index 0) as an example\n",
    "    sample_idx = 0\n",
    "\n",
    "    # Get all three versions of the same email\n",
    "    original_email = original_df[\"text\"].iloc[sample_idx]  # The original email\n",
    "    async_email = async_df[\"text\"].iloc[sample_idx]  # Modified by async method\n",
    "    sync_email = sync_df[\"text\"].iloc[sample_idx]  # Modified by sync method\n",
    "\n",
    "    # Let's print them side by side (but only show first 200 characters to keep it readable)\n",
    "    print(\"\\n=== Let's Compare How the Email Changed ===\")\n",
    "\n",
    "    print(\"\\n1. Original Email (first 200 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(original_email[:200])\n",
    "\n",
    "    print(\"\\n2. After Async LLM Modified It (first 200 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(async_email[:200])\n",
    "\n",
    "    print(\"\\n3. After Sync LLM Modified It (first 200 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(sync_email[:200])\n",
    "\n",
    "    # Let's also see how much the length changed\n",
    "    print(\"\\n=== How Much Did the Emails Change? ===\")\n",
    "    print(f\"Original email was {len(original_email)} characters long\")\n",
    "    print(\n",
    "        f\"Async modified version is {len(async_email)} characters (changed by {len(async_email) - len(original_email)} characters)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Sync modified version is {len(sync_email)} characters (changed by {len(sync_email) - len(original_email)} characters)\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Starting comparison analysis...\")\n",
    "compare_pii_modifications()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ba965-aa4c-4f37-b1b7-58ffaeef7017",
   "metadata": {},
   "source": [
    "## Step 7: Analyze PII Patterns\n",
    "\n",
    "This Analyzes the PII patterns across the dataset indicating what types of PII were modified and how they changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00345f8-293d-41ef-8889-335aeb92cbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive PII modification analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.15it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.13it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:08<00:14,  4.84s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:10<00:17,  5.82s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:10<00:02,  2.39s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:11<00:07,  3.78s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.46s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.30s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.14it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.57it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:16<00:00,  3.33s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.63s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.17it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.11it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:05<00:01,  1.75s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:10<00:00,  2.06s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.30s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.57it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.47it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:05<00:04,  2.14s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:11<00:19,  6.66s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:12<00:03,  3.65s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:13<00:09,  4.60s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:14<00:02,  3.00s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.78s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:18<00:00, 18.92s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.03it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  5.31it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:23<00:00,  4.77s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.83s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.18it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  4.88it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:05<00:01,  1.74s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.56s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.82s/it]\n",
      "\n",
      "100%|██████████| 5/5 [00:26<00:00,  5.32s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Let's Compare How the Email Changed ===\n",
      "\n",
      "1. Original Email (first 200 characters):\n",
      "--------------------------------------------------\n",
      "Message-ID: <26086640.1075858645326.JavaMail.evans@thyme>\n",
      "Date: Mon, 29 Oct 2001 16:22:13 -0800 (PST)\n",
      "From: {{email}}\n",
      "To: {{email}}\n",
      "Subject: Your Approval is Overdue: Access Request for {{email}}\n",
      "Mime\n",
      "\n",
      "2. After Async LLM Modified It (first 200 characters):\n",
      "--------------------------------------------------\n",
      "Message-ID: <26086640.1075858645326.JavaMail.evans@thyme>\n",
      "Date: Mon, 29 Oct 2001 16:22:13 -0800 (PST)\n",
      "From: {{email}}\n",
      "To: {{email}}\n",
      "Subject: Your Approval is Overdue: Access Request for {{email}}\n",
      "Mime\n",
      "\n",
      "3. After Sync LLM Modified It (first 200 characters):\n",
      "--------------------------------------------------\n",
      "Message-ID: <26086640.1075858645326.JavaMail.evans@thyme>\n",
      "Date: {{date_time}} {{date_time}}\n",
      "From: {{email}}\n",
      "To: {{email}}\n",
      "Subject: Your Approval is Overdue: Access Request for {{email}}\n",
      "Mime-Version: \n",
      "\n",
      "=== How Much Did the Emails Change? ===\n",
      "Original email was 862 characters long\n",
      "Async modified version is 862 characters (changed by 0 characters)\n",
      "Sync modified version is 868 characters (changed by 6 characters)\n",
      "\n",
      "Analyzing PII patterns across all emails...\n",
      "Loading datasets for pattern analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.52it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.21it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.67it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:05<00:04,  2.30s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:06<00:11,  3.83s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:10<00:03,  3.56s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:11<00:08,  4.44s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:14<00:03,  3.69s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:14<00:00,  2.96s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.81s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.15it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  5.04it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:04<00:01,  1.36s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.87s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
      "\n",
      "100%|██████████| 5/5 [00:24<00:00,  4.99s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:24<00:00, 24.96s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  2.00it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.56it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.71s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.55s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.41it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:01<00:04,  1.02s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:05<00:09,  3.13s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:07<00:13,  4.43s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:08<00:05,  2.93s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:09<00:07,  3.63s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:13<00:03,  3.69s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:18<00:05,  5.46s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:29<00:00,  5.85s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:29<00:00, 29.23s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.92it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00,  5.06it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:32<00:00,  6.54s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:32<00:00, 32.68s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.09it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.30it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.45s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n",
      "\n",
      " 60%|██████    | 3/5 [00:03<00:03,  1.61s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:05<00:01,  1.50s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.78s/it]\u001b[A\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing PII patterns...\n",
      "\n",
      "=== Average PII Indicators Per Email ===\n",
      "\n",
      "In Original Emails:\n",
      "  • Email: 4.70\n",
      "  • Phone: 0.20\n",
      "  • Address: 0.10\n",
      "  • Name: 1.00\n",
      "\n",
      "After Async Modification:\n",
      "  • Email: 2.30\n",
      "  • Phone: 0.20\n",
      "  • Address: 0.00\n",
      "  • Name: 1.00\n",
      "\n",
      "After Sync Modification:\n",
      "  • Email: 2.30\n",
      "  • Phone: 0.20\n",
      "  • Address: 0.00\n",
      "  • Name: 1.00\n"
     ]
    }
   ],
   "source": [
    "def analyze_pii_patterns() -> None:\n",
    "    \"\"\"\n",
    "    Analyze patterns of PII in the original and modified emails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading datasets for pattern analysis...\")\n",
    "        # Use .compute() synchronously\n",
    "        original_df = dataset.df.compute()\n",
    "        async_df = async_modified.df.compute()\n",
    "        sync_df = sync_modified.df.compute()\n",
    "\n",
    "        def count_pii_indicators(text: str) -> dict[str, int]:\n",
    "            \"\"\"Count potential PII indicators in text\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                return {\"email\": 0, \"phone\": 0, \"address\": 0, \"name\": 0}\n",
    "\n",
    "            return {\n",
    "                \"email\": text.count(\"@\"),\n",
    "                \"phone\": sum(\n",
    "                    1 for line in text.split(\"\\n\") if any(p in line.lower() for p in [\"phone:\", \"tel:\", \"mobile:\"])\n",
    "                ),\n",
    "                \"address\": sum(\n",
    "                    1 for line in text.split(\"\\n\")\n",
    "                    if any(a in line.lower() for a in [\"address:\", \"street:\", \"ave.\", \"road\"])\n",
    "                ),\n",
    "                \"name\": sum(\n",
    "                    1 for line in text.split(\"\\n\")\n",
    "                    if any(n in line.lower() for n in [\"name:\", \"mr.\", \"mrs.\", \"ms.\", \"dr.\"])\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        print(\"\\nAnalyzing PII patterns...\")\n",
    "        orig_indicators = [count_pii_indicators(text) for text in original_df[\"text\"]]\n",
    "        async_indicators = [count_pii_indicators(text) for text in async_df[\"text\"]]\n",
    "        sync_indicators = [count_pii_indicators(text) for text in sync_df[\"text\"]]\n",
    "\n",
    "        def calc_averages(indicators: list[dict[str, int]]) -> dict[str, float]:\n",
    "            if not indicators:\n",
    "                return {}\n",
    "            return {k: sum(d[k] for d in indicators) / len(indicators) for k in indicators[0]}\n",
    "\n",
    "        print(\"\\n=== Average PII Indicators Per Email ===\")\n",
    "\n",
    "        print(\"\\nIn Original Emails:\")\n",
    "        for k, v in calc_averages(orig_indicators).items():\n",
    "            print(f\"  • {k.title()}: {v:.2f}\")\n",
    "\n",
    "        print(\"\\nAfter Async Modification:\")\n",
    "        for k, v in calc_averages(async_indicators).items():\n",
    "            print(f\"  • {k.title()}: {v:.2f}\")\n",
    "\n",
    "        print(\"\\nAfter Sync Modification:\")\n",
    "        for k, v in calc_averages(sync_indicators).items():\n",
    "            print(f\"  • {k.title()}: {v:.2f}\")\n",
    "\n",
    "    except (RuntimeError, KeyError, ValueError) as e:\n",
    "        print(f\"An error occurred during pattern analysis: {e!s}\")\n",
    "        print(\"\\nDebug information:\")\n",
    "        print(f\"Is dataset available? {'Yes' if 'dataset' in locals() else 'No'}\")\n",
    "        print(f\"Is async_modified available? {'Yes' if 'async_modified' in locals() else 'No'}\")\n",
    "        print(f\"Is sync_modified available? {'Yes' if 'sync_modified' in locals() else 'No'}\")\n",
    "\n",
    "\n",
    "# Run both analyses\n",
    "print(\"Starting comprehensive PII modification analysis...\")\n",
    "compare_pii_modifications()\n",
    "print(\"\\nAnalyzing PII patterns across all emails...\")\n",
    "analyze_pii_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a69033-df1f-41dd-8355-992113cb77d3",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "• Both methods preserved the email's structure and business content\n",
    "• The meaning and context of the email remained clear despite the modifications\n",
    "• Since both PII modifiers used the same LLM, they produced the same results",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1f5a6-e18a-41c0-aa07-d4135983db49",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This tutorial has demonstrated a practical implementation of PII modification using NeMo Curator's LLM-based modifiers on the Enron email dataset. \n",
    "\n",
    "The PII modification process used an NVIDIA-hosted LLM to identify and redact personally identifiable information in the emails. Every PII identified was replaced by a generic such as `{{name}}` and `{{email}}`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
