{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indic Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from crossfit import op\n",
    "from crossfit.backend.torch.hf.model import HFModel\n",
    "from dask.distributed import get_worker\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.classifiers.base import DistributedDataClassifier\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.utils.distributed_utils import get_client, load_object_on_worker\n",
    "\n",
    "try:\n",
    "    from IndicTransTokenizer import IndicProcessor\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"IndicTransTokenizer not found. Please install it using the following command: \\n\"\n",
    "        + \"pip install git+https://github.com/VarunGumma/IndicTransTokenizer.git\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINAL_PUNCTUATIONS = (\n",
    "    \".\",\n",
    "    \"!\",\n",
    "    \"?\",\n",
    "    \":\",\n",
    "    \",\",\n",
    "    \";\",\n",
    "    \")\",\n",
    "    \"}\",\n",
    "    \"]\",\n",
    "    '\"',\n",
    "    \"'\",\n",
    "    \"”\",\n",
    "    \"’\",\n",
    ")\n",
    "START_PUNCTUATIONS = (\"(\", \"{\", \"[\", \"'\", '\"', \"“\", \"‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `IndicTranslation` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TranslationConfig:\n",
    "    pretrained_model_name_or_path: str\n",
    "    max_length: int = 50\n",
    "    num_beams: int = 5\n",
    "    autocast: bool = False\n",
    "    max_words_per_sen: int = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config: TranslationConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=config.pretrained_model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.autocast = config.autocast\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _forward(self, batch: dict) -> torch.Tensor:\n",
    "        return self.model.generate(\n",
    "            **batch,\n",
    "            use_cache=True,\n",
    "            min_length=0,\n",
    "            max_length=self.config.max_length,\n",
    "            num_beams=self.config.num_beams,\n",
    "            num_return_sequences=1,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: dict) -> torch.Tensor:\n",
    "        if self.autocast:\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs = self._forward(batch)\n",
    "        else:\n",
    "            outputs = self._forward(batch)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelForSeq2SeqModel(HFModel):\n",
    "    def __init__(self, config: TranslationConfig):\n",
    "        self.trans_config = config\n",
    "        self.config = self.load_config()\n",
    "        super().__init__(self.trans_config.pretrained_model_name_or_path)\n",
    "\n",
    "    def load_model(self, device: str = \"cuda\") -> CustomModel:\n",
    "        model = CustomModel(\n",
    "            self.trans_config,\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def load_config(self) -> AutoConfig:\n",
    "        return AutoConfig.from_pretrained(\n",
    "            pretrained_model_name_or_path=self.trans_config.pretrained_model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    @lru_cache(maxsize=1)\n",
    "    def load_tokenizer(self) -> AutoTokenizer:\n",
    "        return AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=self.trans_config.pretrained_model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    def max_seq_length(self) -> int:\n",
    "        return self.config.max_source_positions\n",
    "\n",
    "    def load_cfg(self):\n",
    "        return self.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    ip = load_object_on_worker(\n",
    "        \"IndicProcessor\", IndicProcessor, {\"inference\": True}\n",
    "    )\n",
    "    indices = df[\"text\"].index.to_arrow().to_pylist()\n",
    "    sentences = df[\"text\"].to_arrow().to_pylist()\n",
    "    sentences = ip.preprocess_batch(\n",
    "        sentences, src_lang=\"eng_Latn\", tgt_lang=\"hin_Deva\"\n",
    "    )\n",
    "    df[\"indic_proc_text\"] = cudf.Series(sentences, index=indices)\n",
    "    return df\n",
    "\n",
    "\n",
    "def has_alphabet_characters(text: str) -> bool:\n",
    "    return any(c.isalpha() for c in text)\n",
    "\n",
    "\n",
    "def combine_text(df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    engligh_stop_flag = df[\"text\"].str.endswith(\".\")\n",
    "    hindi_stop_flag = df[\"translation\"].str.endswith(\"|\")\n",
    "    df[\"translation\"][~engligh_stop_flag & hindi_stop_flag] = df[\n",
    "        \"translation\"\n",
    "    ].str.rstrip(\"|\")\n",
    "    df[\"translation\"] = df[\"translation\"].str.strip()\n",
    "    return df\n",
    "\n",
    "def grouping(df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    df = df.to_pandas()\n",
    "    agg_funcs = {\n",
    "        \"translation\": lambda s: \"\".join(s),\n",
    "        \"text\": lambda s: \"\".join(s),\n",
    "    }\n",
    "    other_columns = {\n",
    "        col: \"first\"\n",
    "        for col in df.columns\n",
    "        if col not in agg_funcs and col != \"doc_id\"\n",
    "    }\n",
    "\n",
    "    agg_funcs.update(other_columns)\n",
    "    df = df.groupby(\"doc_id\").agg(agg_funcs).reset_index()\n",
    "    df = cudf.DataFrame.from_pandas(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndicTranslation(DistributedDataClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name_or_path: str = \"ai4bharat/indictrans2-en-indic-1B\",\n",
    "        input_column: str = \"indic_proc_text\",\n",
    "        batch_size: int = 128,\n",
    "        autocast: bool = False,\n",
    "    ):\n",
    "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
    "        self.input_column = input_column\n",
    "        self.batch_size = batch_size\n",
    "        self.autocast = autocast\n",
    "\n",
    "        self.translation_config = TranslationConfig(\n",
    "            pretrained_model_name_or_path=self.pretrained_model_name_or_path,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            autocast=self.autocast,\n",
    "        )\n",
    "        self.model = ModelForSeq2SeqModel(self.translation_config)\n",
    "        super().__init__(\n",
    "            model=self.model,\n",
    "            batch_size=self.batch_size,\n",
    "            device_type=\"cuda\",\n",
    "            autocast=self.autocast,\n",
    "            labels=None,\n",
    "            filter_by=None,\n",
    "            out_dim=None,\n",
    "            pred_column=None,\n",
    "            max_chars=None,\n",
    "        )\n",
    "\n",
    "    def _run_classifier(self, dataset: DocumentDataset) -> DocumentDataset:\n",
    "        ddf = dataset.df\n",
    "        # Applying process_input_text for following :\n",
    "        # 1. nltk tokenization to break doc into sentences\n",
    "        # 2. craeting a row w.r.t each sentence.\n",
    "        # 3. Process sentences strip symbols from start and end\n",
    "        ddf = ddf.map_partitions(self.process_input_text, enforce_metadata=False)\n",
    "        ddf[\"text\"] = ddf[\"text\"].astype(\"str\")\n",
    "\n",
    "        ddf[\"word_count\"] = ddf[\"text\"].str.split().list.len()\n",
    "        ddf[\"word_count\"] = ddf[\"word_count\"].astype(\"int64\")\n",
    "        ddf_true = ddf[(ddf[\"word_count\"] <= self.translation_config.max_words_per_sen)]\n",
    "        # To filter for atleast one unicode letter in text\n",
    "        has_letter = ddf_true.map_partitions(self.atleast_letter, column_name=\"text\")\n",
    "        ddf_trans = ddf_true[has_letter[\"isalpha\"]]\n",
    "        ddf = ddf_trans.drop(columns=\"word_count\")\n",
    "        ## ddf false operations\n",
    "        ddf_false = ddf_true[~has_letter[\"isalpha\"]]\n",
    "        ddf_false = ddf_false.drop(columns=\"word_count\")\n",
    "        ddf_false[\"translation\"] = ddf_false[\"text\"]\n",
    "        # Applying preprocess_df for Indic preprocessing\n",
    "        ddf[\"text\"] = ddf[\"text\"].astype(\"str\")\n",
    "        ddf_meta = ddf._meta.copy()\n",
    "        ddf_meta[\"indic_proc_text\"] = \"\"\n",
    "        ddf = ddf.map_partitions(self.preprocess_df, meta=ddf_meta)\n",
    "\n",
    "        columns = ddf.columns.tolist()\n",
    "        pipe = op.Sequential(\n",
    "            op.Tokenizer(\n",
    "                self.model, cols=[self.input_column], tokenizer_type=\"default\"\n",
    "            ),\n",
    "            op.Predictor(\n",
    "                self.model,\n",
    "                sorted_data_loader=True,\n",
    "                batch_size=self.batch_size,\n",
    "                pred_output_col=\"translation\",\n",
    "            ),\n",
    "            keep_cols=columns,\n",
    "        )\n",
    "        ddf = pipe(ddf)\n",
    "        translated_meta = ddf._meta.copy()\n",
    "        translated_meta[\"translation\"] = \"DUMMY_STRING\"\n",
    "        ddf = ddf.map_partitions(self.translate_tokens, meta=translated_meta)\n",
    "        ddf = ddf.map_partitions(self.combine_text, meta=translated_meta)\n",
    "\n",
    "        # Merging translated and non-translated samples\n",
    "        ddf_true[\"false_translation\"] = ddf_false[\"translation\"]\n",
    "        ddf_true[\"false_translation\"] = ddf_true[\"false_translation\"].fillna(\"\")\n",
    "        ddf_true[\"translation\"] = ddf[\"translation\"]\n",
    "        ddf_true[\"translation\"] = ddf_true[\"translation\"].fillna(\"\")\n",
    "        ddf_true[\"translation\"] = (\n",
    "            ddf_true[\"translation\"] + ddf_true[\"false_translation\"]\n",
    "        )\n",
    "\n",
    "        ddf = ddf_true.map_partitions(self.grouping)\n",
    "        return DocumentDataset(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tokens(self, df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    worker = get_worker()\n",
    "    if hasattr(worker, \"IndicProcessor\"):\n",
    "        ip = getattr(worker, \"IndicProcessor\")\n",
    "    else:\n",
    "        ip = load_object_on_worker(\n",
    "            \"IndicProcessor\", IndicProcessor, {\"inference\": True}\n",
    "        )\n",
    "    tokenizer = self.model.load_tokenizer()\n",
    "    indices = df[\"translation\"].index.to_arrow().to_pylist()\n",
    "    generated_tokens = df[\"translation\"].to_arrow().to_pylist()\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        generated_tokens = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "    generated_tokens = ip.postprocess_batch(generated_tokens, lang=\"hin_Deva\")\n",
    "    df[\"translation\"] = cudf.Series(data=generated_tokens, index=indices)\n",
    "    return df\n",
    "\n",
    "IndicTranslation.translate_tokens = translate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(self, text: str):\n",
    "    split_text = re.split(\n",
    "        r\"(\\#{2,}|\\_{2,}|\\…{2,}|\\+{2,}|\\.{2,}|\\-{3,}|\\*{2,}|\\~{2,}|\\={2,}|\\!{2,}|\\n|\\t|\\‣|\\⁃|\\⁌|\\⁍|\\●|\\○|\\•|\\·|\\◘|\\◦|\\⦾|\\⦿|\\|)\",\n",
    "        text,\n",
    "    )\n",
    "    split_text = [s for s in split_text if len(s) > 0]\n",
    "    tokenized_sentences = []\n",
    "    len_flag = False\n",
    "    for line in split_text:\n",
    "        # Tokenize sentences using NLTK's sent_tokenize function\n",
    "        if self.has_alphabet_characters(line) == True:\n",
    "            sentences = sent_tokenize(line)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            curr_tokenized_snt = []\n",
    "            non_translation_str = \"\"\n",
    "            # Comparing the list of tokenized sentences (using NLTK) and actual sentence and preserving the spaces,\n",
    "            # newline and other special characters\n",
    "            while i < len(line):\n",
    "                if j < len(sentences):\n",
    "                    stripped_sent = sentences[j].strip()\n",
    "                    if len(stripped_sent) == 0:\n",
    "                        j += 1\n",
    "                        continue\n",
    "                    # If tokenized sentence matches then moving to next sentence\n",
    "                    if line[i] == stripped_sent[0]:\n",
    "                        if non_translation_str != \"\":\n",
    "                            curr_tokenized_snt.append(non_translation_str)\n",
    "                        curr_tokenized_snt.append(stripped_sent)\n",
    "                        i += len(stripped_sent)\n",
    "                        j += 1\n",
    "                        non_translation_str = \"\"\n",
    "                    else:\n",
    "                        non_translation_str += line[i]\n",
    "                        i += 1\n",
    "                else:\n",
    "                    non_translation_str += line[i]\n",
    "                    i += 1\n",
    "            if non_translation_str != \"\":\n",
    "                curr_tokenized_snt.append(non_translation_str)\n",
    "            # Add the tokenized sentences to the list\n",
    "            tokenized_sentences.extend(curr_tokenized_snt)\n",
    "        else:\n",
    "            tokenized_sentences.append(line)\n",
    "\n",
    "    tokenized_sentence_len = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sent = sentence.split()\n",
    "        # removing the sentences with word length greater than threshold as the model may not be able translate it due to constraint on output token size\n",
    "        if len(sent) <= self.translation_config.max_words_per_sen:\n",
    "            tokenized_sentence_len.append(sentence)\n",
    "\n",
    "    return tokenized_sentence_len\n",
    "\n",
    "IndicTranslation.custom_tokenize = custom_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_text(self, df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    df = df.to_pandas()\n",
    "    df[\"text\"] = df[\"text\"].apply(self.custom_tokenize)\n",
    "    df[\"doc_id\"] = np.arange(1, len(df) + 1)\n",
    "    df = df.explode(\"text\", ignore_index=True)\n",
    "    df = df.reset_index(drop=False)\n",
    "    df = cudf.DataFrame.from_pandas(df)\n",
    "    return df\n",
    "\n",
    "def atleast_letter(self, df: cudf.DataFrame, column_name: str) -> cudf.DataFrame:\n",
    "    df = df.to_pandas()\n",
    "    df[\"isalpha\"] = df[column_name].apply(self.has_alphabet_characters)\n",
    "    df = cudf.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "IndicTranslation.process_input_text = process_input_text\n",
    "IndicTranslation.atleast_letter = atleast_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Indic Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\"\n",
    "client = get_client(device=device)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_field = \"text\"\n",
    "batch_size = 128\n",
    "autocast = True\n",
    "\n",
    "translator_model = IndicTranslation(\n",
    "    pretrained_model_name_or_path=\"ai4bharat/indictrans2-en-indic-1B\",\n",
    "    input_column=input_text_field,\n",
    "    batch_size=batch_size,\n",
    "    autocast=autocast,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir = \"input_data_dir/\"\n",
    "input_files = [\n",
    "    os.path.join(input_data_dir, x) for x in os.listdir(input_data_dir)\n",
    "]\n",
    "input_dataset = DocumentDataset.read_json(\n",
    "    input_files, backend=\"cudf\", add_filename=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset = translator_model(dataset=input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_dir = \"output_data_dir/\"\n",
    "result_dataset.to_json(output_file_dir=output_data_dir, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeMo-Curator-env-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
