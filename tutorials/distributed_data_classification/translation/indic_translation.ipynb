{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Indic Translation Pipeline with NeMo Curator\n",
    "\n",
    "In this tutorial, we use the [IndicTransToolkit](https://github.com/VarunGumma/IndicTransToolkit) library, [IndicTrans2](https://huggingface.co/ai4bharat/indictrans2-en-indic-1B) model from Hugging Face, [Dask](https://www.dask.org/), and NeMo Curator to build an Indic language translation pipeline. After creating the pipeline, we demonstrate how to use the model to translate English text to Hindi text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK if not already installed\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/nfs/syurick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "import dask_cudf\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from crossfit import op\n",
    "from crossfit.backend.torch.hf.model import HFModel\n",
    "from dask.distributed import get_worker\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.classifiers.base import DistributedDataClassifier\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.utils.distributed_utils import get_client, load_object_on_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [IndicTransToolkit](https://github.com/VarunGumma/IndicTransToolkit) provides a simple, modular, and extendable toolkit for [IndicTrans2](https://github.com/AI4Bharat/IndicTrans2), an open-source transformer-based multilingual NMT model that supports high-quality translations across all the 22 scheduled Indic languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IndicTransToolkit import IndicProcessor\n",
    "except ModuleNotFoundError:\n",
    "    raise ImportError(\n",
    "        \"IndicTransToolkit not found. Please install it using the following command: \\n\"\n",
    "        + \"pip install git+https://github.com/VarunGumma/IndicTransToolkit.git\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to add `transformers_modules` from the Hugging Face cache to the Python path. First, we can download and cache the model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndicTransForConditionalGeneration(\n",
       "  (model): IndicTransModel(\n",
       "    (encoder): IndicTransEncoder(\n",
       "      (embed_tokens): Embedding(32322, 1024, padding_idx=1)\n",
       "      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x IndicTransEncoderLayer(\n",
       "          (self_attn): IndicTransAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): IndicTransDecoder(\n",
       "      (embed_tokens): Embedding(122672, 1024, padding_idx=1)\n",
       "      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x IndicTransDecoderLayer(\n",
       "          (self_attn): IndicTransAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): IndicTransAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=122672, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the Hugging Face cache is located in the user's home directory, we can then add the `transformers_modules` directory to the Python path with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_modules_path = os.path.expanduser(\"~/.cache/huggingface/modules/transformers_modules\")\n",
    "sys.path.append(hf_modules_path)\n",
    "\n",
    "if \"transformers_modules\" not in sys.modules:\n",
    "    spec = importlib.util.spec_from_file_location(\"transformers_modules\", os.path.join(hf_modules_path, \"__init__.py\"))\n",
    "    if spec and spec.loader:\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        sys.modules[\"transformers_modules\"] = module\n",
    "\n",
    "module_name = \"transformers_modules.ai4bharat\"\n",
    "module_path = os.path.join(hf_modules_path, \"ai4bharat\")\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(module_name, os.path.join(module_path, \"__init__.py\"))\n",
    "if spec and spec.loader:\n",
    "    ai4bharat_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(ai4bharat_module)\n",
    "    sys.modules[module_name] = ai4bharat_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes and Functions for the `IndicTranslation` Class\n",
    "\n",
    "To create our Indic translation classifier, we create an `IndicTranslation` class, which will be extended from NeMo Curator's `DistributedDataClassifier` class.\n",
    "\n",
    "The goal of the base `DistributedDataClassifier` class is to enable multi-node multi-GPU data classification of your data. NeMo Curator provides several subclasses that focus on various tasks, such as domain and quality classification. However, the `DistributedDataClassifier` can be extended to fit *any* model; the only requirement is that the model can fit on a single GPU. See NeMo Curator's [Distributed Data Classification](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/distributeddataclassification.html) documentation for more information.\n",
    "\n",
    "First, let's create a `TranslationConfig` class. Its purpose is to store some of the attributes that will be used by our model, including the model card of the [IndicTrans2 En-Indic 1.1B variant](https://huggingface.co/ai4bharat/indictrans2-en-indic-1B) on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TranslationConfig:\n",
    "    pretrained_model_name_or_path: str = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "    max_length: int = 50\n",
    "    num_beams: int = 5\n",
    "    autocast: bool = False\n",
    "    max_words_per_sen: int = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a `CustomModel` class for sequence-to-sequence language modeling. It inherits from `nn.Module`, the base class for all neural network modules in PyTorch.\n",
    "\n",
    "Inside `__init__`, the model loads a pre-trained sequence-to-sequence model (`AutoModelForSeq2SeqLM`) from Hugging Face, using the model name provided. The `autocast` boolean determines whether mixed precision (`torch.autocast`) is used during inference to speed up computations on CUDA devices; we set it to False above.\n",
    "\n",
    "The `_forward` method performs text generation on the input batch without tracking gradients (`@torch.no_grad()`), which is efficient for inference. `self.model.generate()` is called with the batch inputs and several generation parameters to control the decoding behavior. The `forward` method is required by `nn.Module` and runs the model's forward pass (the computation performed at every call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config: TranslationConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=config.pretrained_model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.autocast = config.autocast\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _forward(self, batch: dict) -> torch.Tensor:\n",
    "        return self.model.generate(\n",
    "            **batch,\n",
    "            use_cache=True,\n",
    "            min_length=0,\n",
    "            max_length=self.config.max_length,\n",
    "            num_beams=self.config.num_beams,\n",
    "            num_return_sequences=1,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: dict) -> torch.Tensor:\n",
    "        if self.autocast:\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs = self._forward(batch)\n",
    "        else:\n",
    "            outputs = self._forward(batch)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the `ModelForSeq2SeqModel` class, a model management class that handles loading configurations, the `CustomModel`, and tokenizers for sequence-to-sequence translation. It inherits from `HFModel`, a class created by NVIDIA's [CrossFit](https://github.com/rapidsai/crossfit) library, which enables multi-node and multi-GPU offline inference.\n",
    "\n",
    "In it, we create several methods which define how to load our model, its configuration, and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelForSeq2SeqModel(HFModel):\n",
    "    def __init__(self, config: TranslationConfig):\n",
    "        self.trans_config = config\n",
    "        self.config = self.load_config()\n",
    "        super().__init__(self.trans_config.pretrained_model_name_or_path)\n",
    "\n",
    "    def load_model(self, device: str = \"cuda\") -> CustomModel:\n",
    "        model = CustomModel(self.trans_config)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def load_config(self) -> AutoConfig:\n",
    "        return AutoConfig.from_pretrained(\n",
    "            pretrained_model_name_or_path=self.trans_config.pretrained_model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    def load_tokenizer(self) -> AutoTokenizer:\n",
    "        return AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=self.trans_config.pretrained_model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    def max_seq_length(self) -> int:\n",
    "        return self.config.max_source_positions\n",
    "\n",
    "    def load_cfg(self):\n",
    "        return self.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define some helper functions which will be used by our `IndicTranslation` class.\n",
    "\n",
    "The `preprocess_df` function is used to load and run the `IndicProcessor` to preprocess our English sentences before tokenization. Note our use of the `load_object_on_worker` function, which loads and stores the `IndicProcessor` on each Dask worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df: cudf.DataFrame, text_field: str = \"text\") -> cudf.DataFrame:\n",
    "    ip = load_object_on_worker(\n",
    "        \"IndicProcessor\", IndicProcessor, {\"inference\": True}\n",
    "    )\n",
    "\n",
    "    indices = df[text_field].index.to_arrow().to_pylist()\n",
    "    sentences = df[text_field].to_arrow().to_pylist()\n",
    "    sentences = ip.preprocess_batch(\n",
    "        sentences, src_lang=\"eng_Latn\", tgt_lang=\"hin_Deva\"\n",
    "    )\n",
    "\n",
    "    df[\"indic_proc_text\"] = cudf.Series(sentences, index=indices)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `has_alphabet_characters` function checks if there is at least one alphabetic character in a given string; the `atleast_letter` function applies it to a DataFrame column to produce another column of booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_alphabet_characters(text: str) -> bool:\n",
    "    return any(c.isalpha() for c in text)\n",
    "\n",
    "\n",
    "def atleast_letter(df: cudf.DataFrame, text_field: str) -> cudf.DataFrame:\n",
    "    df = df.to_pandas()\n",
    "    df[\"isalpha\"] = df[text_field].apply(has_alphabet_characters)\n",
    "    df = cudf.DataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After translating our text, the `combine_text` function modifies the translated column by removing the vertical bar `|` (which is used as a stop marker in our translations) at the end of the text where: (1) the text does not end with a period and (2) the translation ends with a vertical bar. Thus, we keep translations ending with a vertical bar only when the English text ends with a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df: cudf.DataFrame, text_field: str = \"text\") -> cudf.DataFrame:\n",
    "    english_stop_flag = df[text_field].str.endswith(\".\")\n",
    "    hindi_stop_flag = df[\"translation\"].str.endswith(\"|\")\n",
    "\n",
    "    df[\"translation\"][~english_stop_flag & hindi_stop_flag] = df[\n",
    "        \"translation\"\n",
    "    ].str.rstrip(\"|\")\n",
    "\n",
    "    df[\"translation\"] = df[\"translation\"].str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grouping` function groups rows by `doc_id`, concatenates text-based columns, and retains the first value of other columns within each group. This is useful because our texts will be spread across several rows, but marked with the same `doc_id`. Thus, we use this function to combine those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping(df: cudf.DataFrame, text_field: str = \"text\") -> cudf.DataFrame:\n",
    "    df = df.to_pandas()\n",
    "\n",
    "    agg_funcs = {\n",
    "        \"translation\": lambda s: \"\".join(s),\n",
    "        text_field: lambda s: \"\".join(s),\n",
    "    }\n",
    "\n",
    "    other_columns = {\n",
    "        col: \"first\"\n",
    "        for col in df.columns\n",
    "        if col not in agg_funcs and col != \"doc_id\"\n",
    "    }\n",
    "\n",
    "    agg_funcs.update(other_columns)\n",
    "    df = df.groupby(\"doc_id\").agg(agg_funcs).reset_index()\n",
    "    df = cudf.DataFrame.from_pandas(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the `IndicTranslation` Class\n",
    "\n",
    "Our `IndicTranslation` class is a bit of a monster, containing many methods within it. For this tutorial, we aim to make it as digestible as possible by stepping through each method, one by one.\n",
    "\n",
    "While this first method may look intimidating, its goal is very simple: create a list of sentences from a given string. It does this by using NLTK tokenization to break the text into sentences. We also remove sentences that are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(self, text: str):\n",
    "    split_text = re.split(\n",
    "        r\"(\\#{2,}|\\_{2,}|\\…{2,}|\\+{2,}|\\.{2,}|\\-{3,}|\\*{2,}|\\~{2,}|\\={2,}|\\!{2,}|\\n|\\t|\\‣|\\⁃|\\⁌|\\⁍|\\●|\\○|\\•|\\·|\\◘|\\◦|\\⦾|\\⦿|\\|)\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    split_text = [s for s in split_text if len(s) > 0]\n",
    "    tokenized_sentences = []\n",
    "    len_flag = False\n",
    "\n",
    "    for line in split_text:\n",
    "        # Tokenize sentences using NLTK's sent_tokenize function\n",
    "        if has_alphabet_characters(line) == True:\n",
    "            sentences = sent_tokenize(line)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            curr_tokenized_snt = []\n",
    "            non_translation_str = \"\"\n",
    "\n",
    "            # Comparing the list of tokenized sentences (using NLTK) and actual the sentence,\n",
    "            # preserving the spaces, newline and other special characters\n",
    "            while i < len(line):\n",
    "                if j < len(sentences):\n",
    "                    stripped_sent = sentences[j].strip()\n",
    "\n",
    "                    if len(stripped_sent) == 0:\n",
    "                        j += 1\n",
    "                        continue\n",
    "\n",
    "                    # If tokenized sentence matches, then moving to next sentence\n",
    "                    if line[i] == stripped_sent[0]:\n",
    "                        if non_translation_str != \"\":\n",
    "                            curr_tokenized_snt.append(non_translation_str)\n",
    "\n",
    "                        curr_tokenized_snt.append(stripped_sent)\n",
    "                        i += len(stripped_sent)\n",
    "                        j += 1\n",
    "                        non_translation_str = \"\"\n",
    "\n",
    "                    else:\n",
    "                        non_translation_str += line[i]\n",
    "                        i += 1\n",
    "\n",
    "                else:\n",
    "                    non_translation_str += line[i]\n",
    "                    i += 1\n",
    "\n",
    "            if non_translation_str != \"\":\n",
    "                curr_tokenized_snt.append(non_translation_str)\n",
    "\n",
    "            # Add the tokenized sentences to the list\n",
    "            tokenized_sentences.extend(curr_tokenized_snt)\n",
    "\n",
    "        else:\n",
    "            tokenized_sentences.append(line)\n",
    "\n",
    "    tokenized_sentence_len = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sent = sentence.split()\n",
    "        # Removing the sentences with word length greater than threshold\n",
    "        # Since the model may not be able translate it due to constraint on output token size\n",
    "        if len(sent) <= self.translation_config.max_words_per_sen:\n",
    "            tokenized_sentence_len.append(sentence)\n",
    "\n",
    "    return tokenized_sentence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses the `custom_tokenize` method above to create a DataFrame where each sentence has its own row, preserving the `doc_id` for context.\n",
    "\n",
    "For example, if we have the DataFrame:\n",
    "\n",
    "| text                                                   |\n",
    "|--------------------------------------------------------|\n",
    "| \"This is a first sentence. This is a second sentence.\" |\n",
    "| \"This is a third sentence. This is a fourth sentence.\" |\n",
    "\n",
    "Then the resulting DataFrame will be:\n",
    "\n",
    "| text                         | doc_id |\n",
    "|------------------------------|--------|\n",
    "| \"This is a first sentence.\"  | 1      |\n",
    "| \"This is a second sentence.\" | 1      |\n",
    "| \"This is a third sentence.\"  | 2      |\n",
    "| \"This is a fourth sentence.\" | 2      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_text(self, df: cudf.DataFrame, text_field: str = \"text\") -> cudf.DataFrame:\n",
    "    df = df.to_pandas()\n",
    "    df[text_field] = df[text_field].apply(self.custom_tokenize)\n",
    "    df[\"doc_id\"] = np.arange(1, len(df) + 1)\n",
    "    df = df.explode(text_field, ignore_index=True)\n",
    "    df = df.reset_index(drop=False)\n",
    "    df = cudf.DataFrame.from_pandas(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our translations are generated, the `translate_tokens` method further processes the translations by decoding the tokens back to human-readable text and applying postprocessing with the `IndicProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tokens(self, df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    worker = get_worker()\n",
    "\n",
    "    if hasattr(worker, \"IndicProcessor\"):\n",
    "        ip = getattr(worker, \"IndicProcessor\")\n",
    "    else:\n",
    "        ip = load_object_on_worker(\n",
    "            \"IndicProcessor\", IndicProcessor, {\"inference\": True}\n",
    "        )\n",
    "\n",
    "    tokenizer = self.model.load_tokenizer()\n",
    "    indices = df[\"translation\"].index.to_arrow().to_pylist()\n",
    "    generated_tokens = df[\"translation\"].to_arrow().to_pylist()\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        generated_tokens = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "    generated_tokens = ip.postprocess_batch(generated_tokens, lang=\"hin_Deva\")\n",
    "    df[\"translation\"] = cudf.Series(data=generated_tokens, index=indices)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the `IndicTranslation` class by defining the `__init__` and `_run_classifier` methods. We start with the `__init__` method, which uses the `DistributedDataClassifier`, `TranslationConfig`, and `ModelForSeq2SeqModel` classes described above.\n",
    "\n",
    "We then combine all of the helper functions and class methods into the `_run_classifier` method. This is the method that is called by `DistributedDataClassifier`'s `__call__` method; it is required for all classes that inherit the `DistributedDataClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndicTranslation(DistributedDataClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name_or_path: str = \"ai4bharat/indictrans2-en-indic-1B\",\n",
    "        text_field: str = \"text\",\n",
    "        batch_size: int = 128,\n",
    "        autocast: bool = False,\n",
    "    ):\n",
    "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
    "        self.text_field = text_field\n",
    "        self.batch_size = batch_size\n",
    "        self.autocast = autocast\n",
    "\n",
    "        self.translation_config = TranslationConfig(\n",
    "            pretrained_model_name_or_path=self.pretrained_model_name_or_path,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            autocast=self.autocast,\n",
    "        )\n",
    "\n",
    "        self.model = ModelForSeq2SeqModel(self.translation_config)\n",
    "\n",
    "        super().__init__(\n",
    "            model=self.model,\n",
    "            batch_size=self.batch_size,\n",
    "            device_type=\"cuda\",\n",
    "            autocast=self.autocast,\n",
    "            labels=None,\n",
    "            filter_by=None,\n",
    "            out_dim=None,\n",
    "            pred_column=None,\n",
    "            max_chars=None,\n",
    "        )\n",
    "\n",
    "    def _run_classifier(self, dataset: DocumentDataset) -> DocumentDataset:\n",
    "        ddf = dataset.df\n",
    "        # See process_input_text helper function defined above\n",
    "        ddf = ddf.map_partitions(self.process_input_text, text_field=self.text_field, enforce_metadata=False)\n",
    "        ddf[self.text_field] = ddf[self.text_field].astype(\"str\")\n",
    "\n",
    "        ddf[\"word_count\"] = ddf[self.text_field].str.split().list.len()\n",
    "        ddf[\"word_count\"] = ddf[\"word_count\"].astype(\"int64\")\n",
    "        ddf_true = ddf[(ddf[\"word_count\"] <= self.translation_config.max_words_per_sen)]\n",
    "\n",
    "        # Filter for at least one unicode letter in text\n",
    "        # See atleast_letter helper function defined above\n",
    "        has_letter = ddf_true.map_partitions(atleast_letter, text_field=self.text_field)\n",
    "        ddf_trans = ddf_true[has_letter[\"isalpha\"]]\n",
    "        ddf = ddf_trans.drop(columns=\"word_count\")\n",
    "\n",
    "        ## ddf_false operations\n",
    "        ddf_false = ddf_true[~has_letter[\"isalpha\"]]\n",
    "        ddf_false = ddf_false.drop(columns=\"word_count\")\n",
    "        ddf_false[\"translation\"] = ddf_false[self.text_field]\n",
    "\n",
    "        # Applying preprocess_df helper function for Indic preprocessing\n",
    "        ddf[self.text_field] = ddf[self.text_field].astype(\"str\")\n",
    "        ddf_meta = ddf._meta.copy()\n",
    "        ddf_meta[\"indic_proc_text\"] = \"\"\n",
    "        ddf = ddf.map_partitions(preprocess_df, text_field=self.text_field, meta=ddf_meta)\n",
    "\n",
    "        columns = ddf.columns.tolist()\n",
    "        pipe = op.Sequential(\n",
    "            # This step tokenizes the input text found in the specified text_field\n",
    "            op.Tokenizer(\n",
    "                self.model, cols=[self.text_field], tokenizer_type=\"default\"\n",
    "            ),\n",
    "            # The Predictor takes the tokenized input and passes it through the model to generate translations\n",
    "            op.Predictor(\n",
    "                self.model,\n",
    "                sorted_data_loader=True,\n",
    "                batch_size=self.batch_size,\n",
    "                pred_output_col=\"translation\",\n",
    "            ),\n",
    "            keep_cols=columns,\n",
    "        )\n",
    "        ddf = pipe(ddf)\n",
    "        translated_meta = ddf._meta.copy()\n",
    "        translated_meta[\"translation\"] = \"DUMMY_STRING\"\n",
    "        ddf = ddf.map_partitions(self.translate_tokens, meta=translated_meta)\n",
    "        ddf = ddf.map_partitions(combine_text, text_field=self.text_field, meta=translated_meta)\n",
    "\n",
    "        # Merging translated and non-translated samples\n",
    "        ddf_true[\"false_translation\"] = ddf_false[\"translation\"]\n",
    "        ddf_true[\"false_translation\"] = ddf_true[\"false_translation\"].fillna(\"\")\n",
    "        ddf_true[\"translation\"] = ddf[\"translation\"]\n",
    "        ddf_true[\"translation\"] = ddf_true[\"translation\"].fillna(\"\")\n",
    "        ddf_true[\"translation\"] = (\n",
    "            ddf_true[\"translation\"] + ddf_true[\"false_translation\"]\n",
    "        )\n",
    "\n",
    "        # See grouping helper function defined above\n",
    "        ddf = ddf_true.map_partitions(grouping, text_field=self.text_field)\n",
    "\n",
    "        ddf = ddf.drop(columns=[\"index\", \"word_count\", \"false_translation\"])\n",
    "        return DocumentDataset(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the functions defined above to the IndicTranslation class\n",
    "IndicTranslation.custom_tokenize = custom_tokenize\n",
    "IndicTranslation.process_input_text = process_input_text\n",
    "IndicTranslation.translate_tokens = translate_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Indic Translation \n",
    "\n",
    "We have successfully built our Indic translation pipeline! Now, let's demonstrate how to use it with a simple example.\n",
    "\n",
    "First, let's create a Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Spilling is enabled\n"
     ]
    }
   ],
   "source": [
    "device = \"gpu\"\n",
    "client = get_client(cluster_type=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a `DocumentDataset` with some English sentences to translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Quantum computing is set to revolutionize the field of cryptography.\",\n",
    "    \"Investing in index funds is a popular strategy for long-term financial growth.\",\n",
    "    \"Recent advancements in gene therapy offer new hope for treating genetic disorders.\",\n",
    "    \"Online learning platforms have transformed the way students access educational resources.\",\n",
    "    \"Traveling to Europe during the off-season can be a more budget-friendly option.\",\n",
    "    \"Training regimens for athletes have become more sophisticated with the use of data analytics.\",\n",
    "    \"Streaming services are changing the way people consume television and film content.\",\n",
    "    \"Vegan recipes have gained popularity as more people adopt plant-based diets.\",\n",
    "    \"Climate change research is critical for developing sustainable environmental policies.\",\n",
    "    \"Telemedicine has become increasingly popular due to its convenience and accessibility.\",\n",
    "]\n",
    "df = cudf.DataFrame({\"text\": text})\n",
    "input_dataset = DocumentDataset(dask_cudf.from_cudf(df, npartitions=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can initialize our `IndicTranslation` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.4.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_field = \"text\"\n",
    "batch_size = 128\n",
    "autocast = True\n",
    "\n",
    "translator_model = IndicTranslation(\n",
    "    pretrained_model_name_or_path=\"ai4bharat/indictrans2-en-indic-1B\",\n",
    "    text_field=text_field,\n",
    "    batch_size=batch_size,\n",
    "    autocast=autocast,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's translate our text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset = translator_model(dataset=input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU: tcp://127.0.0.1:34409, Part: 0: 100%|██████████| 10/10 [00:13<00:00,  1.34s/it]\n",
      "/home/nfs/syurick/miniforge3/envs/nemo_curator/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>translation</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>क्वाण्टम कम्प्यूटिंग क्रिप्टोग्राफी के क्षेत्र...</td>\n",
       "      <td>Quantum computing is set to revolutionize the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>इन्डेक्स फंड्स (अनुक्रमणिका फंड्स) में निवेश द...</td>\n",
       "      <td>Investing in index funds is a popular strategy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>जीन चिकित्सा में हालिया प्रगति आनुवंशिक विकारो...</td>\n",
       "      <td>Recent advancements in gene therapy offer new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ऑनलाइन लर्निंग (ऑनलाइन शिक्षण) प्लेटफॉर्म ने छ...</td>\n",
       "      <td>Online learning platforms have transformed the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"...</td>\n",
       "      <td>Traveling to Europe during the off-season can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>डेटा एनालिटिक्स के उपयोग के साथ एथलीटों के लिए...</td>\n",
       "      <td>Training regimens for athletes have become mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>स्ट्रीमिंग स्ट्रीमिंग सेवाएँ टेलीविजन और फिल्म...</td>\n",
       "      <td>Streaming services are changing the way people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>वेगन वेज के व्यंजनों ने लोकप्रियता हासिल कर ली...</td>\n",
       "      <td>Vegan recipes have gained popularity as more p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>जलवायु परिवर्तन संशोधन-परिवर्तन अनुसंधान, टिका...</td>\n",
       "      <td>Climate change research is critical for develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>टेलिमेडिसिन टेलिमेडिसिन (TELMED), सुविधा और सु...</td>\n",
       "      <td>Telemedicine has become increasingly popular d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                        translation  \\\n",
       "0       1  क्वाण्टम कम्प्यूटिंग क्रिप्टोग्राफी के क्षेत्र...   \n",
       "1       2  इन्डेक्स फंड्स (अनुक्रमणिका फंड्स) में निवेश द...   \n",
       "2       3  जीन चिकित्सा में हालिया प्रगति आनुवंशिक विकारो...   \n",
       "3       4  ऑनलाइन लर्निंग (ऑनलाइन शिक्षण) प्लेटफॉर्म ने छ...   \n",
       "4       5  \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"...   \n",
       "5       6  डेटा एनालिटिक्स के उपयोग के साथ एथलीटों के लिए...   \n",
       "6       7  स्ट्रीमिंग स्ट्रीमिंग सेवाएँ टेलीविजन और फिल्म...   \n",
       "7       8  वेगन वेज के व्यंजनों ने लोकप्रियता हासिल कर ली...   \n",
       "8       9  जलवायु परिवर्तन संशोधन-परिवर्तन अनुसंधान, टिका...   \n",
       "9      10  टेलिमेडिसिन टेलिमेडिसिन (TELMED), सुविधा और सु...   \n",
       "\n",
       "                                                text  \n",
       "0  Quantum computing is set to revolutionize the ...  \n",
       "1  Investing in index funds is a popular strategy...  \n",
       "2  Recent advancements in gene therapy offer new ...  \n",
       "3  Online learning platforms have transformed the...  \n",
       "4  Traveling to Europe during the off-season can ...  \n",
       "5  Training regimens for athletes have become mor...  \n",
       "6  Streaming services are changing the way people...  \n",
       "7  Vegan recipes have gained popularity as more p...  \n",
       "8  Climate change research is critical for develo...  \n",
       "9  Telemedicine has become increasingly popular d...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataset.df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we close our Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for following this tutorial! We have demonstrated how to create and run an Indic translation pipeline in NeMo Curator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo_curator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
