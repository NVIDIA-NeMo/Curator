{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd01afc",
   "metadata": {},
   "source": [
    "# Nemo Curator With PySpark Example\n",
    "\n",
    "## NeMo Curator Introduction\n",
    "NeMo Curator is a Python library that consists of a collection of scalable data processing modules for curating natural language processing (NLP) data for training large language models (LLMs). The modules within the NeMo Data Curator enable NLP researchers to mine high-quality text at scale from massive uncurated web corpora. \n",
    "\n",
    "NeMo Curator includes the following modules to perform data curation:\n",
    "- Data download and Extraction\n",
    "- Language identification and separation\n",
    "- Text reformatting and cleaning\n",
    "- Quality filtering\n",
    "- Document-level deduplication\n",
    "- Multilingual downstream-task decontamination\n",
    "- Distributed Data Classification\n",
    "- Personal identifiable information (PII) redaction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1808ea",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "\n",
    "This notebook will use the **Tiny Stories [Dataset](https://huggingface.co/datasets/roneneldan/TinyStories)** as an example to demonstrate how you can run data curation using NeMo Curator, integrate additional data processing you may already have on PySpark and build and end to end curation pipeline. \n",
    "\n",
    "Step description:\n",
    "1. Download and extract data using Nemo Curator\n",
    "2. Clean and process data using Nemo Curator\n",
    "3. Perform additional processing using PySpark\n",
    "4. Deduplication using Nemo Curator\n",
    "\n",
    "For a full working example of Nemo Curator, please refer this [tutorial](https://github.com/NVIDIA/NeMo-Curator/blob/main/tutorials/single_node_tutorial/single_gpu_tutorial.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78537bd7",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### System Requirements\n",
    "Here is the hardware setting for this notebook\n",
    "\n",
    "**GPU**: NVIDIA A10 24G. \n",
    "\n",
    "**CUDA & Nvidia Drivers**: CUDA 12.2 with Driver 535.154.05\n",
    "\n",
    "**OS**: ubuntu 22.04\n",
    "\n",
    "### Getting NeMo Framework Training Container\n",
    "- Get access to the container via https://developer.nvidia.com/nemo-framework\n",
    "- Set your docker credentials \n",
    "    ```bash\n",
    "    docker login nvcr.io\n",
    "\n",
    "    Username: $oauthtoken\n",
    "    Password: <Your NGC Key>\n",
    "- Get NeMo NeMo Framework Training Container\n",
    "    ```bash\n",
    "    docker pull nvcr.io/nvidia/nemo:dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b5423",
   "metadata": {},
   "source": [
    "## 0. Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add9bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97079227-d9c3-40d2-b939-64b221b86990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env DASK_DATAFRAME__QUERY_PLANNING False\n",
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940c70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from nemo_curator.utils.distributed_utils import get_client,get_num_workers\n",
    "from nemo_curator.utils.script_utils import ArgumentHelper\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under, separate_by_metadata\n",
    "from nemo_curator.utils.distributed_utils import read_data,write_to_disk\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a381d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_imports():\n",
    "    import cudf \n",
    "\n",
    "def attach_args(parser=argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)):\n",
    "    return ArgumentHelper(parser).add_distributed_args()\n",
    "\n",
    "def check_jsonl_file(file_dir):\n",
    "    for file in os.listdir(file_dir):\n",
    "        if 'jsonl' not in file:\n",
    "            continue\n",
    "        with open(os.path.join(file_dir,file), 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            print(first_line)\n",
    "        break\n",
    "\n",
    "def extract_lines_with_id(file_path,target_list):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for obj in reader:\n",
    "            if obj.get('id') in target_list:\n",
    "                yield obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ff257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUR_DIR = os.getcwd()\n",
    "print(CUR_DIR)\n",
    "DATA_DIR = f\"{CUR_DIR}/workspace/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d505f",
   "metadata": {},
   "source": [
    "## 1. Download\n",
    "In this example, we will download the Tiny Stories validation dataset which is < 20 MB.  The small size of this dataset makes it ideal to demonstrate data curation pipelines on a local machine.  Nemo Curator supports several default downloader [implementations](https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/download.html).  In this example, we will use a custom downloader from this [tutorial](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/tinystories) to download the dataset and convert it to JSONL format.\n",
    "\n",
    "The resultant .jsonl will contain the following keys:\n",
    "1. text\n",
    "2. file_name\n",
    "3. id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa21e6a",
   "metadata": {},
   "source": [
    "Import custom downloader libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/tutorials/tinystories/docbuilder.py\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/tutorials/tinystories/helpers.py\n",
    "    \n",
    "from docbuilder import TinyStoriesDownloader, TinyStoriesIterator, TinyStoriesExtractor\n",
    "from helpers import write_jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b650e",
   "metadata": {},
   "source": [
    "Download dataset and convert to JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "TINY_STORIES_URL = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-valid.txt\"\n",
    "JSONL_ROOT_DIR = os.path.join(DATA_DIR, \"jsonl\")\n",
    "\n",
    "downloader = TinyStoriesDownloader(DATA_DIR)\n",
    "tinystories_val_fp = downloader.download(TINY_STORIES_URL)\n",
    "\n",
    "# Convert to JSONL files.\n",
    "write_jsonl(tinystories_val_fp, JSONL_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd58e39",
   "metadata": {},
   "source": [
    "Verify output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b985f553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\": \"Spot. Spot saw the shiny car and said, \\\"Wow, Kitty, your car is so bright and clean!\\\" Kitty smiled and replied, \\\"Thank you, Spot. I polish it every day.\\\" After playing with the car, Kitty and Spot felt thirsty. They found a small pond with clear water. They drank the water and felt very happy. They played together all day and became best friends.\", \"filename\": \"TinyStories-valid.txt\", \"id\": \"TinyStories-valid.txt-0\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_jsonl_file(JSONL_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56f12a",
   "metadata": {},
   "source": [
    " Start a CPU based Dask cluster. Please modify `n_workers` and `memory_limit` according to your hardware specification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822b5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=8, processes=True, memory_limit='16GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cc8b1",
   "metadata": {},
   "source": [
    "Next we will read the JSONL files into Curator's DocumentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a03b463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 3 files\n"
     ]
    }
   ],
   "source": [
    "source_dataset = DocumentDataset.read_json(JSONL_ROOT_DIR, add_filename=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b334065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-0</td>\n",
       "      <td>Spot. Spot saw the shiny car and said, \"Wow, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-1</td>\n",
       "      <td>Once upon a time, in a big forest, there lived...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-2</td>\n",
       "      <td>Once upon a time, in a small yard, there was a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-3</td>\n",
       "      <td>Once upon a time, there was a thoughtful girl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-4</td>\n",
       "      <td>Once upon a time, there was a kind farmer. He ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename                       id  \\\n",
       "0  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-0   \n",
       "1  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-1   \n",
       "2  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-2   \n",
       "3  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-3   \n",
       "4  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-4   \n",
       "\n",
       "                                                text  \n",
       "0  Spot. Spot saw the shiny car and said, \"Wow, K...  \n",
       "1  Once upon a time, in a big forest, there lived...  \n",
       "2  Once upon a time, in a small yard, there was a...  \n",
       "3  Once upon a time, there was a thoughtful girl ...  \n",
       "4  Once upon a time, there was a kind farmer. He ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43334988",
   "metadata": {},
   "source": [
    "## 2. Unicode fixing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ccdc1f",
   "metadata": {},
   "source": [
    "In this section, we apply `UnicodeReformatter` to the data to ensure consisent and accurate representations of text data for down stream processing like deduplication\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9198e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator.modules.modify import Modify\n",
    "from nemo_curator.modifiers import UnicodeReformatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a5f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLEAN_DATA_DIR = os.path.join(DATA_DIR, \"cleaned\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "cleaner = Modify(UnicodeReformatter())\n",
    "cleaned_data = cleaner(source_dataset)\n",
    "\n",
    "# Write the cleaned_data\n",
    "cleaned_data.to_json(CLEAN_DATA_DIR, write_to_filename=True)\n",
    "\n",
    "print(f\"Time taken for fixing unicode:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cbc26",
   "metadata": {},
   "source": [
    "Verify output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d944c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_jsonl_file(CLEAN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3fc5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Additional Processing with Dask/PySpark\n",
    "\n",
    "We will compute additional info and add it to the dataset.  We will calculate: \n",
    "- Word Count: The number of words in each story\n",
    "- Character Count: The number of characters in each story  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ffb8d0",
   "metadata": {},
   "source": [
    "**We can compute this using Dask DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71f007b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>CharacterCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-0</td>\n",
       "      <td>Spot. Spot saw the shiny car and said, \"Wow, K...</td>\n",
       "      <td>64</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-1</td>\n",
       "      <td>Once upon a time, in a big forest, there lived...</td>\n",
       "      <td>235</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-2</td>\n",
       "      <td>Once upon a time, in a small yard, there was a...</td>\n",
       "      <td>105</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-3</td>\n",
       "      <td>Once upon a time, there was a thoughtful girl ...</td>\n",
       "      <td>154</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TinyStories-valid.txt-0.jsonl</td>\n",
       "      <td>TinyStories-valid.txt-4</td>\n",
       "      <td>Once upon a time, there was a kind farmer. He ...</td>\n",
       "      <td>113</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename                       id  \\\n",
       "0  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-0   \n",
       "1  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-1   \n",
       "2  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-2   \n",
       "3  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-3   \n",
       "4  TinyStories-valid.txt-0.jsonl  TinyStories-valid.txt-4   \n",
       "\n",
       "                                                text  WordCount  \\\n",
       "0  Spot. Spot saw the shiny car and said, \"Wow, K...         64   \n",
       "1  Once upon a time, in a big forest, there lived...        235   \n",
       "2  Once upon a time, in a small yard, there was a...        105   \n",
       "3  Once upon a time, there was a thoughtful girl ...        154   \n",
       "4  Once upon a time, there was a kind farmer. He ...        113   \n",
       "\n",
       "   CharacterCount  \n",
       "0             348  \n",
       "1            1198  \n",
       "2             507  \n",
       "3             785  \n",
       "4             535  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.df['WordCount'] = cleaned_data.df['text'].str.split(r'\\s+').str.len()\n",
    "cleaned_data.df['CharacterCount'] = cleaned_data.df['text'].str.len()\n",
    "cleaned_data.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ad459",
   "metadata": {},
   "source": [
    "**Altenatively, you can also compute the same using PySpark.  Dask DataFrames can be converted to Spark DataFrames using ```spark.createDataFrame(<dask df>)```. However, for large datasets it is more efficient to first write output from a Dask DataFrame into Parquet or JSONL, and read it back into a Spark DataDrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309934ff",
   "metadata": {},
   "source": [
    "Start a local Spark session. Ensure you have enough resources to perform this operation. In real-world deployments, you will likely connect to a remote Spark cluster\n",
    "\n",
    "Note: if you do not have PySpark installed, uncomment the cell below to install both java & spark \n",
    "\n",
    "You can also Spark processing using NVIDIA Rapids [Accelerator](https://docs.nvidia.com/spark-rapids/index.html).  The spark session below starts a CPU session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt update && apt install -y openjdk-11-jdk \n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662aafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9c348ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Data Curation\") \\\n",
    "    .config(\"spark.driver.memory\", '4g') \\\n",
    "    .config(\"spark.executor.memory\", '4g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45816b2",
   "metadata": {},
   "source": [
    "Read the JSONL output from Nemo Curator into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5056618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(CLEAN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6af95dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            filename|                  id|                text|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|TinyStories-valid...|TinyStories-valid...|Spot. Spot saw th...|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd92ba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+--------------+\n",
      "|            filename|                  id|                text|WordCount|CharacterCount|\n",
      "+--------------------+--------------------+--------------------+---------+--------------+\n",
      "|TinyStories-valid...|TinyStories-valid...|Spot. Spot saw th...|       64|           348|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|      235|          1198|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|      105|           507|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|      154|           785|\n",
      "|TinyStories-valid...|TinyStories-valid...|Once upon a time,...|      113|           535|\n",
      "+--------------------+--------------------+--------------------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size, split, length, expr\n",
    "# Calculate Word Count\n",
    "df = df.withColumn(\"WordCount\", size(split(df[\"text\"], r'\\s+')))\n",
    "\n",
    "# Calculate Character Count\n",
    "df = df.withColumn(\"CharacterCount\", length(df[\"text\"]))\n",
    "\n",
    "df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9aa6a",
   "metadata": {},
   "source": [
    "Write dataset to a processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "df.write.mode(\"overwrite\").parquet(PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602e822",
   "metadata": {},
   "source": [
    "Shutdown the Dask cluster, we will launch a GPU cluster in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2491f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf027e",
   "metadata": {},
   "source": [
    "## 4.Exact Deduplication\n",
    "\n",
    "We will continue processing the dataset in Nemo Curator.  In exact deduplication, the document text is hashed into unique string using certain hashing algorithm, such as 'md5'. The documents with exact hashed values are having identical text. We will output the `ID` of duplicated documents for removal later. The function used is `ExactDuplicates()`. Arguments for this function include:\n",
    "- `id_field`: Key in input file for identifying document ID\n",
    "- `text_field`: Key in input file which contains document text.\n",
    "- `hash_method`: Hashing algorithm used. Default is `md5`\n",
    "- `cache_dir`: If specified, the duplicated document IDs will be output to the `cache_dir`. Otherwise, the IDs will not be saved\n",
    "\n",
    "We will start a GPU cluster to accelerate the deduplication step.  Since GPU based Dask cluster involves setting several arguments, we will use the get_client() wrapper function to quickly set up a single node cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client(cluster_type = 'gpu', set_torch_to_use_rmm=False)\n",
    "print(f\"Number of dask worker:{get_num_workers(client)}\")\n",
    "client.run(pre_imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ba34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator.modules import ExactDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b866dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(DATA_DIR, \"logs\")\n",
    "EXACT_DEDUP_OUT_DIR = os.path.join(DATA_DIR, \"exact_dedup\")\n",
    "\n",
    "!mkdir -p {LOG_DIR}\n",
    "!mkdir -p {EXACT_DEDUP_OUT_DIR}\n",
    "\n",
    "#ignores checksum and marker files created by Spark job\n",
    "processed_files = [filename for filename in get_all_files_paths_under(PROCESSED_DIR) \n",
    "                   if not filename.endswith('.crc') or filename.endswith('_SUCCESS')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaaa765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# Read input dataset from Spark output\n",
    "input_dataset = DocumentDataset.read_parquet(processed_files, backend='cudf')\n",
    "\n",
    "#Run exact deduplication to the input\n",
    "exact_dup = ExactDuplicates(\n",
    "    logger=LOG_DIR,\n",
    "    id_field=\"id\",\n",
    "    text_field=\"text\",\n",
    "    hash_method=\"md5\",\n",
    "    cache_dir=EXACT_DEDUP_OUT_DIR #Duplicated document ID list is output to the cache_dir\n",
    ")\n",
    "duplicates = exact_dup(dataset=input_dataset)\n",
    "\n",
    "print(f\"Number of exact duplicated file:{len(duplicates)}\")\n",
    "\n",
    "print(f\"Time taken for exact duplicate:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f0399",
   "metadata": {},
   "source": [
    "**[Optional]** Verify the output duplicated ID. We can group by the `_hashes` to get the list of duplicated documents having the same _hashes and use `extract_lines_with_id()` to verify that those documents are indeed exact duplicates. Please note that the `id` might changes, therefore, please replace the `target_list` when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8bb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exact_dedup_res = pd.read_parquet(os.path.join(EXACT_DEDUP_OUT_DIR,\"_exact_duplicates.parquet\"))\n",
    "print(f\"Number of exact duplicated document:{len(exact_dedup_res)}\")\n",
    "exact_dedup_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8b173",
   "metadata": {},
   "source": [
    "Close the CPU Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12508f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaee8e",
   "metadata": {},
   "source": [
    "Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4c0c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
