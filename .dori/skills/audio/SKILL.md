---
name: audio
description: Process audio and speech data including ASR transcription, Word Error Rate calculation, and quality filtering. Use when the user wants to transcribe audio, assess speech quality, or filter audio data. GPU recommended.
license: Apache-2.0
metadata:
  author: nvidia
  version: "1.0"
  modality: audio
  gpu-required: "recommended"
---

# Audio Curation Skill

Help users process and curate audio/speech data using NeMo Curator. This skill covers ASR transcription, quality assessment via Word Error Rate (WER), and filtering.

## When This Skill Applies

- User wants to process audio or speech data
- User mentions: "audio", "speech", "transcription", "ASR", "WER", "voice"
- User is building speech datasets for training or analysis

## Important: GPU Recommendations

Audio processing resource requirements:
- ASR transcription (NeMo ASR models) - GPU recommended, CPU supported but slower
- WER calculation - CPU only (no GPU required)
- Audio embedding (if used) - GPU recommended

## Skill Workflow

### Step 1: Understand the Goal

Ask the user:
1. What's your end goal? (ASR training, TTS training, speech analysis)
2. What's your audio format? (WAV, MP3, FLAC)
3. Do you have reference transcripts? (for WER-based filtering)
4. What language is the audio?

### Step 2: Recommend Pipeline Stages

| Goal | Recommended Stages |
|------|-------------------|
| Create transcripts | Read → ASR → Write |
| Filter by quality | Read → ASR → WER → Filter → Write |
| Speech dataset curation | Read → ASR → WER Filter → Duration Filter → Write |

### Step 3: Explain Available Stages

**ASR Transcription (GPU Recommended)**
- `InferenceAsrNemoStage`: NeMo ASR models
  - Models: Parakeet, Canary, FastConformer
  - Languages: English, multilingual options
  - Output: transcript text via `pred_text_key` field

**WER Calculation (CPU)**
- `GetPairwiseWerStage`: Compare ASR output to reference
  - Requires reference transcripts
  - Output: WER score as percentage (e.g., 15.0 = 15%)

**Filtering**
- `PreserveByValueStage`: Filter based on any field
  - Filter by WER (e.g., keep WER < 15.0 for 15%)
  - Filter by duration (e.g., 1-30 seconds)
  - Requires explicit `operator` parameter ("lt", "le", "eq", etc.)

### Step 4: Generate Pipeline Code

```python
# Generated by NeMo Curator Agent
# Audio Processing Pipeline
# GPU Recommended for ASR

import torch
if not torch.cuda.is_available():
    print("Warning: GPU not available. ASR will run on CPU (slower).")

# Audio stages
from nemo_curator.stages.audio.inference.asr_nemo import InferenceAsrNemoStage
from nemo_curator.stages.audio.metrics.get_wer import GetPairwiseWerStage
from nemo_curator.stages.audio.common import PreserveByValueStage

from nemo_curator.pipeline import Pipeline

# Configuration
ASR_CONFIG = {
    "model_name": "nvidia/parakeet-tdt-0.6b-v2",  # Verified English ASR model
    "batch_size": 32,
}

WER_CONFIG = {
    "max_wer": 15.0,  # Keep utterances with WER < 15% (percentage, not decimal)
}

DURATION_CONFIG = {
    "min_duration": 1.0,   # Minimum 1 second
    "max_duration": 30.0,  # Maximum 30 seconds
}


def process_audio(input_manifest: str, output_manifest: str, reference_field: str = "text"):
    """Process audio: transcribe with ASR, calculate WER, filter."""
    
    print(f"Input manifest: {input_manifest}")
    print(f"Output manifest: {output_manifest}")
    
    # Stage 1: ASR Transcription
    print("\n[1/3] Running ASR transcription...")
    asr_stage = InferenceAsrNemoStage(
        model_name=ASR_CONFIG["model_name"],
        batch_size=ASR_CONFIG["batch_size"],
        pred_text_key="asr_transcript",  # Output field for ASR predictions
    )
    print(f"  Model: {ASR_CONFIG['model_name']}")
    
    # Stage 2: Calculate WER
    print("[2/3] Calculating Word Error Rate...")
    wer_stage = GetPairwiseWerStage(
        text_key=reference_field,             # Ground truth field
        pred_text_key="asr_transcript",       # ASR output field
        wer_key="wer",                        # Output field for WER scores
    )
    
    # Stage 3: Filter by WER (keep low WER = high quality)
    print("[3/3] Filtering by WER threshold...")
    filter_stage = PreserveByValueStage(
        input_value_key="wer",
        target_value=WER_CONFIG["max_wer"],
        operator="lt",  # Less than - keep if WER < threshold
    )
    print(f"  Keep if WER < {WER_CONFIG['max_wer']}%")
    
    # Build and run pipeline
    pipeline = Pipeline(
        name="audio_curation",
        stages=[asr_stage, wer_stage, filter_stage],
    )
    
    results = pipeline.run()
    
    print(f"\nProcessed audio files")
    print(f"Output: {output_manifest}")
    
    return results


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="Input NeMo manifest (JSON lines)")
    parser.add_argument("--output", required=True, help="Output manifest path")
    parser.add_argument("--reference-field", default="text", help="Field with reference transcript")
    args = parser.parse_args()
    process_audio(args.input, args.output, args.reference_field)
```

### Step 5: Input Format

NeMo audio manifests are JSON lines with:
```json
{"audio_filepath": "/path/to/audio.wav", "duration": 5.2, "text": "reference transcript"}
{"audio_filepath": "/path/to/audio2.wav", "duration": 3.1, "text": "another reference"}
```

Create manifest from audio files:
```python
import os
import json
import librosa

def create_manifest(audio_dir: str, output_path: str):
    """Create NeMo manifest from audio directory."""
    with open(output_path, "w") as f:
        for filename in os.listdir(audio_dir):
            if filename.endswith((".wav", ".mp3", ".flac")):
                filepath = os.path.join(audio_dir, filename)
                duration = librosa.get_duration(path=filepath)
                entry = {
                    "audio_filepath": filepath,
                    "duration": duration,
                    "text": "",  # Fill in if you have transcripts
                }
                f.write(json.dumps(entry) + "\n")
```

### Step 6: Execution Instructions

**Docker with GPU:**
```bash
docker run --gpus all --rm \
    -v $(pwd):/data \
    nvcr.io/nvidia/nemo-curator:latest \
    python /data/audio_pipeline.py \
    --input /data/manifest.json \
    --output /data/filtered_manifest.json
```

**Resource requirements:**
- Parakeet 0.6B: ~4GB GPU memory
- Larger models (FastConformer Hybrid): ~8-16GB GPU memory
- Note: ASR can run on CPU but will be slower

---

## Available Stages (Reference)

### ASR Stages
| Stage | Location | Key Parameters |
|-------|----------|----------------|
| `InferenceAsrNemoStage` | `stages/audio/inference/asr_nemo.py` | `model_name`, `pred_text_key`, `filepath_key`, `batch_size` |

### Metrics Stages
| Stage | Location | Key Parameters |
|-------|----------|----------------|
| `GetPairwiseWerStage` | `stages/audio/metrics/get_wer.py` | `text_key`, `pred_text_key`, `wer_key` |

### Filter Stages
| Stage | Location | Key Parameters |
|-------|----------|----------------|
| `PreserveByValueStage` | `stages/audio/common.py` | `input_value_key`, `target_value`, `operator` |

### ASR Models (Verified)
| Model | Languages | Notes |
|-------|-----------|-------|
| `nvidia/parakeet-tdt-0.6b-v2` | English | Test-verified, recommended |

For additional models, see [NeMo ASR Model Catalog](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/all_chkpt.html).

---

## Example Interactions

### Example 1: Create transcripts
```
User: I have audio files and want to create transcripts

Agent: I can set up ASR transcription. Questions:

1. What language is the audio?
2. How many hours of audio?
3. What audio format?
4. What GPU do you have?

User: English, about 100 hours, WAV files, RTX 3090

Agent: For 100 hours of English audio:
- Model: Parakeet TDT 0.6B (verified, good accuracy)
- Batch size: 32 (fits in 24GB)
- Estimated time: ~2-3 hours

First, I'll help you create a NeMo manifest, then run transcription.

Do you have any reference transcripts for quality checking?
```

### Example 2: Quality filtering
```
User: I have audio with transcripts but quality varies

Agent: WER-based filtering can identify low-quality pairs:

1. Run ASR on your audio
2. Compare ASR output to your transcripts
3. Filter out high-WER pairs (likely bad audio or transcripts)

What WER threshold would you like?
- 5.0 (5%): Very strict, only near-perfect
- 10.0 (10%): Strict, professional quality
- 15.0 (15%): Standard, good quality
- 20.0 (20%): Permissive, acceptable quality

Note: WER values are percentages (15.0 = 15%), not decimals.

User: Let's try 10%

Agent: Good choice for high-quality data. Here's your pipeline: [generates code]
```

### Example 3: Duration filtering
```
User: I want to remove very short and very long utterances

Agent: Duration filtering is common for speech training:

- Too short (< 1s): Often noise or fragments
- Too long (> 30s): May have alignment issues

Recommended: 1-30 seconds for most speech tasks, 
or 2-15 seconds for TTS training.

What duration range works for your use case?
```
