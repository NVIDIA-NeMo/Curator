---
name: image
description: Process image datasets including CLIP embedding, aesthetic filtering, NSFW detection, and deduplication. Use when the user wants to filter images, generate embeddings, or remove inappropriate content. GPU required.
license: Apache-2.0
metadata:
  author: nvidia
  version: "1.1"
  modality: image
  gpu-required: "true"
---

# Image Curation Skill

Help users process and curate image datasets using NeMo Curator. This skill covers embedding, filtering (aesthetic, NSFW), and deduplication of image data.

## When This Skill Applies

- User wants to process image data
- User mentions: "image", "photos", "pictures", "aesthetic", "NSFW filter", "image embeddings"
- User is building image datasets for training or retrieval

## Important: GPU Required

All image processing stages require GPU:
- CLIP embedding
- Aesthetic scoring
- NSFW detection
- Semantic deduplication

## Skill Workflow

### Step 1: Understand the Goal

Ask the user:
1. What's your end goal? (training data, retrieval, filtering)
2. What format is your data? (image files, URLs, parquet with paths)
3. What processing do you need?
   - Embedding (CLIP vectors for search/similarity)
   - Aesthetic filtering (remove low-quality images)
   - NSFW filtering (remove inappropriate content)
   - Deduplication (remove duplicates/near-duplicates)

### Step 2: Recommend Pipeline Stages

| Goal | Recommended Stages |
|------|-------------------|
| Training dataset | Read → Embed → NSFW Filter → Aesthetic Filter → Dedup |
| Image search | Read → CLIP Embed → Store vectors |
| Quality filtering | Read → Embed → Aesthetic Filter → Write |
| Content moderation | Read → Embed → NSFW Filter → Write |

**Note**: Embedding must come first — filter stages require pre-computed CLIP embeddings.

### Step 3: Explain Available Stages

**Embedding (GPU Required)**
- `ImageEmbeddingStage`: CLIP-based embeddings
  - Model: `openai/clip-vit-large-patch14` (ViT-L/14)
  - Output: 768-dim normalized vectors
  - Use for: similarity search, deduplication, downstream filtering

**Aesthetic Filtering (GPU Required)**
- `ImageAestheticFilterStage`: Score image quality
  - Model: `ttj/sac-logos-ava1-l14-linearMSE` (LAION aesthetic predictor)
  - Score range: 0.0-1.0 (higher = more aesthetic)
  - Default threshold: 0.5 (keep images with score >= threshold)

**NSFW Filtering (GPU Required)**
- `ImageNSFWFilterStage`: Detect inappropriate content
  - Score range: 0.0-1.0 (higher = more likely NSFW)
  - Default threshold: 0.5 (keep images with score < threshold)

**Deduplication**
- Semantic dedup using CLIP embeddings
- Cosine similarity threshold (e.g., 0.95)

### Step 4: Generate Pipeline Code

```python
# Generated by NeMo Curator Agent
# Image Processing Pipeline
# GPU Required: Yes

import torch
if not torch.cuda.is_available():
    raise RuntimeError("GPU required for image processing")

print(f"GPU: {torch.cuda.get_device_name(0)}")

# Image stages
from nemo_curator.stages.image.embedders.clip_embedder import ImageEmbeddingStage
from nemo_curator.stages.image.filters.aesthetic_filter import ImageAestheticFilterStage
from nemo_curator.stages.image.filters.nsfw_filter import ImageNSFWFilterStage

from nemo_curator.pipeline import Pipeline

# Configuration
MODEL_DIR = "/path/to/models"  # Directory for model weights

EMBED_CONFIG = {
    "model_dir": MODEL_DIR,
    "model_inference_batch_size": 32,
    "remove_image_data": False,      # Keep image data for downstream stages
}

AESTHETIC_CONFIG = {
    "model_dir": MODEL_DIR,
    "score_threshold": 0.5,          # Keep images scoring >= 0.5
}

NSFW_CONFIG = {
    "model_dir": MODEL_DIR,
    "score_threshold": 0.5,          # Keep images scoring < 0.5
}


def process_images(input_path: str, output_path: str):
    """Process images: embed, filter NSFW, filter aesthetic."""
    
    print(f"Input: {input_path}")
    print(f"Output: {output_path}")
    
    # Stage 1: Generate CLIP embeddings (required for downstream stages)
    print("\n[1/3] Generating CLIP embeddings...")
    embed_stage = ImageEmbeddingStage(
        model_dir=EMBED_CONFIG["model_dir"],
        model_inference_batch_size=EMBED_CONFIG["model_inference_batch_size"],
        remove_image_data=EMBED_CONFIG["remove_image_data"],
    )
    print("  Model: openai/clip-vit-large-patch14 (768-dim)")
    
    # Stage 2: Filter NSFW content (keep score < threshold)
    print("[2/3] Filtering NSFW content...")
    nsfw_stage = ImageNSFWFilterStage(
        model_dir=NSFW_CONFIG["model_dir"],
        score_threshold=NSFW_CONFIG["score_threshold"],
    )
    print(f"  Keep if NSFW score < {NSFW_CONFIG['score_threshold']}")
    
    # Stage 3: Filter by aesthetic quality (keep score >= threshold)
    print("[3/3] Filtering by aesthetic quality...")
    aesthetic_stage = ImageAestheticFilterStage(
        model_dir=AESTHETIC_CONFIG["model_dir"],
        score_threshold=AESTHETIC_CONFIG["score_threshold"],
    )
    print(f"  Keep if aesthetic score >= {AESTHETIC_CONFIG['score_threshold']}")
    
    # Build and run pipeline
    pipeline = Pipeline(
        name="image_curation",
        stages=[embed_stage, nsfw_stage, aesthetic_stage],
    )
    
    results = pipeline.run()
    
    print(f"\nProcessed {len(results)} images")
    print(f"Output: {output_path}")
    
    return results


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="Input image directory or parquet")
    parser.add_argument("--output", required=True, help="Output path")
    parser.add_argument("--model-dir", default="/models", help="Model weights directory")
    args = parser.parse_args()
    process_images(args.input, args.output)
```

### Step 5: Execution Instructions

**Docker with GPU:**
```bash
docker run --gpus all --rm \
    -v $(pwd)/images:/input \
    -v $(pwd)/output:/output \
    -v $(pwd)/models:/models \
    nvcr.io/nvidia/nemo-curator:latest \
    python /data/image_pipeline.py --input /input --output /output --model-dir /models
```

**Resource requirements:**
- CLIP ViT-L/14: ~4GB GPU memory
- Aesthetic model: ~2GB GPU memory  
- NSFW model: ~2GB GPU memory
- Total: ~8GB for full pipeline

### Step 6: Tune Thresholds

Help user adjust based on results:

```
User: The aesthetic filter is removing too many images

Agent: Let's check the score distribution. What's the current threshold?

User: 0.5

Agent: Try lowering to 0.3 or 0.4. The aesthetic model outputs scores 0.0-1.0:
- 0.0-0.3: Low quality
- 0.3-0.5: Acceptable
- 0.5-0.7: Good
- 0.7-1.0: Excellent

For training data, 0.4 is often a good balance for keeping more images.
```

---

## Available Stages (Reference)

### Embedding Stage

| Stage | Location | Purpose |
|-------|----------|---------|
| `ImageEmbeddingStage` | `stages/image/embedders/clip_embedder.py` | CLIP embeddings |

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model_dir` | str | None | Directory for model weights |
| `model_inference_batch_size` | int | 32 | Images per batch |
| `num_gpus_per_worker` | float | 0.25 | GPU fraction per worker |
| `remove_image_data` | bool | False | Clear image data after embedding |
| `verbose` | bool | False | Enable detailed logging |

### Filter Stages

| Stage | Location | Purpose |
|-------|----------|---------|
| `ImageAestheticFilterStage` | `stages/image/filters/aesthetic_filter.py` | Aesthetic quality filtering |
| `ImageNSFWFilterStage` | `stages/image/filters/nsfw_filter.py` | NSFW content filtering |

**Shared Parameters (inherited from BaseFilterStage):**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model_dir` | str | None | Directory for model weights |
| `score_threshold` | float | 0.5 | Filtering threshold |
| `model_inference_batch_size` | int | 32 | Images per batch |
| `num_gpus_per_worker` | float | 0.25 | GPU fraction per worker |
| `verbose` | bool | False | Enable detailed logging |

### CLIP Model

Uses `openai/clip-vit-large-patch14` (ViT-L/14):
- **Embedding dimensions**: 768
- **Input resolution**: 224x224 (auto-resized)
- **Normalization**: L2-normalized (unit vectors)

---

## Example Interactions

### Example 1: Training dataset
```
User: I want to filter my image dataset for AI training

Agent: I can help set up image filtering. Questions:

1. How many images do you have?
2. What quality level do you need? (strict vs permissive)
3. Do you need NSFW filtering?
4. What GPU do you have?

User: 1 million images, high quality needed, yes NSFW filter, A100 40GB

Agent: For 1M high-quality images:
- Aesthetic threshold: 0.6-0.7 (strict quality)
- NSFW threshold: 0.3 (strict safety)
- Process in batches of 64 (fits in A100)

Estimated: ~30-50% of images will pass both filters.

Here's your pipeline: [generates code]
```

### Example 2: Image search
```
User: I want to build image search with CLIP

Agent: For image search, you need CLIP embeddings stored in a vector DB:

1. Generate embeddings (ImageEmbeddingStage)
2. Store in vector database (Milvus, Pinecone, FAISS)
3. Query with text or image embeddings

The embeddings are 768-dimensional normalized vectors from ViT-L/14.

Which vector database are you using?
```

### Example 3: Deduplication
```
User: I have duplicate images I want to remove

Agent: For image deduplication:

1. Generate CLIP embeddings
2. Compute pairwise cosine similarity
3. Remove images with similarity > threshold (e.g., 0.95)

This catches both exact duplicates and near-duplicates (crops, resizes, etc.)

How strict should dedup be? Options:
- 0.99: Near-exact duplicates only
- 0.95: Include minor variations
- 0.90: Aggressive (may remove similar but different images)
```
