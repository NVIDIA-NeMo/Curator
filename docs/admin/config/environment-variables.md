(admin-config-environment-variables)=

# Environment Variables Reference

This reference covers environment variables that affect NeMo Curator behavior. Most configuration in NeMo Curator is done through code parameters rather than environment variables. This document focuses on the external library variables (Ray, RAPIDS, CUDA) that NeMo Curator's dependencies use, plus the variables that NeMo Curator directly processes.

```{important}
**Ray-Based Architecture**: NeMo Curator uses Ray as its distributed computing backend. Previous versions used Dask, but the current architecture is built on Ray for better scalability and multi-modal support.
```

```{note}
**Variable Types**: This reference includes:
- **Ray Integration Variables**: Variables that NeMo Curator's RayClient checks
- **External Library Variables**: Used by dependencies (RAPIDS, UCX, CUDA) that affect NeMo Curator behavior
- **Standard Service Variables**: Cloud storage and API authentication (not NeMo Curator-specific)
```

```{tip}
**Applying Environment Variables**: These variables are used throughout NeMo Curator deployments:
- {doc}`Deployment Environment Configuration <deployment-environments>`: Environment-specific variable patterns for Ray clusters
- **Container Deployments**: Setting variables in container environments
- {doc}`Slurm Deployment <../deployment/slurm/index>`: Using variables in Slurm job scripts

**Deployment-Generated Variables**: Some variables (like `LOGDIR`, `PROFILESDIR`, `SCHEDULER_FILE`) are automatically generated by deployment systems (Slurm) and are not NeMo Curator-specific.
```

---

## Ray Integration Variables

NeMo Curator integrates with Ray through its `RayClient` class. NeMo Curator configures Ray through `RayClient` parameters rather than environment variables. One Ray-related environment variable that NeMo Curator directly checks:

```{list-table} Ray Integration Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `RAY_ADDRESS`
  - none
  - Address of existing Ray cluster (checked by NeMo Curator's RayClient)
```

**Example Usage:**

```bash
# Connect to existing Ray cluster (only variable that works with NeMo Curator)
export RAY_ADDRESS="ray://head-node:10001"
```

```python
# Configure Ray resources through RayClient parameters (recommended approach)
from nemo_curator.core.client import RayClient

ray_client = RayClient(
    num_cpus=8,
    num_gpus=2,
    enable_object_spilling=True
)
ray_client.start()
```

---

## External Library Variables

NeMo Curator depends on RAPIDS, UCX, and CUDA libraries that use environment variables to control their behavior. These variables affect NeMo Curator indirectly through its dependencies.

### RAPIDS and GPU Configuration

### Memory Management (RMM)

```{list-table} RMM Memory Pool Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `RMM_WORKER_POOL_SIZE`
  - "72GiB"
  - GPU memory pool size per worker
* - `RMM_SCHEDULER_POOL_SIZE`
  - "1GB"
  - GPU memory pool size for scheduler
* - `RMM_ALLOCATOR`
  - "pool"
  - Memory allocator: "pool", "arena", "binning"
* - `RMM_POOL_INIT_SIZE`
  - "256MB"
  - Initial pool size
* - `RMM_MAXIMUM_POOL_SIZE`
  - auto-detect
  - Maximum pool size
```

**Memory Sizing Guidelines:**

```bash
# For 80GB GPU (A100/H100)
export RMM_WORKER_POOL_SIZE="72GiB"  # 90% of GPU memory

# For 40GB GPU (A100)
export RMM_WORKER_POOL_SIZE="36GiB"  # 90% of GPU memory

# For 16GB GPU (V100)
export RMM_WORKER_POOL_SIZE="14GiB"  # 87.5% of GPU memory

# Percentage-based allocation (alternative)
export RMM_WORKER_POOL_SIZE="0.9"  # 90% of available memory
```

### RAPIDS Initialization

```{list-table} RAPIDS Initialization Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `RAPIDS_NO_INITIALIZE`
  - "1" (hardcoded)
  - Delay CUDA context creation (hardcoded to "1" in NeMo Curator)
* - `CUDF_SPILL`
  - "1"
  - Enable automatic GPU memory spilling: "0" or "1"
* - `CUDF_SPILL_DEVICE_LIMIT`
  - "0.8"
  - Spill threshold (fraction of GPU memory)
* - `LIBCUDF_CUFILE_POLICY`
  - "OFF"
  - GPUDirect Storage policy: "OFF", "ON", "GDS"
```

**Configuration Examples:**

```bash
# High-performance setup (sufficient GPU memory)
# Note: RAPIDS_NO_INITIALIZE is hardcoded to "1" in NeMo Curator
export CUDF_SPILL="0"            # Disable spilling
export LIBCUDF_CUFILE_POLICY="ON"  # Enable direct storage access

# Memory-constrained setup
export CUDF_SPILL="1"            # Enable spilling
export CUDF_SPILL_DEVICE_LIMIT="0.7"  # Spill at 70% capacity
```

```{note}
**RAPIDS_NO_INITIALIZE**: This variable is hardcoded to "1" in NeMo Curator source code and cannot be overridden via environment variables. See `nemo_curator/__init__.py` and classifier modules.
```

---

### Ray Framework Variables

These are standard Ray environment variables that affect Ray's behavior when NeMo Curator connects to existing Ray clusters. When using NeMo Curator's `RayClient` to create clusters, these are typically ignored in favor of `RayClient` parameters.

For complete Ray configuration details, refer to the [Ray Configuration Documentation](https://docs.ray.io/en/latest/ray-core/configure.html).

```{list-table} Common Ray Framework Variables
:header-rows: 1
:widths: 40 20 40

* - Variable
  - Default
  - Description
* - `RAY_DEDUP_LOGS`
  - "1"
  - Deduplicate log messages: "0" or "1"
* - `RAY_USAGE_STATS_ENABLED`
  - "1"
  - Enable usage statistics: "0" or "1"
* - `RAY_TMPDIR`
  - "/tmp/ray"
  - Temporary directory for Ray files
```

---

### Network and Communication

These variables configure network communication libraries used by Ray and NeMo Curator.

#### UCX Configuration

UCX (Unified Communication X) variables for high-performance networking. Refer to the [UCX Documentation](https://openucx.readthedocs.io/) for complete details.

```{list-table} UCX Communication Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `UCX_TLS`
  - auto-detect
  - Transport layers: "rc,cuda_copy,cuda_ipc"
* - `UCX_NET_DEVICES`
  - auto-detect
  - Network devices to use
* - `UCX_MEMTYPE_CACHE`
  - "y"
  - Enable memory type cache
* - `UCX_RNDV_SCHEME`
  - "put_zcopy"
  - Rendezvous protocol scheme
* - `UCX_IB_GPU_DIRECT_RDMA`
  - "yes"
  - Enable GPU Direct RDMA
```

**InfiniBand Optimization:**

```bash
# Optimized UCX for InfiniBand + GPU
export UCX_TLS="rc,cuda_copy,cuda_ipc"
export UCX_NET_DEVICES="mlx5_0:1"  # Specific InfiniBand device
export UCX_IB_GPU_DIRECT_RDMA="yes"
export UCX_MEMTYPE_CACHE="y"
```

#### TCP Configuration

```{list-table} TCP Network Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `DASK_UCX__CUDA_COPY`
  - "True"
  - Enable CUDA memory copy
* - `DASK_UCX__TCP`
  - "True"
  - Enable TCP transport
* - `DASK_UCX__NVLINK`
  - "True"
  - Enable NVLink transport
* - `DASK_UCX__INFINIBAND`
  - "True"
  - Enable InfiniBand transport
* - `DASK_UCX__RDMACM`
  - "True"
  - Enable RDMA CM
```

---

## Standard Service Variables

These are standard environment variables for cloud storage and API services.

### Storage and I/O

#### Cloud Storage Optimization

##### AWS S3 Variables

```{list-table} AWS S3 Configuration Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `AWS_ACCESS_KEY_ID`
  - none
  - AWS access key identifier
* - `AWS_SECRET_ACCESS_KEY`
  - none
  - AWS secret access key
* - `AWS_DEFAULT_REGION`
  - none
  - Default AWS region
* - `AWS_PROFILE`
  - "default"
  - AWS profile to use
* - `AWS_MAX_ATTEMPTS`
  - "5"
  - Maximum retry attempts
* - `AWS_RETRY_MODE`
  - "legacy"
  - Retry mode: "legacy", "standard", "adaptive"
* - `AWS_S3_USE_ACCELERATE_ENDPOINT`
  - "false"
  - Use S3 Transfer Acceleration
* - `AWS_S3_ADDRESSING_STYLE`
  - "auto"
  - S3 addressing style: "auto", "virtual", "path"
```

##### Azure Storage Variables

```{list-table} Azure Storage Configuration Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `AZURE_STORAGE_CONNECTION_STRING`
  - none
  - Azure storage connection string
* - `AZURE_STORAGE_ACCOUNT_NAME`
  - none
  - Azure storage account name
* - `AZURE_STORAGE_ACCOUNT_KEY`
  - none
  - Azure storage account key
* - `AZURE_STORAGE_SAS_TOKEN`
  - none
  - Azure SAS token
```

#### Local I/O Optimization

```{list-table} Local I/O Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `OMP_NUM_THREADS`
  - CPU count
  - OpenMP thread count
* - `MKL_NUM_THREADS`
  - CPU count
  - Intel MKL thread count
* - `NUMBA_NUM_THREADS`
  - CPU count
  - Numba thread count
* - `TMPDIR`
  - "/tmp"
  - Temporary directory
* - `PYTHONPATH`
  - system default
  - Python module search path
```

**Thread Optimization:**
```bash
# Prevent oversubscription in distributed environments
export OMP_NUM_THREADS="1"
export MKL_NUM_THREADS="1"
export NUMBA_NUM_THREADS="1"

# Use fast storage for temporary files
export TMPDIR="/fast/ssd/tmp"
```

### API and Service Configuration

#### Machine Learning Services

```{list-table} ML Service API Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `HUGGINGFACE_HUB_TOKEN`
  - none
  - HuggingFace Hub API token
* - `HF_HOME`
  - "~/.cache/huggingface"
  - HuggingFace cache directory
* - `OPENAI_API_KEY`
  - none
  - OpenAI API key
* - `OPENAI_ORG`
  - none
  - OpenAI organization ID
* - `NVIDIA_API_KEY`
  - none
  - NVIDIA AI foundation models API key
* - `NVIDIA_BASE_URL`
  - "https://integrate.api.nvidia.com/v1"
  - NVIDIA API base URL
```

#### Model Caching

```{list-table} Model Cache Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `TRANSFORMERS_CACHE`
  - "~/.cache/huggingface/transformers"
  - Transformers model cache
* - `HF_DATASETS_CACHE`
  - "~/.cache/huggingface/datasets"
  - HuggingFace datasets cache
* - `TORCH_HOME`
  - "~/.cache/torch"
  - PyTorch model cache
* - `SENTENCE_TRANSFORMERS_HOME`
  - "~/.cache/torch/sentence_transformers"
  - Sentence transformers cache
```

---

## CUDA and GPU Runtime Variables

These variables configure CUDA and GPU runtime behavior. Refer to the [CUDA Environment Variables Documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars) for complete details.

### CUDA Configuration

```{list-table} CUDA Runtime Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `CUDA_VISIBLE_DEVICES`
  - all
  - Visible GPU devices (e.g., "0,1,2,3")
* - `CUDA_LAUNCH_BLOCKING`
  - "0"
  - Synchronous CUDA launches: "0" or "1"
* - `CUDA_CACHE_PATH`
  - auto
  - CUDA kernel cache path
* - `CUDA_FORCE_PTX_JIT`
  - "0"
  - Force PTX JIT compilation
* - `CUDA_MODULE_LOADING`
  - "LAZY"
  - Module loading strategy: "LAZY" or "EAGER"
```

### GPU Memory Management

```{list-table} GPU Memory Variables
:header-rows: 1
:widths: 30 20 50

* - Variable
  - Default
  - Description
* - `PYTORCH_CUDA_ALLOC_CONF`
  - none
  - PyTorch CUDA allocator configuration
* - `CUDA_MPS_PIPE_DIRECTORY`
  - none
  - Multi-Process Service pipe directory
* - `CUDA_MPS_LOG_DIRECTORY`
  - none
  - Multi-Process Service log directory
```

**Memory Optimization Examples:**
```bash
# PyTorch memory optimization
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"

# Enable CUDA MPS for multi-process GPU sharing
export CUDA_MPS_PIPE_DIRECTORY="/tmp/nvidia-mps"
export CUDA_MPS_LOG_DIRECTORY="/tmp/nvidia-log"
```


---

## Environment Variable Management

### Loading Environment Variables

#### From File

```bash
# Load from environment file
set -a  # Automatically export variables
source /path/to/nemo-curator.env
set +a  # Stop auto-export

# Or use explicit loading
export $(cat /path/to/nemo-curator.env | xargs)
```

#### systemd Service

```ini
# /etc/systemd/system/nemo-curator.service
[Unit]
Description=NeMo Curator Service
After=network.target

[Service]
Type=exec
User=curator
Group=curator
EnvironmentFile=/etc/nemo-curator/environment
ExecStart=/usr/local/bin/nemo-curator-script
Restart=on-failure

[Install]
WantedBy=multi-user.target
```

#### Docker Environment

```dockerfile
# Dockerfile
FROM nvcr.io/nvidia/nemo-curator:latest

# Set external library environment variables
ENV RMM_WORKER_POOL_SIZE=72GiB
ENV CUDF_SPILL=0

# Ray cluster connection (if connecting to existing cluster)
ENV RAY_ADDRESS=ray://ray-head:10001

# Or load from file
COPY nemo-curator.env /etc/environment
RUN set -a && source /etc/environment && set +a
```

### Validation Script

```{note}
**Creating the Validation Script**: Save this script as `validate_env.py` in your project directory to validate your environment configuration.
```

```python
#!/usr/bin/env python3
"""Validate NeMo Curator environment variables."""

import os
import sys

def validate_environment():
    """Validate environment variable configuration."""
    
    required_vars = {
        "RAY_ADDRESS": str,  # Optional but recommended for cluster connections
    }
    
    recommended_vars = {
        "RMM_WORKER_POOL_SIZE": str,
        "CUDF_SPILL": ["0", "1"],
    }
    
    issues = []
    
    # Check required variables
    for var, valid_values in required_vars.items():
        value = os.getenv(var)
        if not value:
            issues.append(f"Missing required variable: {var}")
        elif valid_values and value not in valid_values:
            issues.append(f"Invalid value for {var}: {value} (valid: {valid_values})")
    
    # Check recommended variables
    for var, expected_type in recommended_vars.items():
        value = os.getenv(var)
        if not value:
            print(f"⚠ Recommended variable not set: {var}")
        elif expected_type == str:
            print(f"✓ {var} = {value}")
        elif isinstance(expected_type, list) and value not in expected_type:
            issues.append(f"Invalid value for {var}: {value} (valid: {expected_type})")
    
    # GPU-specific validation (check if RAPIDS libraries are available)
    try:
        import cudf
        gpu_vars = ["RMM_WORKER_POOL_SIZE", "CUDF_SPILL"]
        for var in gpu_vars:
            if not os.getenv(var):
                print(f"⚠ GPU environment detected but {var} not set")
    except ImportError:
        pass  # CPU-only environment
    
    # Report results
    if issues:
        print("❌ Environment validation failed:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print("✅ Environment validation passed")
        return True

if __name__ == "__main__":
    success = validate_environment()
    sys.exit(0 if success else 1)
```
