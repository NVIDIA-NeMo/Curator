---
description: "Integrate NeMo Curator with Apache Spark for distributed processing with enhanced performance and scalability"
categories: ["how-to-guides"]
tags: ["spark", "integration", "distributed-processing", "performance", "scalability", "apache-spark"]
personas: ["admin-focused", "devops-focused", "data-scientist-focused"]
difficulty: "advanced"
content_type: "how-to"
modality: "universal"
---

(admin-integrations-spark)=

# Reading and Writing Datasets with NeMo Curator and Apache Spark

## Background

NeMo Curator uses a pipeline-based architecture with `DocumentBatch` tasks to read and write JSONL and Parquet files. The pipeline system processes data through stages that transform tasks containing PyArrow tables or Pandas DataFrames. Apache Spark can read and write JSONL and Parquet files generated by NeMo Curator, and similarly, NeMo Curator can work with the outputs generated by Spark.

## Usage

To show how this works, consider the following example:

```python
import pandas as pd
from nemo_curator.stages.text.io.writer.jsonl import JsonlWriter
from nemo_curator.tasks import DocumentBatch

# Create sample data
data = {
    "id": [1, 2, 3],
    "text": [
        "This is a tiny story.",
        "Another tiny story appears here.",
        "Yet another tiny story for you."
    ]
}

# Convert to a pandas DataFrame and create DocumentBatch
df = pd.DataFrame(data)
document_batch = DocumentBatch(data=df, task_id="sample_data", dataset_name="tiny_stories")

# Write the dataset to JSONL files
output_dir = "tiny_stories"
jsonl_writer = JsonlWriter(path=output_dir)
jsonl_writer.write_data(document_batch, f"{output_dir}/stories.jsonl")
```

This will create a JSONL file in the directory `tiny_stories/`:

```text
tiny_stories/
    stories.jsonl
```

Apache Spark can read these files using standard application programming interfaces. First create a Spark session called `NeMoCuratorExample`, then read the file using:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("NeMoCuratorExample").getOrCreate()

# Reading JSONL file
stories_df = spark.read.json("tiny_stories/stories.jsonl")
stories_df.show()
```

Add a couple of columns to the Spark DataFrame:

```python
from pyspark.sql.functions import size, split, length

# Calculate Word Count
stories_df = stories_df.withColumn("WordCount", size(split(stories_df["text"], r"\s+")))

# Calculate Character Count
stories_df = stories_df.withColumn("CharacterCount", length(stories_df["text"]))

stories_df.write.mode("overwrite").parquet("tiny_stories_transformed")
```

To connect between NeMo Curator pipelines and Spark DataFrames, we recommend using Parquet files for data exchange. The following code snippet demonstrates how to read output from a Spark DataFrame into a NeMo Curator pipeline:

```python
from nemo_curator.stages.text.io.reader.parquet import ParquetReader
from nemo_curator.utils.file_utils import get_all_files_paths_under
from nemo_curator.pipeline import Pipeline

# Ignores checksum and marker files created by Spark
processed_files = [
     filename for filename in get_all_files_paths_under("tiny_stories_transformed")
     if not (filename.endswith(".crc") or filename.endswith("_SUCCESS"))
]

# Create pipeline to read Spark-generated Parquet files
pipeline = Pipeline(name="read_spark_output")
reader = ParquetReader(file_paths=processed_files)
pipeline.add_stage(reader)
results = pipeline.run()
```

It's worth noting that Spark typically tends to create checksum and other marker files which can vary by Spark distribution, so it's advisable to ignore them when reading data into a NeMo Curator pipeline.
