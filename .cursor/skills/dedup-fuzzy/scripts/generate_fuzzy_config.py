#!/usr/bin/env python3
# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Generate NeMo Curator fuzzy deduplication YAML configuration.

This script generates a Hydra-compatible YAML configuration file for
running FuzzyDeduplicationWorkflow.

Examples:
    # Basic usage
    python generate_fuzzy_config.py \\
        --input-path /data/text \\
        --output-path /data/deduped \\
        --cache-path /data/cache

    # With custom parameters for large dataset
    python generate_fuzzy_config.py \\
        --input-path /data/text \\
        --output-path /data/deduped \\
        --cache-path /data/cache \\
        --char-ngrams 30 \\
        --bands-per-iteration 2 \\
        --output-file fuzzy_dedup.yaml

    # For very large datasets (>1B documents)
    python generate_fuzzy_config.py \\
        --input-path /data/text \\
        --output-path /data/deduped \\
        --cache-path /data/cache \\
        --use-64-bit-hash \\
        --bands-per-iteration 1
"""
import argparse
import json
import sys
from pathlib import Path


def generate_config(
    input_path: str,
    output_path: str,
    cache_path: str,
    input_filetype: str = "parquet",
    text_field: str = "text",
    char_ngrams: int = 24,
    num_bands: int = 20,
    minhashes_per_band: int = 13,
    bands_per_iteration: int = 5,
    use_64_bit_hash: bool = False,
    seed: int = 42,
) -> str:
    """Generate Hydra YAML configuration for fuzzy deduplication.

    Args:
        input_path: Path to input data
        output_path: Path for output (duplicate IDs)
        cache_path: Path for intermediate cache files
        input_filetype: Input format (parquet or jsonl)
        text_field: Field containing text to deduplicate
        char_ngrams: Shingle size (20-50, default 24)
        num_bands: Number of LSH bands (5-50, default 20)
        minhashes_per_band: Hashes per band (5-25, default 13)
        bands_per_iteration: Memory control (1-num_bands, default 5)
        use_64_bit_hash: Use 64-bit hash for large datasets
        seed: Random seed

    Returns:
        YAML configuration string
    """
    # Calculate similarity threshold for documentation
    threshold = (1 / num_bands) ** (1 / minhashes_per_band)

    return f'''# Fuzzy Deduplication Pipeline Configuration
# Generated by CURATOR-OS dedup-fuzzy skill
#
# Similarity threshold: ~{threshold:.2f} ({threshold * 100:.0f}%)
# Total hashes: {num_bands * minhashes_per_band}
#
# Run with:
#   python -m nemo_curator.config.run --config-path=. --config-name=<filename>

defaults:
  - _self_
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none

hydra:
  run:
    dir: .
  output_subdir: null

# I/O Configuration
input_path: {input_path}
output_path: {output_path}
cache_path: {cache_path}
input_filetype: {input_filetype}
input_blocksize: "1GiB"
text_field: "{text_field}"

# Ray Client Configuration
ray_client:
  _target_: nemo_curator.core.client.RayClient
  num_cpus: null  # Use all available CPUs
  num_gpus: 4     # Adjust based on your cluster

# Fuzzy Deduplication Workflow
# Note: This is a WorkflowBase class that orchestrates multiple internal pipelines
workflow:
  - _target_: nemo_curator.stages.deduplication.fuzzy.workflow.FuzzyDeduplicationWorkflow
    input_path: ${{input_path}}
    output_path: ${{output_path}}
    cache_path: ${{cache_path}}
    input_filetype: ${{input_filetype}}
    input_blocksize: ${{input_blocksize}}
    text_field: ${{text_field}}
    perform_removal: false  # Set to true to remove duplicates in place

    # MinHash Configuration
    seed: {seed}
    char_ngrams: {char_ngrams}  # Shingle size (higher = more precise, slower)

    # LSH Configuration
    num_bands: {num_bands}  # More bands = lower similarity threshold
    minhashes_per_band: {minhashes_per_band}  # More hashes = stricter matching

    # Memory Optimization
    # Lower bands_per_iteration reduces memory but increases runtime
    # Recommended: 5 for <100GB, 2-3 for 100-500GB, 1 for >500GB
    bands_per_iteration: {bands_per_iteration}

    # Use 64-bit hash for very large datasets (>1B documents)
    use_64_bit_hash: {str(use_64_bit_hash).lower()}

    # Environment variables (optional)
    env_vars: null
'''


def main():
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # Required arguments
    parser.add_argument(
        "--input-path",
        required=True,
        help="Path to input data directory",
    )
    parser.add_argument(
        "--output-path",
        required=True,
        help="Path for output (duplicate IDs will be written here)",
    )
    parser.add_argument(
        "--cache-path",
        required=True,
        help="Path for intermediate cache files (should have sufficient space)",
    )

    # Optional arguments
    parser.add_argument(
        "--input-filetype",
        default="parquet",
        choices=["parquet", "jsonl"],
        help="Input file format (default: parquet)",
    )
    parser.add_argument(
        "--text-field",
        default="text",
        help="Field containing text to deduplicate (default: text)",
    )
    parser.add_argument(
        "--char-ngrams",
        type=int,
        default=24,
        help="Shingle size for hashing, 20-50 (default: 24)",
    )
    parser.add_argument(
        "--num-bands",
        type=int,
        default=20,
        help="Number of LSH bands, 5-50 (default: 20)",
    )
    parser.add_argument(
        "--minhashes-per-band",
        type=int,
        default=13,
        help="Hashes per band, 5-25 (default: 13)",
    )
    parser.add_argument(
        "--bands-per-iteration",
        type=int,
        default=5,
        help="Memory control: lower = less memory, slower (default: 5)",
    )
    parser.add_argument(
        "--use-64-bit-hash",
        action="store_true",
        help="Use 64-bit hash for very large datasets (>1B documents)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility (default: 42)",
    )
    parser.add_argument(
        "--output-file",
        help="Write config to file instead of stdout",
    )

    args = parser.parse_args()

    # Validation
    errors = []
    warnings = []

    if args.char_ngrams < 20:
        warnings.append(
            f"char_ngrams={args.char_ngrams} is low, may cause high false positive rate"
        )
    if args.char_ngrams > 50:
        warnings.append(
            f"char_ngrams={args.char_ngrams} is high, may miss near-duplicates"
        )

    if args.bands_per_iteration > args.num_bands:
        errors.append(
            f"bands_per_iteration ({args.bands_per_iteration}) must be <= num_bands ({args.num_bands})"
        )

    if args.bands_per_iteration < 1:
        errors.append("bands_per_iteration must be >= 1")

    # Print warnings
    for warning in warnings:
        print(json.dumps({"warning": warning}), file=sys.stderr)

    # Exit on errors
    if errors:
        for error in errors:
            print(json.dumps({"error": error}), file=sys.stderr)
        sys.exit(1)

    # Generate config
    config = generate_config(
        input_path=args.input_path,
        output_path=args.output_path,
        cache_path=args.cache_path,
        input_filetype=args.input_filetype,
        text_field=args.text_field,
        char_ngrams=args.char_ngrams,
        num_bands=args.num_bands,
        minhashes_per_band=args.minhashes_per_band,
        bands_per_iteration=args.bands_per_iteration,
        use_64_bit_hash=args.use_64_bit_hash,
        seed=args.seed,
    )

    # Output
    if args.output_file:
        output_path = Path(args.output_file)
        output_path.write_text(config)
        # Print success message as JSON for agent consumption
        print(
            json.dumps(
                {
                    "status": "success",
                    "file": str(output_path),
                    "similarity_threshold": f"{(1 / args.num_bands) ** (1 / args.minhashes_per_band):.2f}",
                    "run_command": f"python -m nemo_curator.config.run --config-path=. --config-name={output_path.stem}",
                }
            )
        )
    else:
        print(config)


if __name__ == "__main__":
    main()
