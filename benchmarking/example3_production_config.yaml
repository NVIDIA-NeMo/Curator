---
# Example 3: Production Setup with Sinks (Adapted for our environment)
results_dir: "/raid/jnke/curator/Curator/benchmarking_results"
default_timeout_s: 7200

# Production-level sinks configuration
sinks:
  - name: mlflow
    tracking_uri: http://localhost:5000  # Mock MLflow server
    experiment: nemo-curator-benchmarks
    enabled: false  # Disabled for demo (no MLflow server running)
  - name: slack
    webhook_url: "https://hooks.slack.com/services/DUMMY/WEBHOOK/URL"  # Dummy URL for demo
    enabled: false  # Disabled for demo (no Slack webhook)
  - name: gdrive
    enabled: false  # Disabled for demo

# Advanced cleanup configuration
delete_scratch: true

# Dataset configuration (adapted to our environment) 
datasets:
  - name: sample_dataset
    formats:
      - type: text
        path: "/tmp/dummy_input"
      - type: json
        path: "/tmp/dummy_input"  # Same file, different format reference

# Production-style benchmark entries
entries:
  - name: production_benchmark_cpu
    script: test_benchmark.py
    args: >-
      --input-path {dataset:sample_dataset,text}
      --output-path {session_entry_dir}/scratch/output
    timeout_s: 1800
    ray:
      num_cpus: 4  # Scaled down for our container
      num_gpus: 0
      enable_object_spilling: false
    delete_scratch: false  # Override global setting for inspection

  - name: production_benchmark_advanced
    script: test_benchmark.py  
    args: >-
      --input-path {dataset:sample_dataset,json}
      --output-path {session_entry_dir}/scratch/advanced_output
    timeout_s: 2400
    ray:
      num_cpus: 2
      num_gpus: 0
      object_store_gb: 1
