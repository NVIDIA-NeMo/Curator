# These three paths must be defined and are used to resolve the paths for
# results, artifacts, and datasets. These must be existing paths on the host.
# If running inside a Docker container started with tools/run.sh, the paths
# will automatically be mapped to the appropriate volume mount.
# If running on the host, the paths will remain as defined below.
# Paths can be used in other values in this file by using their placeholder
# (e.g. {datasets_path}/my/test/dataset.parquet) and will be resolved to the
# appropriate path at runtime.
results_path: /raid/curator-team/nightly/results
artifacts_path: /raid/curator-team/nightly/artifacts
datasets_path: /raid

datasets:
  - name: "tinystories_train"
    formats:
    - type: "parquet"
      path: "{datasets_path}/prospector-lm/clean/tinystories_train_parquet"
  - name: "cleaned_exact_dedup_all_cc"
    formats:
    - type: "jsonl"
      path: "{datasets_path}/prospector-lm/cleaned_exact_dedup_all_cc"

default_timeout_s: 7200

# Optional sinks
sinks:
#  - name: mlflow
#    enabled: false
#    tracking_uri: ${MLFLOW_TRACKING_URI}
#    experiment: ray-curator-common-crawl
  - name: slack
    enabled: true
    webhook_url: ${SLACK_WEBHOOK_URL}
    default_metrics: ["exec_time_s"]
#  - name: gdrive
#    enabled: false
#    drive_folder_id: ${GDRIVE_FOLDER_ID}
#    service_account_file: ${GDRIVE_SERVICE_ACCOUNT_FILE}

# Whether to delete scratch dirs after each run
delete_scratch: true

entries:
  - name: domain_classification_raydata
    enabled: true
    script: domain_classification_benchmark.py
    args: >-
      --executor=ray_data
      --input-path={dataset:tinystories_train,parquet}
      --dataset-size-gb=10
      --model-inference-batch-size=1024
    timeout_s: 20000
    sink_data:
      - name: slack
        # Additional metrics to include in the Slack report.  These must be present in the metrics.json file generated by the script.
        additional_metrics: ["num_documents_processed", "throughput_docs_per_sec"]
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    # Additional requirements for the benchmark to pass.  These will result in the benchmark being marked as failed if not met.
    requirements:
      - metric: throughput_docs_per_sec
        min_value: 0.2

  - name: domain_classification_xenna
    enabled: true
    script: domain_classification_benchmark.py
    args: >-
      --executor=xenna
      --input-path={dataset:tinystories_train,parquet}
      --dataset-size-gb=10
      --model-inference-batch-size=1024
    timeout_s: 20000

  - name: embedding_generation_raydata
    enabled: true
    script: embedding_generation_benchmark.py
    args: >-
      --executor=ray_data
      --input-path={dataset:tinystories_train,parquet}
      --dataset-size-gb=10
      --model-identifier=sentence-transformers/all-MiniLM-L6-v2
      --model-inference-batch-size=1024
    timeout_s: 20000
    sink_data:
      - name: slack
        additional_metrics: ["num_documents_processed", "throughput_docs_per_sec"]
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false

  - name: embedding_generation_xenna
    enabled: true
    script: embedding_generation_benchmark.py
    args: >-
      --executor=xenna
      --input-path={dataset:tinystories_train,parquet}
      --dataset-size-gb=10
      --model-identifier=sentence-transformers/all-MiniLM-L6-v2
      --model-inference-batch-size=1024
    timeout_s: 20000

  - name: fuzzy_dedup_identification
    enabled: true
    script: fuzzy_dedup_identification_benchmark.py
    args: >-
      --input-path={dataset:cleaned_exact_dedup_all_cc,jsonl}
      --cache-path={session_entry_dir}/scratch/cache
      --output-path={session_entry_dir}/output
      --input-filetype=jsonl
      --bands-per-iteration=20
      --text-field=text
      --input-blocksize=1.5GiB
    timeout_s: 20000
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false

  - name: dedup_removal_raydata
    enabled: true
    script: dedup_removal_benchmark.py
    args: >-
      --input-path={dataset:cleaned_exact_dedup_all_cc,jsonl}
      --id-generator-path={datasets_path}/prospector-lm/cleaned_exact_dedup_all_cc_fuzzy_output_nightly_container_paths/fuzzy_id_generator.json
      --ids-to-remove-path={datasets_path}/prospector-lm/cleaned_exact_dedup_all_cc_fuzzy_output_nightly_container_paths/FuzzyDuplicateIds/
      --output-path={session_entry_dir}/scratch/output
      --executor=ray_data
      --input-filetype=jsonl
      --output-filetype=parquet
      --id-field=_curator_dedup_id
      --duplicate-id-field=_curator_dedup_id
      --blocksize=1.5GiB
    timeout_s: 20000
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false

  - name: dedup_removal_xenna
    enabled: true
    script: dedup_removal_benchmark.py
    args: >-
      --input-path={dataset:cleaned_exact_dedup_all_cc,jsonl}
      --id-generator-path={datasets_path}/prospector-lm/cleaned_exact_dedup_all_cc_fuzzy_output_nightly_container_paths/fuzzy_id_generator.json
      --ids-to-remove-path={datasets_path}/prospector-lm/cleaned_exact_dedup_all_cc_fuzzy_output_nightly_container_paths/FuzzyDuplicateIds/
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --input-filetype=jsonl
      --output-filetype=parquet
      --id-field=_curator_dedup_id
      --duplicate-id-field=_curator_dedup_id
      --blocksize=1.5GiB
    timeout_s: 20000
