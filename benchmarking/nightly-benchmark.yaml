# These two paths must be defined and are used to resolve the results and
# datasets paths. These must be existing paths on the host.
# If running inside a Docker container started with tools/run.sh, the paths
# will automatically be mapped to the appropriate volume mount.
# If running on the host, the paths will remain as defined below.
# Paths can be used in other values in this file by using their placeholder
# (e.g. {datasets_path}/my/test/dataset.parquet) and will be resolved to the
# appropriate path at runtime.
results_path: /path/where/results/are/stored
datasets_path: /path/to/datasets
model_weights_path: /path/to/model_weights

datasets:
  - name: "tinystories"
    formats:
    - type: "parquet"
      path: "{datasets_path}/tinystories/parquet_data"
    - type: "jsonl"
      path: "{datasets_path}/tinystories/jsonl_data"
  - name: "commoncrawl"
    formats:
    - type: "jsonl"
      path: "{datasets_path}/commoncrawl/jsonl_data"
    - type: "parquet_gemma_embeddings"
      path: "{datasets_path}/cleaned_exact_dedup_all_cc_gemma_embeddings"
  - name: "commoncrawl_id_map"
    formats:
    - type: "json"
      path: "{datasets_path}/commoncrawl/id_generator.json"
  - name: "commoncrawl_ids"
    formats:
    - type: "parquet"
      path: "{datasets_path}/commoncrawl/IDs/parquet_data"
  - name: "mscoco"
    formats:
    - type: "wds"
      path: "{datasets_path}/mscoco/wds/truncated_100K_mscoco_benchmarking"
  - name: "mscoco_model_weights"
    formats:
    - type: "files"
      path: "{datasets_path}/mscoco/model_weights"
  - name: "videos"
    formats:
    - type: "mp4"
      path: "{datasets_path}/videos"
  - name: " "
    formats:
    - type: "files"
      path: "{datasets_path}/video_model_weights"
  - name: "rpv2-2023-14-en"
    formats:
    - type: "parquet"
      path: "{datasets_path}/rpv2_2023-14_en"
  - name: "arxiv_downloads"
    formats:
    - type: "tar"
      path: "{datasets_path}/arxiv_downloads"
  - name: "fasttext_model"
    formats:
    - type: "bin"
      path: "{model_weights_path}/fasttext/lid.176.bin"

default_timeout_s: 7200

# Optional sinks
sinks:
#  - name: mlflow
#    enabled: false
#    tracking_uri: ${MLFLOW_TRACKING_URI}
#    experiment: ray-curator-common-crawl
  - name: slack
    enabled: true
    webhook_url: ${SLACK_WEBHOOK_URL}
    default_metrics: ["exec_time_s"]
#  - name: gdrive
#    enabled: false
#    drive_folder_id: ${GDRIVE_FOLDER_ID}
#    service_account_file: ${GDRIVE_SERVICE_ACCOUNT_FILE}

# Whether to delete scratch dirs after each run
delete_scratch: true

# The size of the object store used by Ray which can be either a value in bytes (int), or
# a fraction of total system memory (float), or the value "default" (string) which allows
# for "ray start" to determine object store size.
object_store_size: 536870912000  # 500GB

entries:
  # - name: video_embedding
  #   enabled: true
  #   script: video_pipeline_benchmark.py
  #   args: >-
  #     --benchmark-results-path={session_entry_dir}
  #     --output-path={session_entry_dir}/scratch/output
  #     --executor=xenna
  #     --video-dir={dataset:videos,mp4}
  #     --video-limit=1000
  #   timeout_s: 400
  #   ray:
  #     num_cpus: 64
  #     num_gpus: 4
  #     enable_object_spilling: false
  #   requirements:
  #     # ensure the total number of documents processed is correct
  #     - metric: num_clips_generated
  #       exact_value: 1400
  #     - metric: throughput_clips_per_sec
  #       min_value: 4.0

  # - name: video_transcoding
  #   enabled: true
  #   script: video_pipeline_benchmark.py
  #   args: >-
  #     --benchmark-results-path={session_entry_dir}
  #     --output-path={session_entry_dir}/scratch/output
  #     --executor=xenna
  #     --no-generate-embeddings
  #     --video-dir={dataset:videos,mp4}
  #     --video-limit=1000
  #   timeout_s: 400
  #   ray:
  #     num_cpus: 64
  #     num_gpus: 0
  #     enable_object_spilling: false
  #   requirements:
  #     # ensure the total number of documents processed is correct
  #     - metric: num_clips_generated
  #       exact_value: 1400
  #     - metric: throughput_clips_per_sec
  #       min_value: 5.0

  # - name: video_captioning
  #   enabled: true
  #   script: video_pipeline_benchmark.py
  #   args: >-
  #     --benchmark-results-path={session_entry_dir}
  #     --output-path={session_entry_dir}/scratch/output
  #     --executor=xenna
  #     --video-dir={dataset:videos,mp4}
  #     --generate-captions
  #     --no-generate-embeddings
  #     --enhance-captions
  #     --video-limit=256
  #   timeout_s: 1800
  #   ray:
  #     num_cpus: 64
  #     num_gpus: 4
  #     enable_object_spilling: false
  #   requirements:
  #     # ensure the total number of documents processed is correct
  #     - metric: num_clips_generated
  #       exact_value: 300 # TODO: update this value after benchmarking
  #     - metric: throughput_clips_per_sec
  #       min_value: 0.25

  - name: video_transnetv2_motion_aesthetic_filter_embeddings
    enabled: true
    script: video_pipeline_benchmark.py
    args: >-
      --benchmark-results-path={session_entry_dir}
      --output-path={session_entry_dir}/scratch/output
      --executor=xenna
      --video-dir={dataset:videos,mp4}
      --splitting-algorithm=transnetv2
      --motion-filter=enable
      --aesthetic-threshold=3.5
      --transnetv2-frame-decoder-mode ffmpeg_cpu
      --video-limit=1000
    timeout_s: 500
    ray:
      num_cpus: 64
      num_gpus: 4
      enable_object_spilling: false
    requirements:
      # ensure the total number of documents processed is correct
      - metric: num_clips_generated
        exact_value: 113
      - metric: throughput_clips_per_sec
        min_value: 0.25
