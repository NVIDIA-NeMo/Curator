import argparse
from pathlib import Path
import yaml
import os
import time
import sys
from typing import Any
import json
import traceback
from statistics import mean, stdev
import pickle
import shutil
import traceback

from loguru import logger

from nemo_curator.tasks import Task
from nemo_curator.tasks.utils import TaskPerfUtils

# FIXME: How do we want to package this tool? Perhaps a package extra for
#  nemo-curator, i.e. nemo-curator[benchmarking]?
# For now, add this directory to PYTHONPATH to import the runner modules
sys.path.insert(0, Path(__file__).parent)
from runner.matrix import MatrixConfig, MatrixEntry    
from runner.datasets import DatasetResolver
from runner.utils import get_obj_for_json
from runner.process import run_command_with_timeout
from runner.env_capture import dump_env


def ensure_dir(dir_path: Path) -> None:
    """Ensure dir_path and parents exists, creating them if necessary."""
    dir_path.mkdir(parents=True, exist_ok=True)


def create_or_overwrite_dir(dir_path: Path) -> None:
    """Create directory, removing it if it exists."""
    if dir_path.exists():
        shutil.rmtree(dir_path, ignore_errors=True)
    dir_path.mkdir(parents=True, exist_ok=True)


def aggregate_task_metrics(tasks: list[Task], prefix: str | None = None) -> dict[str, Any]:
    """Aggregate task metrics by computing mean/std/sum."""
    metrics = {}
    tasks_metrics = TaskPerfUtils.collect_stage_metrics(tasks)
    # For each of the metric compute mean/std/sum and flatten the dict
    for stage_name, stage_data in tasks_metrics.items():
        for metric_name, values in stage_data.items():
            for agg_name, agg_func in [("sum", sum), ("mean", mean), ("std", stdev)]:
                stage_key = stage_name if prefix is None else f"{prefix}_{stage_name}"
                if len(values) > 0:
                    metrics[f"{stage_key}_{metric_name}_{agg_name}"] = float(agg_func(values))
                else:
                    metrics[f"{stage_key}_{metric_name}_{agg_name}"] = 0.0
    return metrics


def get_entry_script_persisted_data(benchmark_results_path: Path) -> dict[str, Any]:
    """ Read the files that are expected to be generated by the individual benchmark scripts.
    """
    params_json = benchmark_results_path / "params.json"
    if not params_json.exists():
        logger.warning(f"Params JSON file not found at {params_json}")
        script_params = {}
    else:
        with open(params_json) as f:
            script_params = json.load(f)

    metrics_json = benchmark_results_path / "metrics.json"
    if not metrics_json.exists():
        logger.warning(f"Metrics JSON file not found at {metrics_json}")
        script_metrics = {}
    else:
        with open(metrics_json) as f:
            script_metrics = json.load(f)

    tasks_pkl = benchmark_results_path / "tasks.pkl"
    if not tasks_pkl.exists():
        logger.warning(f"Tasks pickle file not found at {tasks_pkl}")
        script_tasks = []
    else:
        with open(tasks_pkl, "rb") as f:
            script_tasks = pickle.load(f)  # noqa: S301
        if isinstance(script_tasks, list):
            script_metrics.update(aggregate_task_metrics(script_tasks, prefix="task"))
        elif isinstance(script_tasks, dict):
            for pipeline_name, pipeline_tasks in script_tasks.items():
                script_metrics.update(aggregate_task_metrics(pipeline_tasks, prefix=pipeline_name.lower()))

    return {"params": script_params, "metrics": script_metrics}


def run_entry(  # noqa: PLR0915
    entry: MatrixEntry,
    dataset_resolver: DatasetResolver,
    session_path: Path,
    result: dict[str, Any],
) -> tuple[dict[str, Any], bool, dict[str, Any]]:
    
    started_at = time.time()    
    session_entry_path = session_path / entry.name

    # scratch_path : This is the directory user can use to store scratch data; it'll be cleaned up after the entry is done
    # ray_cluster_path : This is the directory where Ray cluster is started; it'll be cleaned up after the entry is done
    # logs_path : This is the directory where logs are stored
    # benchmark_results_path : This is the directory where benchmark results are stored
    scratch_path, ray_cluster_path, logs_path, benchmark_results_path = [
        (session_entry_path / d).absolute() for d in ["scratch", "ray_cluster", "logs", "benchmark_results"]
    ]

    cmd = entry.get_command_to_run(session_entry_path, benchmark_results_path, dataset_resolver)
    run_id = result.get("run_id", f"{entry.name}-{int(time.time())}")

    try:
        # Create directories individually
        for directory in [scratch_path, ray_cluster_path, logs_path, benchmark_results_path]:
            create_or_overwrite_dir(directory)

        # Execute command with timeout
        logger.info(f"\t\tRunning command {' '.join(cmd) if isinstance(cmd, list) else cmd}")
        started_exec = time.time()
        completed = run_command_with_timeout(
            command=cmd,
            timeout=entry.timeout_s,
            stdouterr_path=logs_path / "stdouterr.log",
            env=os.environ,
            run_id=run_id,
        )
        ended_exec = time.time()

        # Show result
        duration = ended_exec - started_exec
        if completed["returncode"] == 0:
            success = True
            logger.success(f"\t\t✅ Run Succeeded in {duration:.1f} seconds")
        else:
            success = False
            logger.error(f"\t\t❌ Run Failed in {duration:.1f} seconds")
            if completed["timed_out"]:
                logger.warning(f"\t\t⏰ Timed out after {entry.timeout_s}s")
        logger.info(f"\t\tLogs found in {logs_path}")

        result.update({
            "cmd": cmd,
            "started_at": started_at,
            "ended_at": time.time(),
            "exec_started_at": started_exec,
            "exec_time_s": ended_exec - started_exec,
            "exit_code": completed["returncode"],
            "timed_out": completed["timed_out"],
            "logs_dir": logs_path,
            "success": success,
        })
        ray_data = {}
        script_persisted_data = get_entry_script_persisted_data(benchmark_results_path)
        result.update({
            "ray_data": ray_data,
            "script_persisted_data": script_persisted_data,
        })
        Path(session_entry_path / "results.json").write_text(json.dumps(get_obj_for_json(result)))

        return success

    finally:
        # Ray cleanup stuff here
        # Clean up the scratch dir if configured to delete
        if entry.delete_scratch:
            shutil.rmtree(scratch_path, ignore_errors=True)


def main() -> None:
    parser = argparse.ArgumentParser(description="Runs the benchmarking application")
    parser.add_argument(
        "--config",
        required=True,
        action="append",
        help="Path to YAML config for benchmark matrix, machine paths, etc. Can be specified multiple times.",
    )
    parser.add_argument(
        "--session-name", default=None, help="Optional human-readable session name (default nightly-run-<timestamp>)"
    )
    args = parser.parse_args()

    # Consolidate the configuration from all YAML files into a single dict,
    # and use by passing individual components the keys they need
    config_dict = {}
    for yml_file in args.config:
        config_dicts = yaml.full_load_all(open(yml_file))
        for d in config_dicts:
            config_dict.update(d)
    
    config = MatrixConfig.create_from_dict(config_dict)
    resolver = DatasetResolver.create_from_dicts(config_dict["datasets"])

    # Create session folder under results_dir
    session_name_prefix = args.session_name or "benchmark-run"
    session_name = time.strftime(f"{session_name_prefix}__%Y-%m-%d__%H-%M-%S")
    session_path = (Path(config.results_dir) / session_name).absolute()
    ensure_dir(session_path)

    session_overall_success = True
    logger.info(f"Started session {session_name}...")
    env_data = dump_env(session_path)
 
    for sink in config.sinks:
        sink.initialize(session_name, env_data)

    for entry in config.entries:
        run_success = False
        run_id = f"{entry.name}-{int(time.time())}"
        result = {
            "name": entry.name,
            "run_id": run_id,
            "success": run_success,
        }
        logger.info(f"\tRunning {entry.name} (run ID: {run_id})")
        try:
            run_success = run_entry(
                entry=entry,
                dataset_resolver=resolver,
                session_path=session_path,
                result=result,
            )

        except Exception as e:  # noqa: BLE001
            run_success = False
            error_traceback = traceback.format_exc()
            logger.error(f"\t\t❌ Entry failed with exception: {e}")
            logger.debug(f"Full traceback:\n{error_traceback}")
            result.update({
                "error": str(e),
                "traceback": error_traceback,
                "success": run_success,
            })
        
        finally:
            session_overall_success &= run_success
            for sink in config.sinks:
                sink.process_result(result)

    for sink in config.sinks:
        sink.finalize()
    logger.info(f"Session {session_name} completed with overall success: {session_overall_success}")
    return 0 if session_overall_success else 1


if __name__ == "__main__":
    raise SystemExit(main())
