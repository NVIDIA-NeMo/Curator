# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import json
import os
import pickle
import shutil
import sys
import time
import traceback
from pathlib import Path
from typing import Any

import yaml
from loguru import logger

from nemo_curator.tasks.utils import TaskPerfUtils
from nemo_curator.utils.file_utils import create_or_overwrite_dir

_this_script_dir = Path(__file__).parent

# TODO: How do we want to package this tool? Perhaps a package extra for
#  nemo-curator, i.e. nemo-curator[benchmarking]?
# For now, add this directory to PYTHONPATH to import the runner modules
sys.path.insert(0, _this_script_dir)

from runner.datasets import DatasetResolver  # noqa: E402
from runner.env_capture import dump_env  # noqa: E402
from runner.matrix import MatrixConfig, MatrixEntry  # noqa: E402
from runner.process import run_command_with_timeout  # noqa: E402
from runner.utils import get_obj_for_json, resolve_env_vars  # noqa: E402

_default_config_file = _this_script_dir / "config.yaml"


def ensure_dir(dir_path: Path) -> None:
    """Ensure dir_path and parents exists, creating them if necessary."""
    dir_path.mkdir(parents=True, exist_ok=True)


def get_entry_script_persisted_data(benchmark_results_path: Path) -> dict[str, Any]:
    """Read the files that are expected to be generated by the individual benchmark scripts."""
    params_json = benchmark_results_path / "params.json"
    if not params_json.exists():
        logger.warning(f"Params JSON file not found at {params_json}")
        script_params = {}
    else:
        with open(params_json) as f:
            script_params = json.load(f)

    metrics_json = benchmark_results_path / "metrics.json"
    if not metrics_json.exists():
        logger.warning(f"Metrics JSON file not found at {metrics_json}")
        script_metrics = {}
    else:
        with open(metrics_json) as f:
            script_metrics = json.load(f)

    tasks_pkl = benchmark_results_path / "tasks.pkl"
    if not tasks_pkl.exists():
        logger.warning(f"Tasks pickle file not found at {tasks_pkl}")
        script_tasks = []
    else:
        with open(tasks_pkl, "rb") as f:
            script_tasks = pickle.load(f)  # noqa: S301
        if isinstance(script_tasks, list):
            script_metrics.update(TaskPerfUtils.aggregate_task_metrics(script_tasks, prefix="task"))
        elif isinstance(script_tasks, dict):
            for pipeline_name, pipeline_tasks in script_tasks.items():
                script_metrics.update(
                    TaskPerfUtils.aggregate_task_metrics(pipeline_tasks, prefix=pipeline_name.lower())
                )

    return {"params": script_params, "metrics": script_metrics}


def run_entry(
    entry: MatrixEntry,
    dataset_resolver: DatasetResolver,
    session_path: Path,
    result: dict[str, Any],
) -> tuple[dict[str, Any], bool, dict[str, Any]]:
    started_at = time.time()
    session_entry_path = session_path / entry.name

    # scratch_path : This is the directory user can use to store scratch data; it'll be cleaned up after the entry is done
    # ray_cluster_path : This is the directory where Ray cluster is started; it'll be cleaned up after the entry is done
    # logs_path : This is the directory where logs are stored
    # benchmark_results_path : This is the directory where benchmark results are stored
    scratch_path, ray_cluster_path, logs_path, benchmark_results_path = [
        (session_entry_path / d).absolute() for d in ["scratch", "ray_cluster", "logs", "benchmark_results"]
    ]

    cmd = entry.get_command_to_run(session_entry_path, benchmark_results_path, dataset_resolver)
    run_id = result.get("run_id", f"{entry.name}-{int(time.time())}")

    try:
        # Create directories individually
        for directory in [scratch_path, ray_cluster_path, logs_path, benchmark_results_path]:
            create_or_overwrite_dir(directory)

        # Execute command with timeout
        logger.info(f"\t\tRunning command {' '.join(cmd) if isinstance(cmd, list) else cmd}")
        started_exec = time.time()
        completed = run_command_with_timeout(
            command=cmd,
            timeout=entry.timeout_s,
            stdouterr_path=logs_path / "stdouterr.log",
            env=os.environ,
            run_id=run_id,
        )
        ended_exec = time.time()

        # Show result
        duration = ended_exec - started_exec
        if completed["returncode"] == 0:
            success = True
            logger.success(f"\t\t✅ Run Succeeded in {duration:.1f} seconds")
        else:
            success = False
            logger.error(f"\t\t❌ Run Failed in {duration:.1f} seconds")
            if completed["timed_out"]:
                logger.warning(f"\t\t⏰ Timed out after {entry.timeout_s}s")
        logger.info(f"\t\tLogs found in {logs_path}")

        result.update(
            {
                "cmd": cmd,
                "started_at": started_at,
                "ended_at": time.time(),
                "exec_started_at": started_exec,
                "exec_time_s": ended_exec - started_exec,
                "exit_code": completed["returncode"],
                "timed_out": completed["timed_out"],
                "logs_dir": logs_path,
                "success": success,
            }
        )
        ray_data = {}
        script_persisted_data = get_entry_script_persisted_data(benchmark_results_path)
        result.update(
            {
                "ray_data": ray_data,
                "script_persisted_data": script_persisted_data,
            }
        )
        Path(session_entry_path / "results.json").write_text(json.dumps(get_obj_for_json(result)))

        return success

    finally:
        # Ray cleanup stuff here
        # Clean up the scratch dir if configured to delete
        if entry.delete_scratch:
            shutil.rmtree(scratch_path, ignore_errors=True)


def main() -> None:
    parser = argparse.ArgumentParser(description="Runs the benchmarking application")
    parser.add_argument(
        "--config",
        type=Path,
        action="append",
        help=(
            "Path to YAML config for benchmark matrix, machine paths, etc. Can be "
            "specified multiple times to merge configs. If not specified, "
            f"{_default_config_file} will be used."
        ),
    )
    parser.add_argument(
        "--session-name",
        default=None,
        help=("Optional human-readable session name. Default is benchmark-run__<timestamp>."),
    )
    args = parser.parse_args()

    # Consolidate the configuration from all YAML files into a single dict
    config_dict = {}
    for yml_file in args.config or [_default_config_file]:
        with open(yml_file) as f:
            config_dicts = yaml.full_load_all(f)
            for d in config_dicts:
                config_dict.update(d)
    # Preprocess the config dict prior to creating objects from it
    try:
        MatrixConfig.assert_valid_config(config_dict)
        config_dict = resolve_env_vars(config_dict)
    except ValueError as e:
        logger.error(f"Invalid configuration: {e}")
        return 1

    config = MatrixConfig.create_from_dict(config_dict)
    resolver = DatasetResolver.create_from_dicts(config_dict.get("datasets", []))

    # Create session folder under results_dir
    session_name = args.session_name or time.strftime("benchmark-run__%Y-%m-%d__%H-%M-%S")
    session_path = (Path(config.results_dir) / session_name).absolute()
    ensure_dir(session_path)

    session_overall_success = True
    logger.info(f"Started session {session_name}...")
    env_data = dump_env(session_path)

    for sink in config.sinks:
        sink.initialize(session_name, env_data)

    for entry in config.entries:
        run_success = False
        run_id = f"{entry.name}-{int(time.time())}"
        result = {
            "name": entry.name,
            "run_id": run_id,
            "success": run_success,
        }
        logger.info(f"\tRunning {entry.name} (run ID: {run_id})")
        try:
            run_success = run_entry(
                entry=entry,
                dataset_resolver=resolver,
                session_path=session_path,
                result=result,
            )

        except Exception as e:  # noqa: BLE001
            run_success = False
            error_traceback = traceback.format_exc()
            logger.error(f"\t\t❌ Entry failed with exception: {e}")
            logger.debug(f"Full traceback:\n{error_traceback}")
            result.update(
                {
                    "error": str(e),
                    "traceback": error_traceback,
                    "success": run_success,
                }
            )

        finally:
            session_overall_success &= run_success
            for sink in config.sinks:
                sink.process_result(result)

    for sink in config.sinks:
        sink.finalize()
    logger.info(f"Session {session_name} completed with overall success: {session_overall_success}")
    return 0 if session_overall_success else 1


if __name__ == "__main__":
    raise SystemExit(main())
